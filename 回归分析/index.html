<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>回归分析 | 小明的博客</title><meta name="author" content="爱编程的小明"><meta name="copyright" content="爱编程的小明"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#e68ab8"><meta name="description" content="..."><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "回归分析",
  "url": "https://kebuaaa.github.io/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/",
  "image": "https://kebuaaa.github.io/top_img/10043.webp",
  "datePublished": "2022-03-09T07:26:00.000Z",
  "dateModified": "2022-08-30T07:26:00.000Z",
  "author": [
    {
      "@type": "Person",
      "name": "爱编程的小明",
      "url": "https://kebuaaa.github.io/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://kebuaaa.github.io/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//www.clarity.ms"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>(()=>{const e={set:(e,t,o)=>{if(!o)return;const a=Date.now()+864e5*o;localStorage.setItem(e,JSON.stringify({value:t,expiry:a}))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const{value:o,expiry:a}=JSON.parse(t);if(!(Date.now()>a))return o;localStorage.removeItem(e)}};window.btf={saveToLocal:e,getScript:(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,Object.entries(t).forEach((([e,t])=>n.setAttribute(e,t))),n.onload=n.onreadystatechange=()=>{n.readyState&&!/loaded|complete/.test(n.readyState)||o()},n.onerror=a,document.head.appendChild(n)})),getCSS:(e,t)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onload=n.onreadystatechange=()=>{n.readyState&&!/loaded|complete/.test(n.readyState)||o()},n.onerror=a,document.head.appendChild(n)})),addGlobalFn:(e,t,o=!1,a=window)=>{const n=a.globalFn||{};n[e]=n[e]||{},n[e][o||Object.keys(n[e]).length]=t,a.globalFn=n}};const t=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},o=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#e68ab8")};btf.activateDarkMode=t,btf.activateLightMode=o;const a=e.get("theme");"dark"===a?t():"light"===a&&o();const n=e.get("aside-status");void 0!==n&&document.documentElement.classList.toggle("hide-aside","hide"===n);/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})()</script><script>var _hmt=_hmt||[];!function(){var t=document.createElement("script");t.src="https://hm.baidu.com/hm.js?4e3a07c287f8fb6cfc09bf5a7fdc1dd7";var a=document.getElementsByTagName("script")[0];a.parentNode.insertBefore(t,a)}(),btf.addGlobalFn("pjaxComplete",(()=>{_hmt.push(["_trackPageview",window.location.pathname])}),"baidu_analytics")</script><script>!function(e,t,n,c,a,i,r){e[n]=e[n]||function(){(e[n].q=e[n].q||[]).push(arguments)},(i=t.createElement(c)).async=1,i.src="https://www.clarity.ms/tag/e8bjif1knd",(r=t.getElementsByTagName(c)[0]).parentNode.insertBefore(i,r)}(window,document,"clarity","script")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:void 0,translate:{defaultEncoding:1,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"簡"},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:!1,highlightFullpage:!1,highlightMacStyle:!1},copy:{success:"复制成功",error:"复制失败",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!1},runtime:"",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"null",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!1,islazyloadPlugin:!0,isAnchor:!1,percent:{toc:!0,rightside:!1},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"回归分析",isHighlightShrink:!1,isToc:!0,pageType:"post"}</script><style>#article-container.post-content h1:before,h2:before,h3:before,h4:before,h5:before,h6:before{-webkit-animation:avatar_turn_around 1s linear infinite;-moz-animation:avatar_turn_around 1s linear infinite;-o-animation:avatar_turn_around 1s linear infinite;-ms-animation:avatar_turn_around 1s linear infinite;animation:avatar_turn_around 1s linear infinite}</style><link rel="stylesheet" href="/Scripts/css/transparent.css"><link rel="stylesheet" href="/Scripts/css/font.css"><link rel="stylesheet" href="/Scripts/css/foot_style.css"><link rel="stylesheet" href="/Scripts/css/twikoo_beautify.css"><link rel="stylesheet" href="/Scripts/css/tags.css"><link rel="stylesheet" href="/Scripts/css/cardlistpost.min.css"><style>#recent-posts>.recent-post-item>.recent-post-info>.article-meta-wrap>.tags:before{content:"\A";white-space:pre}#recent-posts>.recent-post-item>.recent-post-info>.article-meta-wrap>.tags>.article-meta__separator{display:none}</style><link rel="stylesheet" href="https://unpkg.zhimg.com/hexo-butterfly-wowjs/lib/animate.min.css" media="print" onload='this.media="screen"'><meta name="generator" content="Hexo 7.3.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet"></head><body><div id="web_bg" style="background-image:url(url(/img/index_img.webp))"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="/img/avatar.webp" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">100</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">57</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">18</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>存档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i> <span>关于</span></a></div><div class="menus_item"><a class="site-page" href="/wallpaper/"><i class="fa-fw fas fa-image fa-fw"></i> <span>壁纸</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://www.travellings.cn/go.html"><i class="fa-fw fa fa-subway"></i> <span>开往</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">小明的博客</span></a><a class="nav-page-title" href="/"><span class="site-name">回归分析</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>存档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i> <span>关于</span></a></div><div class="menus_item"><a class="site-page" href="/wallpaper/"><i class="fa-fw fas fa-image fa-fw"></i> <span>壁纸</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://www.travellings.cn/go.html"><i class="fa-fw fa fa-subway"></i> <span>开往</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">回归分析<a class="post-edit-link" href="https://github.dev/kebuAAA/myblog/blob/main/source/_posts/回归分析.md" title="编辑" target="_blank"><i class="fas fa-pencil-alt"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-03-09T07:26:00.000Z" title="发表于 2022-03-09 15:26:00">2022-03-09</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-08-30T07:26:00.000Z" title="更新于 2022-08-30 15:26:00">2022-08-30</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/">模型</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="container post-content" id="article-container"><p>在实际中，常常希望根据已有数据，确定因变量（数值变量）与自变量(可以是类别变量)的关系,在此关系的基础上对未知数据进行预测。这种方法叫<strong>回归分析</strong>。<br><strong>一般情况下可以按照模型的形式分为线性回归和非线性回归两种形式</strong>。</p><h1 id="分类">分类</h1><h2 id="线性模型">线性模型</h2><p>线性回归是在研究相关关系时优先考虑(最为直观)的一种模型，它假定所有解释变量对被解释变量的影响是线性叠加的，一般假设总体存在如下关系(矩阵形式):</p><p class="katex-block katex-error" title="ParseError: KaTeX parse error: No such environment: equation at position 7: \begin{̲e̲q̲u̲a̲t̲i̲o̲n̲}̲
\boldsymbol{Y}…">\begin{equation} \boldsymbol{Y}=\boldsymbol{X} \boldsymbol{\beta}+\boldsymbol{\mu} \end{equation}</p><p>在统计学习的视角下，不同的线性模型则是从不同的假设出发建立不同的<strong>目标函数</strong>/约束条件,进而求解得到不同的估计结果，常见的线性模型有最小二乘法、岭回归、套索回归、弹性网络回归等。对于线性回归的线性也可以从多个角度理解，通过适当放松线性条件则可以得到其它的回归方法，如多项式回归(变量非线性)、</p><h3 id="ols">OLS</h3><p>最小二乘法是用于拟合回归线最常用的方法。对于观测数据，它通过<strong>最小化每个数据点到线的垂直偏差平方和</strong>来计算最佳拟合线。<br>在计算总偏差时，偏差先平方，所以正值和负值没有抵消。<br><a href="./Linear%20Regression">线性回归-统计学视角</a><br><a href="./%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92">多元线性回归-计量视角</a></p><h3 id="岭回归-ridge-regression">岭回归(Ridge Regression)</h3><p>岭回归是解决OLS回归中多重共线性问题时提出的一种方法，后被进一步延伸和Lasso回归并称为线性模型的正则化方法，岭回归的具体介绍见文章<a href="./%E5%B2%AD%E5%9B%9E%E5%BD%92.md">岭回归</a>。</p><h3 id="套索回归-lasso-regression">套索回归(Lasso Regression)</h3><p>详情见<a href="./Lasso%E5%9B%9E%E5%BD%92.md">文章</a><br>套索回归类似于岭回归，Lasso（Least Absolute Shrinkage and Selection Operator）也会就回归系数向量给出惩罚值项。此外，它能够减少变化程度并提高线性回归模型的精度。套索回归的损失函数：</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mn>1</mn><mo>=</mo><mi>a</mi><mi>r</mi><mi>g</mi><mspace width="0.5em"><mi>m</mi><mi>i</mi><mi>n</mi><mo stretchy="false">{</mo><mrow><mo fence="true">∣</mo><mi mathvariant="normal">∣</mi><mi>y</mi><mo>−</mo><mi>β</mi><mi>x</mi><mo fence="true">∣</mo></mrow><mi mathvariant="normal">∣</mi><mo>+</mo><mi>λ</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>β</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">L1=arg\hspace{5pt} min\{\left||y-\beta x\right||+\lambda||\beta||\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathdefault">L</span><span class="mord">1</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right:.02778em">r</span><span class="mord mathdefault" style="margin-right:.03588em">g</span><span class="mspace" style="margin-right:.5em"></span><span class="mord mathdefault">m</span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span><span class="mopen">{</span><span class="minner"><span class="mopen delimcenter" style="top:0">∣</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:.03588em">y</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mord mathdefault" style="margin-right:.05278em">β</span><span class="mord mathdefault">x</span><span class="mclose delimcenter" style="top:0">∣</span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord">∣</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathdefault">λ</span><span class="mord">∣</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:.05278em">β</span><span class="mord">∣</span><span class="mord">∣</span><span class="mclose">}</span></span></span></span></span></p><p>使用的惩罚函数(<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>β</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">||\beta||</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord">∣</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:.05278em">β</span><span class="mord">∣</span><span class="mord">∣</span></span></span></span>)是l1范数。这导致惩罚值使一些参数估计结果等于零。<br>使用惩罚值越大，对模型的惩罚力度越大，从而获得一个特征较少的模型。参数的确定可以借助交叉验证法。<br><a target="_blank" rel="noopener" href="https://player.bilibili.com/player.html?aid=668496410&amp;bvid=BV1wa4y1Y7ai&amp;cid=200354037&amp;page=1">点击查看【bilibili】</a><br>优缺点：</p><ul><li>弥补最小二乘和逐步回归的不足，可以很好的进行特征选择</li><li>很好解决了特征多重共线性的问题</li><li>特征高度相关，模型倾向于选择一个特征忽略其它特征，会导致结果的不稳定</li></ul><h3 id="弹性网络回归-elasticnet">弹性网络回归(ElasticNet)</h3><p>ElasticNet是Lasso和Ridge回归技术的混合体。它使用L1来训练并且L2优先作为正则化矩阵。<br><strong>当有多个相关的特征时</strong>，ElasticNet是很有用的。Lasso 会随机挑选他们其中的一个，而ElasticNet则会选择两个。</p><h2 id="多项式回归-polynomial-regression">多项式回归(Polynomial Regression)</h2><p>对于一个回归方程，如果自变量的指数大于1，那么它就是多项式回归方程。<br>通常，<strong>多项式回归的方法是通过增加特征的方法，将高次项变换为1次项，从而将多项式回归问题转化为线性回归问题</strong>。</p><h2 id="逐步回归-stepwise-regression">逐步回归(Stepwise Regression)</h2><p>在处理多个自变量时，可以使用逐步回归。在这种技术中，自变量的选择是在一个自动的过程中完成的，其中包括非人为操作。<br><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/1.png" alt="image.png"><br>通过观察统计的值，来识别重要变量。逐步回归通过增删制定标准的协变量来拟合模型。<br>（1）标准逐步回归法。该方法做两件事情，即增加和删除每个步骤所需的预测。<br>（2）向前选择法。该方法从模型中最显著的预测开始，然后为每一步添加变量（依据AIC值）。<br>（3）向后剔除法。该方法与模型的所有预测同时开始，然后在每一步消除最小显著性的变量。<br>使用最少的预测变量数来最大化预测能力。是处理高维数据集的方法之一。<br><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/2.png" alt="image.png"></p><h3 id="支持向量机回归-svr">支持向量机回归（SVR）</h3><p><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/4.png" alt="image.png"><br>优点：</p><ul><li>不仅支持线性模型，对于数据和特征之间的非线性关系也能很好抓住；</li><li>不需要担心多重共线性问题，可以避免局部极小化问题，提高泛化性能，解决高维问题；</li><li>支持向量回归虽然不会在过程中直接排除异常点，但会使得由异常点引起的偏差更小。</li></ul><p>缺点：计算复杂度高，在面临数据量大的时候，计算耗时长。</p><h1 id="模型评价">模型评价</h1><p>回归分析在数据量远大于特征数量时往往能表现出比较优良的效果，但是需要注意的是线性模型对于特征之间的共线性非常敏感，当特征之间存在共线性时，数据稍微有一些变动（噪声的存在）都会对回归结果产生巨大影响。</p><h1 id="python实现">python实现</h1><p><a href="./%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%98.md">回归分析实战</a></p></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kebuaaa.github.io">爱编程的小明</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kebuaaa.github.io/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/">https://kebuaaa.github.io/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://kebuaaa.github.io" target="_blank">小明的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><a class="post-meta__tags" href="/tags/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/">回归分析</a></div><div class="post-share"><div class="social-share" data-image="/top_img/10043.webp" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/sklearn/" title="sklearn"><img class="cover" src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="/top_img/10004.webp" onerror='onerror=null,src="https://images.pexels.com/photos/374918/pexels-photo-374918.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=600"' alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">sklearn</div></div><div class="info-2"><div class="info-item-1">介绍 Scikit-learn项目始于scikits.learn，这是David Cournapeau的Google Summer of Code项目。它的名称源于它是“ SciKit”（SciPy工具包）的概念，它是SciPy的独立开发和分布式第三方扩展。原始代码库后来被其他开发人员重写。2010年费边Pedregosa，盖尔Varoquaux，亚历山大Gramfort和Vincent米歇尔，全部由法国国家信息与自动化研究所的罗屈昂库尔，法国，把该项目的领导和做出的首次公开发行在二月一日2010在各种scikits中，scikit-learn以及scikit-image在2012年11月被描述为“维护良好且受欢迎” 。Scikit-learn是GitHub上最受欢迎的机器学习库之一。 scikit-learn.pdf 8 Scikit Learn 速查表.pdf Python数据科学速查表 - Scikit-Learn.pdf</div></div></div></a><a class="pagination-related" href="/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" title="强化学习"><img class="cover" src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="/top_img/10047.webp" onerror='onerror=null,src="https://images.pexels.com/photos/374918/pexels-photo-374918.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=600"' alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">强化学习</div></div><div class="info-2"><div class="info-item-1">强化学习(reinforcement learning.)是指智能系统在与环境的连续互动中学习最优行为策略的机器学习问题。假设智能系统与环境的互动基于马尔可夫决策过程(Markov decision process).,智能系统能观测到的是与环境互动得到的数据序列。强化学习的本质是学习最优的序贯决策。</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/Logistic%20Regression/" title="Logistic Regression"><img class="cover" src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="/top_img/10044.webp" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-17</div><div class="info-item-2">Logistic Regression</div></div><div class="info-2"><div class="info-item-1">当因变量的类型属于二元（1 / 0，真/假，是/否）变量时，应该使用逻辑回归。这里，Y的值为0或1，它可以用以下方程表示： \begin{equation*} \begin{aligned} odds &amp;= \frac{p}{1-p}\\ &amp;=\frac{probability\hspace{5pt} of\hspace{5pt} event\hspace{5pt} occurrence}{probability\hspace{5pt} of\hspace{5pt} not\hspace{5pt} event...</div></div></div></a><a class="pagination-related" href="/EM%E7%AE%97%E6%B3%95%E5%8F%8A%E5%85%B6%E6%8E%A8%E5%B9%BF/" title="EM算法及其推广"><img class="cover" src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="/top_img/10048.webp" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-17</div><div class="info-item-2">EM算法及其推广</div></div><div class="info-2"><div class="info-item-1">EM算法 对于一般概率模型的学习策略，我们往往会采取极大似然估计或者贝叶斯估计的方法对模型的参数进行估计，但是需要注意的是这种估计方法都是建立在待估参数全部为已经知道结果的参数的基础之上的(complete-data problem)。当模型中有隐变量/潜在变量（数据不可观测的变量）时，似然函数的最大化变得困难。这是就可以使用EM算法,EM算法是在不完全数据下求解MLE估计结果的一种近似求解方法，用迭代去逼近原来不完整数据问题的结果。EM算法主要分为两步： E:求期望(expectation) M:求极大(maximization) EM算法的核心思想是在既定样本数据下在因变量最有可能的分布状态下利用极大似然估计模型的参数。 算法导出 针对一个含有隐变量的概率模型，这里假设隐变量为Z，观测数据Y关于参数θ\thetaθ的对数似然函数为L(θ)L(\theta)L(θ): \begin{equation} \begin{aligned} L(\theta) &amp; = \log...</div></div></div></a><a class="pagination-related" href="/Entropy/" title="Entroy"><img class="cover" src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="/top_img/10057.webp" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2022-04-05</div><div class="info-item-2">Entroy</div></div><div class="info-2"><div class="info-item-1">点击查看【bilibili】 1 章 绪论.pdf 2 章 离散信源及其信息测度.pdf intro 熵可以从随机变量状态需要的平均信息量角度理解, 也可以从描述统计力学中无序程度的度量角度理解。从平均信息量的角度来看，对于不确定性事件，可以用消除其不确定性需要的信息量(bit 数)来表示，这里表示成−log⁡pi-\log p_i−logpi​,而考虑到随机事件的不确定性，可以通过对信息量求期望得到某随机事件（随机变量）的信息熵，信息熵越大，则说明（消除随机性）需要的信息量越大，即不确定性越大。 一般来说，对于随机变量XXX，其信息熵定义如下: H(X)=−∑i=1np(xi)log⁡2p(xi)H(X)=-\sum\limits_{i=1}^{n}p(x_i)\log_2{p(x_i)} H(X)=−i=1∑n​p(xi​)log2​p(xi​) if p=0p=0p=0，then...</div></div></div></a><a class="pagination-related" href="/SVM%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98/" title="对偶问题（SVM）"><img class="cover" src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="/top_img/10029.webp" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2022-04-04</div><div class="info-item-2">对偶问题（SVM）</div></div><div class="info-2"><div class="info-item-1">Duality (optimization) In mathematical optimization theory, duality or the duality principle is the principle that optimization problems may be viewed from either of two perspectives, the primal problem or the dual problem. The solution to the dual problem provides a lower bound to the solution of the primal (minimization) problem.However in general the optimal values of the primal and dual problems need not be equal. Their difference is called the duality gap. For convex optimization...</div></div></div></a><a class="pagination-related" href="/%E5%86%B3%E7%AD%96%E6%A0%91/" title="决策树模型"><img class="cover" src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="/top_img/10001.webp" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-12</div><div class="info-item-2">决策树模型</div></div><div class="info-2"><div class="info-item-1">人们的决策过程是一个类似“观察因素A的情况，再根据A的情况观察因素B的情况”的形式，从而形成一种树状结构。决策树学习是模仿人类这一结构化决策过程而发展起来的一种有监督机器学习方法。 它可以被认为是if-then规则的集合，也可以被认为是定义在特征空间和类空间上的条件概率分布。 模型具有可读性 分类速度快 决策树的思想主要来源于Quinlan在1986年提出的ID3和1993提出的C4.5算法，以及由Breiman等人1984年提出的CART算法。 模型 决策树学习本质上是从训练数据集中归纳出一组分类规则或者条件概率模型（在节点处取条件概率最大的进行分类）。决策树问题一般可以分成特征选择、决策树生成、剪枝三部分。 特征选择：通过建立一个函数来衡量特征划分的效果 生成：递归构造决策树的过程 剪枝：递归产生的决策树往往会递归到不能分类为止，这会导致出现过拟合现象，因此需要已经生成的决策树进行剪枝(pruning)，一般是通过极小化决策树整体的损失函数(loss function)或者代价函数(cost...</div></div></div></a><a class="pagination-related" href="/%E5%88%86%E7%B1%BB%E6%96%B9%E6%B3%95/" title="分类方法"><img class="cover" src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="/top_img/10036.webp" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-06</div><div class="info-item-2">分类方法</div></div><div class="info-2"><div class="info-item-1">线性分类方法 感知机和线性判别分析/Fisher分析是非常经典的硬分类线性模型，模型提出都比较早。 感知机 感知机是二类分类的线性分类模型。 感知机只在求出线性可分的分类超平面，通过梯度下降法对损失函数极小化建立感知机模型。 感知机1957年由Rosenblatt提出，是神经网络和支持向量机的基础 模型 输入空间是实例向量组成的空间，输出空间是-1和+1（正负两类）。建立如下函数： \begin{align*} f(x)&amp;=sign(\omega \cdot x+b)\\ \omega&amp;:weight\quad or\quad weight\quad...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="/img/avatar.webp" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info-name">爱编程的小明</div><div class="author-info-description">只要不折腾，万般可将就</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">100</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">57</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">18</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/kebuAAA"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/kebuAAA" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="/2945190789@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="/img/wechat.webp" target="_blank" title="欢迎交流"><i class="fa-brands fa-weixin"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">如网页加载较慢请尝试魔法上网，博客图文可能无关可以忽略</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%88%86%E7%B1%BB"><span class="toc-number">1.</span> <span class="toc-text">分类</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.1.</span> <span class="toc-text">线性模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#ols"><span class="toc-number">1.1.1.</span> <span class="toc-text">OLS</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B2%AD%E5%9B%9E%E5%BD%92-ridge-regression"><span class="toc-number">1.1.2.</span> <span class="toc-text">岭回归(Ridge Regression)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A5%97%E7%B4%A2%E5%9B%9E%E5%BD%92-lasso-regression"><span class="toc-number">1.1.3.</span> <span class="toc-text">套索回归(Lasso Regression)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%B9%E6%80%A7%E7%BD%91%E7%BB%9C%E5%9B%9E%E5%BD%92-elasticnet"><span class="toc-number">1.1.4.</span> <span class="toc-text">弹性网络回归(ElasticNet)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92-polynomial-regression"><span class="toc-number">1.2.</span> <span class="toc-text">多项式回归(Polynomial Regression)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%80%90%E6%AD%A5%E5%9B%9E%E5%BD%92-stepwise-regression"><span class="toc-number">1.3.</span> <span class="toc-text">逐步回归(Stepwise Regression)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E5%9B%9E%E5%BD%92-svr"><span class="toc-number">1.3.1.</span> <span class="toc-text">支持向量机回归（SVR）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7"><span class="toc-number">2.</span> <span class="toc-text">模型评价</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#python%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.</span> <span class="toc-text">python实现</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/Lasso%E5%9B%9E%E5%BD%92/" title="Lasso回归"><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="/top_img/10029.webp" onerror='this.onerror=null,this.src="https://images.pexels.com/photos/374918/pexels-photo-374918.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=600"' alt="Lasso回归"></a><div class="content"><a class="title" href="/Lasso%E5%9B%9E%E5%BD%92/" title="Lasso回归">Lasso回归</a><time datetime="2023-11-14T16:00:00.000Z" title="更新于 2023-11-15 00:00:00">2023-11-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/%E5%B2%AD%E5%9B%9E%E5%BD%92/" title="岭回归"><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/岭回归_20231109082818.png" onerror='this.onerror=null,this.src="https://images.pexels.com/photos/374918/pexels-photo-374918.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=600"' alt="岭回归"></a><div class="content"><a class="title" href="/%E5%B2%AD%E5%9B%9E%E5%BD%92/" title="岭回归">岭回归</a><time datetime="2023-11-07T16:00:00.000Z" title="更新于 2023-11-08 00:00:00">2023-11-08</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/%E4%BC%98%E9%9B%85%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/" title="优雅论文排版"><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/优雅论文排版_20230921093206.png" onerror='this.onerror=null,this.src="https://images.pexels.com/photos/374918/pexels-photo-374918.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=600"' alt="优雅论文排版"></a><div class="content"><a class="title" href="/%E4%BC%98%E9%9B%85%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/" title="优雅论文排版">优雅论文排版</a><time datetime="2023-09-20T16:00:00.000Z" title="更新于 2023-09-21 00:00:00">2023-09-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/%E5%A4%9A%E5%85%83%E7%BB%9F%E8%AE%A1%E5%88%86%E6%9E%90/" title="多元统计分析"><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/多元统计分析_多元正态曲线.png" onerror='this.onerror=null,this.src="https://images.pexels.com/photos/374918/pexels-photo-374918.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=600"' alt="多元统计分析"></a><div class="content"><a class="title" href="/%E5%A4%9A%E5%85%83%E7%BB%9F%E8%AE%A1%E5%88%86%E6%9E%90/" title="多元统计分析">多元统计分析</a><time datetime="2023-06-16T02:22:54.000Z" title="更新于 2023-06-16 10:22:54">2023-06-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/Hypothesis%20testing/" title="假设检验"><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="/top_img/10008.webp" onerror='this.onerror=null,this.src="https://images.pexels.com/photos/374918/pexels-photo-374918.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=600"' alt="假设检验"></a><div class="content"><a class="title" href="/Hypothesis%20testing/" title="假设检验">假设检验</a><time datetime="2023-05-09T02:34:00.000Z" title="更新于 2023-05-09 10:34:00">2023-05-09</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="footer_custom_text"><div id="runtime"></div></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"></div><div id="rightside-config-show"><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><div class="js-pjax"></div><script src="/Scripts/js/beijing.js"></script><script src="/Scripts/js/foot_style.js"></script><script src="/Scripts/js/fireworks.js"></script><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>(()=>{window.pjax=new Pjax({elements:'a:not([target="_blank"])',selectors:["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"],cacheBust:!1,analytics:!1,scrollRestoration:!1});const e=e=>{e&&Object.values(e).forEach((e=>e()))};document.addEventListener("pjax:send",(()=>{btf.removeGlobalFnEvent("pjaxSendOnce"),btf.removeGlobalFnEvent("themeChange");const t=document.body.classList;t.contains("read-mode")&&t.remove("read-mode"),e(window.globalFn.pjaxSend)})),document.addEventListener("pjax:complete",(()=>{btf.removeGlobalFnEvent("pjaxCompleteOnce"),document.querySelectorAll("script[data-pjax]").forEach((e=>{const t=document.createElement("script"),n=e.text||e.textContent||e.innerHTML||"";Array.from(e.attributes).forEach((e=>t.setAttribute(e.name,e.value))),t.appendChild(document.createTextNode(n)),e.parentNode.replaceChild(t,e)})),e(window.globalFn.pjaxComplete)})),document.addEventListener("pjax:error",(e=>{if(404===e.request.status){window.location.href=e.request.responseURL}}))})()</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><div class="js-pjax"><script async>for(var arr=document.getElementsByClassName("recent-post-item"),i=0;i<arr.length;i++)arr[i].classList.add("wow"),arr[i].classList.add("animate__zoomIn"),arr[i].setAttribute("data-wow-duration","2s"),arr[i].setAttribute("data-wow-delay","0.5s"),arr[i].setAttribute("data-wow-offset","100"),arr[i].setAttribute("data-wow-iteration","1")</script><script async>for(var arr=document.getElementsByClassName("card-widget"),i=0;i<arr.length;i++)arr[i].classList.add("wow"),arr[i].classList.add("animate__zoomIn"),arr[i].setAttribute("data-wow-duration",""),arr[i].setAttribute("data-wow-delay",""),arr[i].setAttribute("data-wow-offset",""),arr[i].setAttribute("data-wow-iteration","")</script></div><script defer src="https://unpkg.zhimg.com/hexo-butterfly-wowjs/lib/wow.min.js"></script><script defer src="https://unpkg.zhimg.com/hexo-butterfly-wowjs/lib/wow_init.js"></script></body></html>