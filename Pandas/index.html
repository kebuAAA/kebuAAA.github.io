<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>Pandas | 小明的博客</title><meta name="author" content="爱编程的小明"><meta name="copyright" content="爱编程的小明"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#e68ab8"><meta name="description" content="..."><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://kebuaaa.github.io/Pandas/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//www.clarity.ms"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>(()=>{const e={set:(e,t,o)=>{if(!o)return;const a=Date.now()+864e5*o;localStorage.setItem(e,JSON.stringify({value:t,expiry:a}))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const{value:o,expiry:a}=JSON.parse(t);if(!(Date.now()>a))return o;localStorage.removeItem(e)}};window.btf={saveToLocal:e,getScript:(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,Object.entries(t).forEach((([e,t])=>n.setAttribute(e,t))),n.onload=n.onreadystatechange=()=>{n.readyState&&!/loaded|complete/.test(n.readyState)||o()},n.onerror=a,document.head.appendChild(n)})),getCSS:(e,t)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onload=n.onreadystatechange=()=>{n.readyState&&!/loaded|complete/.test(n.readyState)||o()},n.onerror=a,document.head.appendChild(n)})),addGlobalFn:(e,t,o=!1,a=window)=>{const n=a.globalFn||{};n[e]=n[e]||{},n[e][o||Object.keys(n[e]).length]=t,a.globalFn=n}};const t=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},o=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#e68ab8")};btf.activateDarkMode=t,btf.activateLightMode=o;const a=e.get("theme");"dark"===a?t():"light"===a&&o();const n=e.get("aside-status");void 0!==n&&document.documentElement.classList.toggle("hide-aside","hide"===n);/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})()</script><script>var _hmt=_hmt||[];!function(){var t=document.createElement("script");t.src="https://hm.baidu.com/hm.js?4e3a07c287f8fb6cfc09bf5a7fdc1dd7";var a=document.getElementsByTagName("script")[0];a.parentNode.insertBefore(t,a)}(),btf.addGlobalFn("pjaxComplete",(()=>{_hmt.push(["_trackPageview",window.location.pathname])}),"baidu_analytics")</script><script>!function(e,t,n,c,a,i,r){e[n]=e[n]||function(){(e[n].q=e[n].q||[]).push(arguments)},(i=t.createElement(c)).async=1,i.src="https://www.clarity.ms/tag/e8bjif1knd",(r=t.getElementsByTagName(c)[0]).parentNode.insertBefore(i,r)}(window,document,"clarity","script")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:void 0,translate:{defaultEncoding:1,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"簡"},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:!1,highlightFullpage:!1,highlightMacStyle:!1},copy:{success:"复制成功",error:"复制失败",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!1},runtime:"",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"null",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!1,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!1},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"Pandas",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,isShuoshuo:!1}</script><style>#article-container.post-content h1:before,h2:before,h3:before,h4:before,h5:before,h6:before{-webkit-animation:avatar_turn_around 1s linear infinite;-moz-animation:avatar_turn_around 1s linear infinite;-o-animation:avatar_turn_around 1s linear infinite;-ms-animation:avatar_turn_around 1s linear infinite;animation:avatar_turn_around 1s linear infinite}</style><link rel="stylesheet" href="/Scripts/css/transparent.css"><link rel="stylesheet" href="/Scripts/css/font.css"><link rel="stylesheet" href="/Scripts/css/foot_style.css"><link rel="stylesheet" href="/Scripts/css/twikoo_beautify.css"><link rel="stylesheet" href="/Scripts/css/tags.css"><link rel="stylesheet" href="/Scripts/css/cardlistpost.min.css"><style>#recent-posts>.recent-post-item>.recent-post-info>.article-meta-wrap>.tags:before{content:"\A";white-space:pre}#recent-posts>.recent-post-item>.recent-post-info>.article-meta-wrap>.tags>.article-meta__separator{display:none}</style><link rel="stylesheet" href="https://unpkg.zhimg.com/hexo-butterfly-wowjs/lib/animate.min.css" media="print" onload='this.media="screen"'><meta name="generator" content="Hexo 7.3.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet"></head><body><div id="web_bg" style="background-image:url(url(/img/index_img.webp))"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="/img/avatar.webp" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">100</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">57</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">18</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>存档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i> <span>关于</span></a></div><div class="menus_item"><a class="site-page" href="/wallpaper/"><i class="fa-fw fas fa-image fa-fw"></i> <span>壁纸</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://www.travellings.cn/go.html"><i class="fa-fw fa fa-subway"></i> <span>开往</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">小明的博客</span></a><a class="nav-page-title" href="/"><span class="site-name">Pandas</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>存档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i> <span>关于</span></a></div><div class="menus_item"><a class="site-page" href="/wallpaper/"><i class="fa-fw fas fa-image fa-fw"></i> <span>壁纸</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://www.travellings.cn/go.html"><i class="fa-fw fa fa-subway"></i> <span>开往</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">Pandas<a class="post-edit-link" href="https://github.dev/kebuAAA/myblog/blob/main/source/_posts/Pandas.md" title="编辑" target="_blank"><i class="fas fa-pencil-alt"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-09-02T15:33:00.000Z" title="发表于 2022-09-02 23:33:00">2022-09-02</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-10-19T15:33:00.000Z" title="更新于 2022-10-19 23:33:00">2022-10-19</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Python/">Python</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Python/Python%E5%BA%93/">Python库</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="container post-content" id="article-container"><p>放一个目录做的的思维导图</p><iframe src="../assets/Pandas_mindmap.html" width="100%" height="500px" frameborder="0" loading="lazy" allowfullscreen></iframe><blockquote><p>一场数模国赛下来发现对于pandas里的一些基础操作还不是很熟练，整个建模过程中用到了许多pandas中对DataFrame的索引，切片访问，分组交叉透视等功能，除此以外稍微高级一点的就是用了几次apply函数来对某一行的值进行一个计算输出，虽然知识描述统计这部分的内容，但是能明显的感觉到对于这些基本功能有些生疏。坦白来讲，pandas的描述统计相关的这些工作其实借助excel也能够实现，但是当考虑到可迁移性这些方面的内容时，使用编程语言的优越性也就自然而然地体现出来，当然，如果对相关的函数不能做到很熟悉的话，其实反而加大了工作量。</p></blockquote><h1 id="简介">简介</h1><p><strong>Pandas</strong> 是 Python 的核心数据分析支持库，提供了快速、灵活、明确的数据结构，旨在简单、直观地处理关系型、标记型数据。Pandas 的目标是成为 Python 数据分析实践与实战的必备高级工具，其长远目标是成为<strong>最强大、最灵活、可以支持任何语言的开源数据分析工具</strong>。经过多年不懈的努力，Pandas 离这个目标已经越来越近了。</p><blockquote><p>虽然 pandas 采用了大量的 NumPy 编码风格，但二者最大的不同是 pandas 是专门为处理表格和混杂数据设计的。而 NumPy 更适合处理统一的数值数组数据。</p></blockquote><h1 id="pandas-数据结构">Pandas 数据结构</h1><p>DataFrame 是 Pandas 最常用也是非常重要的一个对象，它是一个二维的数据结构，数据以行和列的表格方式排列。index+value+column<br>Series 是一个一维数据结构，包括 index 和 value。</p><h2 id="series">Series</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.Series([<span class="number">2</span>, <span class="number">9</span>, <span class="number">4</span>], index=[<span class="string">&quot;xiao ming&quot;</span>, <span class="string">&quot;xiao hong&quot;</span>, <span class="string">&quot;xiao lan&quot;</span>])</span><br><span class="line">data.index</span><br><span class="line">data.values</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="dataframe">DataFrame</h2><h3 id="属性：">属性：</h3><ul><li><strong>info:基本信息</strong></li><li>columns:列名</li><li>size</li><li>shape</li><li>len：查看某列的行数</li><li>count:查看某列的有效值（非空）的个数</li></ul><p><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Pandas/2.jpg" alt="66.jpg"></p><h3 id="方法">方法</h3><ul><li>head():</li><li>tail():</li></ul><h3 id="创建-dataframe">创建 DataFrame</h3><p>创建 DataFrame 的方式有很多种，一般比较常用的是利用一个字典或者数组来进行创建</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">data = pd.DataFrame(&#123;<span class="string">&#x27;Height&#x27;</span>: [<span class="number">1.7</span>, <span class="number">1.8</span>, <span class="number">1.9</span>],</span><br><span class="line">                     <span class="string">&#x27;Weight&#x27;</span>: [<span class="number">140</span>, <span class="number">160</span>, <span class="number">180</span>]&#125;,</span><br><span class="line">                    index=[<span class="string">&quot;xiaoming&quot;</span>, <span class="string">&quot;xiaohong&quot;</span>, <span class="string">&quot;xiaowang&quot;</span>])</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line">arr = np.<span class="built_in">abs</span>(np.random.randn(<span class="number">3</span>, <span class="number">2</span>))</span><br><span class="line"><span class="comment"># print(arr)</span></span><br><span class="line">data = pd.DataFrame(arr, columns=[<span class="string">&quot;Height&quot;</span>, <span class="string">&#x27;Weight&#x27;</span>], index=[</span><br><span class="line">                    <span class="string">&quot;xiao ming&quot;</span>, <span class="string">&#x27;xiao hong&#x27;</span>, <span class="string">&#x27;xiao zhang&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="comment">#设置总行名和总列名</span></span><br><span class="line">data.index.name=<span class="string">&#x27;students&#x27;</span></span><br><span class="line">data.columns.name=<span class="string">&#x27;body size&#x27;</span></span><br></pre></td></tr></table></figure><p>也可以通过字典嵌套的方式把 index 揉进去(字典的key作为列名，作为字典的value写作{index:value}的形式 )<br><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Pandas/3.png" alt="image.png"></p><h3 id="访问-dataframe">访问 DataFrame</h3><p>简介：<br><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Pandas/1660837774129.png" alt="1660837774129"></p><h4 id="使用字典方式访问-dataframe">使用字典方式访问 DataFrame。</h4><ul><li>访问单列：DataFrame[‘column1_name’]，DataFrame.column1_name（这种索引方式要保证列名合法）</li><li>访问多列：DataFrame[[‘column1_name’,‘colunmn2_name’]]</li><li>访问单列多行：DataFrame[‘column1_name’][m:n]</li><li>访问多列多行：DataFrame[[‘column1_name’,‘colunmn2_name’]][m:n]</li></ul><h4 id="使用属性方式访问">使用属性方式访问</h4><ul><li>单列:DataFrame.column1_name</li><li>单列多行:DataFrame.column1_name[m:n]</li></ul><h4 id="访问行的特殊方法">访问行的特殊方法</h4><ul><li>访问 m 行到 n 行：DataFrame[:][m:n]</li><li>DataFrame.head/tail()：访问前/后五行</li></ul><h4 id="整数标签的特殊情况">整数标签的特殊情况</h4><p>为了防止计算机不知道用户输入的索引是基于位置还是基于标签的，pd 整数标签的索引是基于标签的，也就是说我们不能像列表一样使用 DataFrame[-1]进行访问（仅针对整数作为索引的情况）</p><h4 id="切片访问方法">切片访问方法</h4><p><strong>DataFrame.loc[]访问</strong><br>访问时主要采用[行索引或者条件,‘column1_name’]的方式对 DataFrame 进行切片，对行的指定要使用索引或者条件，对列的索引必须使用列名称，如果有多列，则还需要借助[]将列名称括起来。使用 loc 传入的行索引名称如果为一个区间，则前后均为闭区间</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#条件表达式切片用法</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;条件表达式使用字典方式，xy123中x&lt;5的x为：\n&#x27;</span>,</span><br><span class="line">     xy123.loc[xy123[<span class="string">&#x27;x&#x27;</span>]&lt;<span class="number">3</span>,</span><br><span class="line">     [<span class="string">&#x27;x&#x27;</span>]])<span class="comment">#条件表达式使用字典方式</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;条件表达式使用属性方式，xy123中x&gt;=8的x, y1为：\n&#x27;</span>,</span><br><span class="line">     xy123.loc[xy123.x&gt;=<span class="number">8</span>,</span><br><span class="line">     [<span class="string">&#x27;x&#x27;</span>,<span class="string">&#x27;y1&#x27;</span>]])<span class="comment">#条件表达式使用属性方式</span></span><br><span class="line"><span class="comment">#df.loc[(df.Age&gt;10) &amp; (df.Age&lt;50)]</span></span><br><span class="line"><span class="comment">#并列条件记得加括号，不然会报错！！！</span></span><br></pre></td></tr></table></figure><p>需要注意的是 loc 函数的第一个参数不能直接传入整数，可以考虑送个列表进去<br><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Pandas/4.png" alt="image.png"><br><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Pandas/5.png" alt="image.png"></p><p><strong>DataFrame.iloc[]访问</strong><br>使用方法与 loc 相似，主要区别是该函数在使用时对列的索引可以用列索引号。使用 iloc 传入的行索引位置或列索引位置为区间时，则为前闭后开区间</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#例3-46，iloc条件切片</span></span><br><span class="line"><span class="comment">#iloc内部传入表达式，进行条件切片，需使用.values属性</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;条件表达式使用字典方式，xy123中x&lt;1的第1,3列数据为：\n&#x27;</span>,</span><br><span class="line">     xy123.iloc[(xy123[<span class="string">&#x27;x&#x27;</span>]&lt;<span class="number">1</span>).values,[<span class="number">1</span>,<span class="number">3</span>]])<span class="comment">#条件表达式使用字典方式</span></span><br></pre></td></tr></table></figure><p>除了上述两种方法外，切片访问还可以使用 <code>DataFrame.ix[]</code> 方法进行切片，该方法可以看成是上述切片访问方法的结合。</p><p><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Pandas/6.png" alt="image.png"><br><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Pandas/7.png" alt="image.png"></p><h4 id="查询函数">查询函数</h4><p><code>query()</code>顾名思义，是pandas中专门执行数据查询的API，相较于前边提到的查询函数，<code>query</code>的查询方法更加灵活方便，<code>query</code>函数的基本结构为:<br><code>df.query('条件表达式')</code><br>其中条件表达式内可以直接用列名表示数据的字段名，还可以用index表示行索引，这样就可以完成df中基本的切片操作，除此以外，query函数内部还支持使用外部定义的变量(外部变量需要添加<code>@</code>)，另外可以使用链式表达式,甚至可以直接使用python语句.几点注意:</p><ul><li>传入df的列名若不符合python的命名要求的话不能被正常解析，需要用``括起来</li><li>支持python中的<code>in</code>或者<code>not in</code>操作符</li><li>除了对常规字段进行条件筛选，query()还支持对数据框自身的index进行条件筛选,这种情况在用到的时候再查阅官方文档吧</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data=pd.read_excel(<span class="string">&#x27;./datasets/house_data.xlsx&#x27;</span>)</span><br><span class="line">data.columns</span><br><span class="line">data.query(<span class="string">&#x27;综合评分 == 8.0&#x27;</span>).shape</span><br><span class="line">target_type = [<span class="string">&#x27;简装修&#x27;</span>, <span class="string">&#x27;精装修&#x27;</span>]</span><br><span class="line">data.query(<span class="string">&#x27;装修 in @target_type&amp;所属区==&quot;某区&quot;&#x27;</span>).shape</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>(<span class="params">s</span>):</span><br><span class="line">    <span class="keyword">if</span> s:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data.query(<span class="string">&#x27;装修 in @target_type&amp;户型.apply(@test)&#x27;</span>).shape</span><br></pre></td></tr></table></figure><p>像query这种直接用于查询的api，pandas中还有类似的可以用来添加列的函数<code>eval()</code>，这里不再过多说明，其实使用query函数的另外一个原因是这个函数的查询效率更高</p><h3 id="更改名称">更改名称</h3><p>pd中的一个df一般会有两个位置有名称，一个是轴的名称(axis_name),一个是行或列的名称，两个名称可以在创建df时进行声明，也可以调用方法进行修改:</p><ul><li><code>df.rename_axis(str,axis=0)</code>：修改轴的名称</li><li><code>df.rename(mapper,axis=0/1)</code>:用于修改行或者列标签的名称，mapper指的是一种映射关系，可以写一个字典，也可以引入一个函数(函数的输入参数为要修改的标签的名称),除了指明axis对行或者列标签的名字进行调整以外，还可以写成类似于<code>index=mapper</code>的形式，<strong>默认情况下，mapper匹配不到的值不会报错</strong></li></ul><h3 id="更改-dataframe-中的数据">更改 DataFrame 中的数据</h3><p><strong>更改值</strong></p><ol><li>更改值可以借助访问 DataFrame 的方法对值进行修改。</li><li>也可以通过建立一个 Series 通过赋值运算把两个中索引一致的位置进行修改<br><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Pandas/8.png" alt="image.png"></li></ol><p><strong>添加或者删除行/列</strong></p><ul><li>添加行或者列可以通过直接赋值的方法进行修改</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">xy123.loc[xy123[<span class="string">&#x27;x&#x27;</span>]&lt;=<span class="number">3</span>,<span class="string">&#x27;x&#x27;</span>] = <span class="number">3</span><span class="comment">#更改符合条件的记录的值</span></span><br></pre></td></tr></table></figure><ul><li>删除行或者列需要借助 drop 函数（要调整 inplace 参数，感觉这个函数主要是用来不显示某些列的）。删除列也可以用 del 函数</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">DataFrame.drop(labels=<span class="literal">None</span>,axis=<span class="number">0</span>,</span><br><span class="line">index=<span class="literal">None</span>,columns=<span class="literal">None</span>,level=<span class="literal">None</span>,inplace=Flase,errors=<span class="string">&#x27;raise&#x27;</span>)</span><br><span class="line"><span class="comment">#labels接收单个列名或者多个列名的列表或者列的索引或者行索引。</span></span><br><span class="line"><span class="comment">#inplace表示是否在原DataFrame上进行操作</span></span><br><span class="line"><span class="comment">#axis表示删除的行还是列，默认是0即删除行</span></span><br></pre></td></tr></table></figure><h3 id="sorting-and-ranking">Sorting and Ranking</h3><ul><li>df.sort_index(axis=1,ascending=False)</li><li>df.sort_values(by=[‘column_name1’,‘column_name2’])</li></ul><p>排名使用 rank 方法，默认是通过取排名的平均值来处理排名相同的问题<br><code>df.rank(axis=,ascending=,method=)</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">ser = pd.Series([<span class="number">7</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">1</span>, -<span class="number">8</span>, <span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(ser)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span>*<span class="number">30</span>)</span><br><span class="line"><span class="built_in">print</span>(ser.rank())</span><br><span class="line"><span class="comment"># 0    4.5</span></span><br><span class="line"><span class="comment"># 1    4.5</span></span><br><span class="line"><span class="comment"># 2    6.0</span></span><br><span class="line"><span class="comment"># 3    2.0</span></span><br><span class="line"><span class="comment"># 4    1.0</span></span><br><span class="line"><span class="comment"># 5    3.0</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Pandas/9.png" alt="image.png"></p><h3 id="transposing">Transposing</h3><p>借鉴 NumPy，可以通过 DataFrame.T 实现行列互换。</p><h2 id="标签重命名">标签重命名</h2><ul><li>pd/se 创建时可以指定行列名字</li><li>df.index.names 属性可以用来修改标签名称</li><li>df.rename()方法进行修改（支持传入函数</li></ul><h1 id="索引">索引</h1><h2 id="index-object">Index object</h2><p>pd 的 Index 对象是用来作为保存轴标签的，而且是不可以被改写的！<br>一些方法：<br><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Pandas/10.png" alt="image.png"></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#在创建df时规定列名与行名的一种方法</span></span><br><span class="line">data = pd.DataFrame(np.arange(<span class="number">6</span>).reshape((<span class="number">2</span>, <span class="number">3</span>)),</span><br><span class="line">                    index=pd.Index([<span class="string">&#x27;Ohio&#x27;</span>, <span class="string">&#x27;Colorado&#x27;</span>], name=<span class="string">&#x27;state&#x27;</span>),</span><br><span class="line">                    columns=pd.Index([<span class="string">&#x27;one&#x27;</span>, <span class="string">&#x27;two&#x27;</span>, <span class="string">&#x27;three&#x27;</span>],</span><br><span class="line">                    name=<span class="string">&#x27;number&#x27;</span>))</span><br></pre></td></tr></table></figure><p>除了创建时修改索引实例的名称，还可以通过以下方法进行修改:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df=pd.DataFrame(data,index,column)</span><br><span class="line">df.index.names=</span><br><span class="line">df.columns.names=</span><br></pre></td></tr></table></figure><p>具体传入参数根据列标签和行标签进行适当调整即可（列表或者列表嵌套）。同样的对行的索引方式也支持对列使用。</p><h2 id="多级索引">多级索引</h2><p>多级索引提供了一种以一个较低维度的形式访问高维数据的方法，每次一个维度的索引都相当于对原数据进行一次降维。多级索引建立与单个索引相似，只需将每一级各个值对应的索引名称传给 index 参数即可，每一级的索引单独组成一个列表，传入 index 的参数应为列表的嵌套。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.Series([<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">                 [</span><br><span class="line">    [<span class="string">&#x27;Grade One&#x27;</span>, <span class="string">&#x27;Grade Two&#x27;</span>, <span class="string">&#x27;Grade Two&#x27;</span>, <span class="string">&#x27;Grade Two&#x27;</span>,</span><br><span class="line">     <span class="string">&#x27;Grade Three&#x27;</span>, <span class="string">&#x27;Grade Three&#x27;</span>, <span class="string">&#x27;Grade Three&#x27;</span>],</span><br><span class="line">    [<span class="string">&#x27;xiaoming&#x27;</span>, <span class="string">&#x27;xiaoming&#x27;</span>, <span class="string">&#x27;xiaohong&#x27;</span>, <span class="string">&#x27;xiaowang&#x27;</span>,</span><br><span class="line">     <span class="string">&#x27;xiaoming&#x27;</span>, <span class="string">&#x27;xiaohong&#x27;</span>, <span class="string">&#x27;xiaowang&#x27;</span>]</span><br><span class="line">],</span><br><span class="line">    name=<span class="string">&#x27;Student_Info&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(data.index)</span><br><span class="line"><span class="built_in">print</span>(data[:, <span class="string">&#x27;xiaoming&#x27;</span>])</span><br><span class="line">data.index.names=[<span class="string">&#x27;Grade&#x27;</span>, <span class="string">&#x27;Student_Name&#x27;</span>]</span><br></pre></td></tr></table></figure><h3 id="multiindex-创建">MultiIndex 创建</h3><p>可以利用 pd 的一些方法来创建一个多级索引对象，可以作为参数 index 的传入值：</p><ul><li><code>pd.MultiIndex.from_arrays</code>：创建方式类似于 zip 函数、</li><li><code>pd.MultiIndex.from_product</code>:有点像 for 循环式的嵌套</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pd.MultiIndex.from_arrays([[<span class="string">&#x27;Ohio&#x27;</span>, <span class="string">&#x27;Ohio&#x27;</span>, <span class="string">&#x27;Colorado&#x27;</span>], [<span class="string">&#x27;Green&#x27;</span>, <span class="string">&#x27;Red&#x27;</span>, <span class="string">&#x27;Green&#x27;</span>]],</span><br><span class="line">                       names=[<span class="string">&#x27;state&#x27;</span>, <span class="string">&#x27;color&#x27;</span>])</span><br><span class="line">pd.MultiIndex.from_product([[<span class="string">&#x27;Ohio&#x27;</span>, <span class="string">&#x27;Ohio&#x27;</span>, <span class="string">&#x27;Colorado&#x27;</span>], [<span class="string">&#x27;Green&#x27;</span>, <span class="string">&#x27;Red&#x27;</span>, <span class="string">&#x27;Green&#x27;</span>]],</span><br><span class="line">                       names=[<span class="string">&#x27;state&#x27;</span>, <span class="string">&#x27;color&#x27;</span>])</span><br></pre></td></tr></table></figure><h3 id="reordering-and-sorting-levels">Reordering and Sorting levels</h3><p>多级标签下的每一级算一个 level，pd 提供了对 level 的重新排列和对任一 level 的排序</p><ul><li>df/series.swaplevel():传递参数为两个 level 的数字或者是名称</li><li>df/series.sort_index(level=):按照任意 level 排序</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = pd.Series([<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">                 [</span><br><span class="line">    [<span class="string">&#x27;Grade One&#x27;</span>, <span class="string">&#x27;Grade Two&#x27;</span>, <span class="string">&#x27;Grade Two&#x27;</span>, <span class="string">&#x27;Grade Two&#x27;</span>,</span><br><span class="line">     <span class="string">&#x27;Grade Three&#x27;</span>, <span class="string">&#x27;Grade Three&#x27;</span>, <span class="string">&#x27;Grade Three&#x27;</span>],</span><br><span class="line">    [<span class="string">&#x27;xiaoming&#x27;</span>, <span class="string">&#x27;xiaoming&#x27;</span>, <span class="string">&#x27;xiaohong&#x27;</span>, <span class="string">&#x27;xiaowang&#x27;</span>,</span><br><span class="line">     <span class="string">&#x27;xiaoming&#x27;</span>, <span class="string">&#x27;xiaohong&#x27;</span>, <span class="string">&#x27;xiaowang&#x27;</span>]</span><br><span class="line">],</span><br><span class="line">    name=<span class="string">&#x27;Student_ID&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(data.index)</span><br><span class="line">data.index.names=[<span class="string">&#x27;Grade&#x27;</span>,<span class="string">&#x27;Stu_Name&#x27;</span>]</span><br><span class="line"><span class="built_in">print</span>(data.swaplevel(<span class="string">&#x27;Stu_Name&#x27;</span>,<span class="string">&#x27;Grade&#x27;</span>))</span><br><span class="line">data.sort_index(level=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>除了 sort_index,还要很多其他函数也支持按照某一个 level 进行函数操作，只需要在原来函数的基础上传入一个 level 参数即可(df.sum)。<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/74461994">多层索引的更多应用</a></p><h2 id="索引重置">索引重置</h2><p>索引重置主要说的是索引调整(数目和顺序的调整)以及层次的调整(列取值变为行索引)。<br>pd 一个重要的方法是 <code>reindex()</code>,可以用来重新定义行/列索引的顺序以及内容(也可以用来增加新的index，该列或者行的值可以按照某种规则填充)：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">data = pd.DataFrame(&#123;<span class="string">&#x27;Height&#x27;</span>: [<span class="number">1.7</span>, <span class="number">1.8</span>, <span class="number">1.9</span>],</span><br><span class="line">                     <span class="string">&#x27;Weight&#x27;</span>: [<span class="number">140</span>, <span class="number">160</span>, <span class="number">180</span>]&#125;,</span><br><span class="line">                    index=[<span class="string">&quot;xiaoming&quot;</span>, <span class="string">&quot;xiaohong&quot;</span>, <span class="string">&quot;xiaowang&quot;</span>])</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="built_in">print</span>(data.reindex([<span class="string">&quot;xiaowang&quot;</span>,<span class="string">&quot;xiaoming&quot;</span>,<span class="string">&quot;xiaozhang&quot;</span>]))</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>对于数值类索引，在进行 reindex 时还可以进行缺失值的填充，一个方法是’ffill’(“forward-fills”)，实现对缺失索引的前向填充：<br><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Pandas/11.png" alt="image.png"></p><p><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Pandas/12.png" alt="image.png"></p><p>一般来说，我们很少使用 df 的多级列标签，更多的情况是将<strong>列标签转化为行标签</strong>，这时就可以借助 <code>df.set_index</code> 方法：</p><ul><li>drop:Bool,决定将列标签设置为行标签时原来的列标签是否保留</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">frame = pd.DataFrame(&#123;<span class="string">&#x27;a&#x27;</span>: <span class="built_in">range</span>(<span class="number">7</span>), <span class="string">&#x27;b&#x27;</span>: <span class="built_in">range</span>(<span class="number">7</span>, <span class="number">0</span>, -<span class="number">1</span>),</span><br><span class="line">                      <span class="string">&#x27;c&#x27;</span>: [<span class="string">&#x27;one&#x27;</span>, <span class="string">&#x27;one&#x27;</span>, <span class="string">&#x27;one&#x27;</span>, <span class="string">&#x27;two&#x27;</span>, <span class="string">&#x27;two&#x27;</span>,</span><br><span class="line">                            <span class="string">&#x27;two&#x27;</span>, <span class="string">&#x27;two&#x27;</span>],</span><br><span class="line">                      <span class="string">&#x27;d&#x27;</span>: [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]&#125;)</span><br><span class="line">frame</span><br><span class="line">frame2 = frame.set_index([<span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;d&#x27;</span>])</span><br><span class="line">frame2</span><br></pre></td></tr></table></figure><p>与该操作进行相反作用的函数是<code>df.reset_index</code></p><h3 id="重复标签下的轴索引">重复标签下的轴索引</h3><p>对重复标签的索引的返回值会是一个 Series，这会使得我们的代码变得复杂</p><h2 id="索引重塑">索引重塑</h2><p>多级标签的重塑主要借助 stack 和 unstack 方法：</p><ul><li>stack：This “rotates” or pivots from the columns in the data to the rows(列值变为行索引)</li><li>unstack：This pivots from the rows into the columns(行索引变为列取值)<br>两个函数默认都从最低level开始操作，然后将转换为另外一个轴的最低层级，可以传入 df 的层级名称或者数字来强制修改操作层级，另外就是堆叠数据（<code>stack</code>）的时候默认是删除缺失值的，可以通过调节 dropna 参数进行调整。<br>series只有<code>unstack()</code>方法，df同时有<code>stack()</code>和<code>unstack()</code>方法来转变为一个Series，两者的区别是原df所对应的index处于最低level还是最高level，转化成功的df列名变成index，列取值变为对应Series的值。</li></ul><p>另外对于<strong>二级索引</strong>的 series，还可以借助 <code>series.unstack()</code>方法将二级索引拆成一个 dataframe，同样的也可以借助 <code>stack</code> 方法将一个 df 转化为一个 series（inverse operation)。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = pd.DataFrame(np.arange(<span class="number">6</span>).reshape((<span class="number">2</span>, <span class="number">3</span>)),</span><br><span class="line">                    index=pd.Index([<span class="string">&#x27;Ohio&#x27;</span>, <span class="string">&#x27;Colorado&#x27;</span>], name=<span class="string">&#x27;state&#x27;</span>),</span><br><span class="line">                    columns=pd.Index([<span class="string">&#x27;one&#x27;</span>, <span class="string">&#x27;two&#x27;</span>, <span class="string">&#x27;three&#x27;</span>],</span><br><span class="line">                    name=<span class="string">&#x27;number&#x27;</span>))</span><br><span class="line">result = data.stack()</span><br><span class="line">df = pd.DataFrame(&#123;<span class="string">&#x27;left&#x27;</span>: result, <span class="string">&#x27;right&#x27;</span>: result + <span class="number">5</span>&#125;,</span><br><span class="line">                  columns=pd.Index([<span class="string">&#x27;left&#x27;</span>, <span class="string">&#x27;right&#x27;</span>], name=<span class="string">&#x27;side&#x27;</span>))</span><br><span class="line">df</span><br><span class="line">df.drop(index=(<span class="string">&#x27;Ohio&#x27;</span>,<span class="string">&#x27;three&#x27;</span>),inplace=<span class="literal">True</span>)</span><br><span class="line"><span class="comment">#df.drop(index=[&#x27;three&#x27;],level=&#x27;number&#x27;,inplace=True)#会删除二级索引为number的所有行</span></span><br><span class="line">df</span><br><span class="line">df.unstack(<span class="string">&#x27;number&#x27;</span>)</span><br><span class="line">df.unstack(<span class="string">&#x27;state&#x27;</span>)</span><br><span class="line">df.unstack(<span class="string">&#x27;state&#x27;</span>).stack(<span class="string">&#x27;side&#x27;</span>)</span><br><span class="line">df.unstack(<span class="string">&#x27;state&#x27;</span>).stack(<span class="string">&#x27;side&#x27;</span>,dropna=<span class="literal">False</span>)</span><br><span class="line">df.unstack(<span class="string">&#x27;state&#x27;</span>).drop(columns=<span class="string">&#x27;Ohio&#x27;</span>,axis=<span class="number">1</span>,level=<span class="number">1</span>)</span><br><span class="line">df.unstack(<span class="string">&#x27;state&#x27;</span>).drop(columns=<span class="string">&#x27;Ohio&#x27;</span>,axis=<span class="number">1</span>,level=<span class="number">1</span>).stack(<span class="string">&#x27;side&#x27;</span>)</span><br><span class="line">df.unstack(<span class="string">&#x27;state&#x27;</span>).drop(columns=<span class="string">&#x27;Ohio&#x27;</span>,axis=<span class="number">1</span>,level=<span class="number">1</span>).stack(<span class="string">&#x27;side&#x27;</span>,dropna=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h2 id="查找特定值">查找特定值</h2><ul><li>df/ser.isin（list）:返回布尔值</li><li>pd.index(list).get_indexer(to_match):根据 to_match 的情况返回一个对 list 的<strong>索引</strong>，值为 list 的索引值</li><li><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Pandas/13.png" alt="image.png"></li></ul><h1 id="分组">分组</h1><p><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Pandas/39.png" alt="image.png"><br>Pandas 提供了 DataFrame.groupby()方法，按照指定的分组键，将具有相同键值的记录划分为同一组，将具有不同键值的记录划分到不同组，并对各组进行统计计算。其格式如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">DataFrame.groupby(by=<span class="literal">None</span>, axis=<span class="number">0</span>, level=<span class="literal">None</span>, as_index=<span class="literal">True</span>,</span><br><span class="line">sort=<span class="literal">True</span>, group_keys=<span class="literal">True</span>, squeeze=<span class="literal">False</span>, observed=<span class="literal">False</span>, **kwargs)</span><br></pre></td></tr></table></figure><p>其中 by 接收分组键。<code>DataFrame.groupby()</code>返回一个称为<strong>GroupBy object</strong>的对象。实际上分组后的数据对象 GroupBy 类似 Series 与 DataFrame，是 pandas 提供的一种对象。<br>python 中可以作为分组键的类型：</p><ul><li>列名</li><li>和分组数据等长的数组或者列表</li><li>一个指明<strong>分组名称</strong>和<strong>分组值关系</strong>的字典或者 series</li><li>A function to be invoked on the axis index or the individual labels in the index。</li></ul><ol><li>利用函数进行分类需要注意的是传入参数是df的行索引，目前我觉得使用这个自定义函数分类的方法主要是使用<code>loc(x,)</code>方法获得所需的列来进行运算</li><li>分组的操作轴默认为 axis=0,也可以进行调整</li><li>对于多级标签的对象，也可以指定 level 参数</li><li>调整 as_index 参数返回不带行标签的索引结果（取消两个及以上分组键的分组结果的多级索引）</li><li>调整 group_keys 参数，决定是否显示分组键索引</li><li>一般用分组键的取值作为行索引，如果是传入一个函数用来分组，那么默认借助函数的返回值作为索引。</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = pd.DataFrame(&#123;<span class="string">&#x27;key1&#x27;</span> : [<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;a&#x27;</span>],</span><br><span class="line">                   <span class="string">&#x27;key2&#x27;</span> : [<span class="string">&#x27;one&#x27;</span>, <span class="string">&#x27;two&#x27;</span>, <span class="string">&#x27;one&#x27;</span>, <span class="string">&#x27;two&#x27;</span>, <span class="string">&#x27;one&#x27;</span>],</span><br><span class="line">                   <span class="string">&#x27;data1&#x27;</span> : np.random.randn(<span class="number">5</span>),</span><br><span class="line">                   <span class="string">&#x27;data2&#x27;</span> : np.random.randn(<span class="number">5</span>)&#125;)</span><br><span class="line"><span class="comment">#列名</span></span><br><span class="line">df.groupby(<span class="string">&#x27;key1&#x27;</span>).first()</span><br><span class="line"><span class="comment">#等长的series作为分组键</span></span><br><span class="line">df[<span class="string">&#x27;data1&#x27;</span>].groupby(df[<span class="string">&#x27;key1&#x27;</span>]).first()</span><br><span class="line"><span class="comment">#列表或者数组</span></span><br><span class="line">df[<span class="string">&#x27;data1&#x27;</span>].groupby([df[<span class="string">&#x27;key1&#x27;</span>],df[<span class="string">&#x27;key2&#x27;</span>]]).first()</span><br><span class="line">states = np.array([<span class="string">&#x27;Ohio&#x27;</span>, <span class="string">&#x27;California&#x27;</span>, <span class="string">&#x27;California&#x27;</span>, <span class="string">&#x27;Ohio&#x27;</span>, <span class="string">&#x27;Ohio&#x27;</span>])</span><br><span class="line">years = np.array([<span class="number">2005</span>, <span class="number">2005</span>, <span class="number">2006</span>, <span class="number">2005</span>, <span class="number">2006</span>])</span><br><span class="line">df[<span class="string">&#x27;data1&#x27;</span>].groupby([states,  years]).first()</span><br><span class="line"><span class="comment">#Grouping with Dicts and Series</span></span><br><span class="line">people = pd.DataFrame(np.random.randn(<span class="number">5</span>, <span class="number">5</span>),</span><br><span class="line">                      columns=[<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;e&#x27;</span>],</span><br><span class="line">                      index=[<span class="string">&#x27;Joe&#x27;</span>, <span class="string">&#x27;Steve&#x27;</span>, <span class="string">&#x27;Wes&#x27;</span>, <span class="string">&#x27;Jim&#x27;</span>, <span class="string">&#x27;Travis&#x27;</span>])</span><br><span class="line">people.iloc[<span class="number">2</span>:<span class="number">3</span>, [<span class="number">1</span>, <span class="number">2</span>]] = np.nan <span class="comment"># Add a few NA values</span></span><br><span class="line">people</span><br><span class="line">mapping = &#123;<span class="string">&#x27;a&#x27;</span>: <span class="string">&#x27;red&#x27;</span>, <span class="string">&#x27;b&#x27;</span>: <span class="string">&#x27;red&#x27;</span>, <span class="string">&#x27;c&#x27;</span>: <span class="string">&#x27;blue&#x27;</span>,</span><br><span class="line">           <span class="string">&#x27;d&#x27;</span>: <span class="string">&#x27;blue&#x27;</span>, <span class="string">&#x27;e&#x27;</span>: <span class="string">&#x27;red&#x27;</span>, <span class="string">&#x27;f&#x27;</span> : <span class="string">&#x27;orange&#x27;</span>&#125;</span><br><span class="line">by_column = people.groupby(mapping, axis=<span class="number">1</span>)</span><br><span class="line">by_column.<span class="built_in">sum</span>()</span><br><span class="line">map_series = pd.Series(mapping)</span><br><span class="line">map_series</span><br><span class="line">people.groupby(map_series, axis=<span class="number">1</span>).count()</span><br><span class="line"><span class="comment">#Grouping with Functions</span></span><br><span class="line">people.groupby(<span class="built_in">len</span>).<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">columns = pd.MultiIndex.from_arrays([[<span class="string">&#x27;US&#x27;</span>, <span class="string">&#x27;US&#x27;</span>, <span class="string">&#x27;US&#x27;</span>, <span class="string">&#x27;JP&#x27;</span>, <span class="string">&#x27;JP&#x27;</span>],</span><br><span class="line">                                    [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">3</span>]],</span><br><span class="line">                                    names=[<span class="string">&#x27;cty&#x27;</span>, <span class="string">&#x27;tenor&#x27;</span>])</span><br><span class="line">hier_df = pd.DataFrame(np.random.randn(<span class="number">4</span>, <span class="number">5</span>), columns=columns)</span><br><span class="line">hier_df</span><br><span class="line">hier_df.groupby(level=<span class="string">&#x27;cty&#x27;</span>, axis=<span class="number">1</span>).count()</span><br></pre></td></tr></table></figure><p>需要注意的是，分组键的值为空的数据将会被移出。</p><h2 id="groupby-object">Groupby object</h2><p>分组后生成的对象支持迭代，默认一个迭代对象是两个元组，分别包含组名和数据。元组的具体情况要根据分组的情况而定（分组键的数量之类的）。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (k1, k2), group <span class="keyword">in</span> df.groupby([<span class="string">&#x27;key1&#x27;</span>, <span class="string">&#x27;key2&#x27;</span>]):</span><br><span class="line">    <span class="built_in">print</span>((k1, k2))</span><br><span class="line">    <span class="built_in">print</span>(group)</span><br><span class="line"><span class="comment">#将分组结果转化为字典形式的方法</span></span><br><span class="line">pieces = <span class="built_in">dict</span>(<span class="built_in">list</span>(df.groupby(<span class="string">&#x27;key1&#x27;</span>)))</span><br><span class="line">pieces[<span class="string">&#x27;b&#x27;</span>]</span><br></pre></td></tr></table></figure><p>实例的属性:</p><ul><li><code>groupby.groups</code>：返回每组中数据的索引，字典类型。可以使用<code>get_group('group name')</code>方法返回组名为 group name 的全部记录。<br>分组后的对象其实可以视作一个新的 df 或者 se(SeriesGroupBy object)，名字即为分组键的值（如果是通过传递函数进行分组那么索引值就是函数的返回值），当数据集比较大时，我们有时候只希望对分组结果的部分列进行运算，因此可以写成类似于下边的形式：<br><code>df.groupby(['key1', 'key2'])[['data2']].mean()</code></li></ul><p>分组后可以进行的操作：</p><ul><li>描述性统计分析（见描述性统计分析）</li><li>聚合运算</li></ul><h2 id="使用-groupby-进行描述性统计">使用 GroupBy 进行描述性统计</h2><ul><li>对分组结果 GroupBy object 的描述性统计<ul><li><code>GroupBy object.count()</code>——返回每组记录数量，包括缺失值。</li><li><code>GroupBy object.max()</code>——返回组内最大值。</li><li><code>GroupBy object.min()</code>——返回组内最小值。</li><li><code>GroupBy object.sum()</code>——返回每组的和。</li><li><code>GroupBy object.mean()</code>——返回每组的均值。</li><li><code>GroupBy object.std()</code>——返回每组的标准差。</li><li><code>GroupBy object.median()</code>——返回每组的中位数。</li><li><code>GroupBy object.size()</code>——返回每组的大小。</li></ul></li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#例4-10 对汽车销售数据表进行分组聚合，观察各个描述性统计</span></span><br><span class="line">vs[<span class="string">&#x27;date&#x27;</span>]=pd.to_datetime(vs[<span class="string">&#x27;date&#x27;</span>])<span class="comment">#将&#x27;date&#x27;转换成日期型</span></span><br><span class="line"><span class="comment">#按照日期进行分组</span></span><br><span class="line">vsGroup = vs.groupby(by=<span class="string">&#x27;date&#x27;</span>)</span><br><span class="line"><span class="comment">#各个特征使用相同的函数统计计算</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;汽车销售数据表按日期分组后前5组每组的数量为：\n&#x27;</span>,</span><br><span class="line">      vsGroup.count().head())</span><br></pre></td></tr></table></figure><h1 id="基本用法">基本用法</h1><h2 id="pandas-读写文件">Pandas 读写文件</h2><p>Pd 提供了许多读写结构化数据的为 df 的函数：<br><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Pandas/14.png" alt="image.png"><br><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Pandas/15.png" alt="image.png"><br><strong>由于pd特殊的数据结构，在读写或者保存数据时需要注意的是一定要声明索引，不然它会使用默认的索引，这也意味着当我们将有默认索引的df进行保存时，也会将默认索引保存进数据文件中，这点一定要注意</strong>。</p><h2 id="算术运算和数据对齐">算术运算和数据对齐</h2><p>pd 最重要的一个功能是可以对不同索引的对象进行算术运算。以加法为例，它会匹配索引相同（行和列）的进行算术运算，再将索引不匹配的数据视作缺失值，但是也会添加到最后的运算结果中，从而组成加法运算的结果。</p><ul><li>如果想给缺失值赋予自己想要的值，则需要利用方法，以 add 为例<br><code>df1.add(df2,fill_value=0)</code></li></ul><p><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Pandas/21.png" alt="image.png"><br>r 表示翻转参数</p><h3 id="df-和-ser-之间的算术运算">Df 和 Ser 之间的算术运算</h3><p>与数组的不同维度的数组进行算术运算的方法相似，pd 会将 df 拆成 n 个一维的分别与 ser 进行匹配然后进行算术运算</p><blockquote><p>By default, arithmetic between DataFrame and Series matches the index of the Series<br>on the DataFrame’s columns, broadcasting down the rows:</p></blockquote><p>如果有不匹配的索引，那么将会重新进行索引来形成一个联合：<br><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Pandas/22.png" alt="image.png"><br><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Pandas/23.png" alt="image.png"><br>想要改变逐行进行匹配的广播机制，需要借助<code>df.sub(ser,axis='index')</code>方法：<br><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Pandas/24.png" alt="image.png"></p><h3 id="数学运算">数学运算</h3><p>Numpy 基于元素的公式运算对于 pd 也适用</p><ul><li>np.abs(df)</li><li>df.apply(f,axis=’’)</li></ul><h2 id="数据转换">数据转换</h2><h3 id="数据转换">数据转换</h3><p>数据转换主要是通过定义一些函数来实现映射，下边介绍一种使用 map 函数的数据转换方法：</p><blockquote><p>The map method on a Series accepts a function or dict-like object containing a map‐<br>ping</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = pd.DataFrame(&#123;<span class="string">&#x27;food&#x27;</span>: [<span class="string">&#x27;bacon&#x27;</span>, <span class="string">&#x27;pulled pork&#x27;</span>, <span class="string">&#x27;bacon&#x27;</span>,</span><br><span class="line">                              <span class="string">&#x27;Pastrami&#x27;</span>, <span class="string">&#x27;corned beef&#x27;</span>, <span class="string">&#x27;Bacon&#x27;</span>,</span><br><span class="line">                              <span class="string">&#x27;pastrami&#x27;</span>, <span class="string">&#x27;honey ham&#x27;</span>, <span class="string">&#x27;nova lox&#x27;</span>],</span><br><span class="line">                     <span class="string">&#x27;ounces&#x27;</span>: [<span class="number">4</span>, <span class="number">3</span>, <span class="number">12</span>, <span class="number">6</span>, <span class="number">7.5</span>, <span class="number">8</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">6</span>]&#125;)</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line">meat_to_animal = &#123;</span><br><span class="line">    <span class="string">&#x27;bacon&#x27;</span>: <span class="string">&#x27;pig&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;pulled pork&#x27;</span>: <span class="string">&#x27;pig&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;pastrami&#x27;</span>: <span class="string">&#x27;cow&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;corned beef&#x27;</span>: <span class="string">&#x27;cow&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;honey ham&#x27;</span>: <span class="string">&#x27;pig&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;nova lox&#x27;</span>: <span class="string">&#x27;salmon&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">lowercased = data[<span class="string">&#x27;food&#x27;</span>].<span class="built_in">str</span>.lower()</span><br><span class="line">lowercased</span><br><span class="line">data[<span class="string">&#x27;animal&#x27;</span>] = lowercased.<span class="built_in">map</span>(meat_to_animal)</span><br><span class="line">data</span><br><span class="line">data[<span class="string">&#x27;food&#x27;</span>].<span class="built_in">map</span>(<span class="keyword">lambda</span> x: meat_to_animal[x.lower()])</span><br></pre></td></tr></table></figure><p>同样的方法，也可以用来对 df 的轴标签进行重新索引，只不过操作对象变成了 df.index</p><h3 id="df-replace">df.replace()</h3><p>df.replace()主要接受两个参数，第一个参数表示被替换值，第二个参数表示替换值，这两个参数可以是两个等长的列表（一一匹配），亦可以是一个字典键值对匹配即可。<br>在多数情况下，对时间类型数据进行分析的前提就是将原本为字符串的时间转换为标准时间类型。pandas 继承了 NumPy 库和 datetime 库的时间相关模块，提供了 6 种时间相关的类。</p><h3 id="df-str-replace">df.str.replace</h3><blockquote><p>The data.replace method is distinct from data.str.replace, which performs string substitution element-wise. （按元素执行字符串匹配）We look at these string methods on Series later in the chapter.</p></blockquote><h2 id="时间序列数据">时间序列数据</h2><blockquote><p>A basic kind of time series object in pandas is a <strong>Series</strong> indexed by timestamps, which is often represented external to pandas as Python strings or datetime objects</p></blockquote><h3 id="创建">创建</h3><p>pd 的<code>to_datetime</code>能够将字符串解析为时间对象，并会将缺失值记作‘NAT’，该函数解析之后会返回一个 timestamp 对象，该对象的</p><blockquote><p>NaT (Not a Time) is pandas’s null value for timestamp data.</p></blockquote><h4 id="timestamp-时间点">Timestamp–时间点</h4><p>多个 timestamp 对象储存在一个 series 或者 df 或者列表中时，这些对象是通过 datetimeindex 组织起来的。</p><h5 id="常用属性">常用属性</h5><ul><li>在多数涉及时间相关的数据处理，统计分析的过程中，需要提取时间中的年份，月份等数据。使用对应的 Timestamp 类属性就能够实现这一目的。</li><li>结合 Python 列表推导式，可以实现对 DataFrame 某一列时间信息数据的提取</li></ul><p><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Pandas/25.png" alt="image.png"></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">year1 = [i.year <span class="keyword">for</span> i <span class="keyword">in</span> order[<span class="string">&#x27;lock_time&#x27;</span>]]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;lock_time中的年数据前5个为：&#x27;</span>,year1[:<span class="number">5</span>])</span><br><span class="line">month1 = [i.month <span class="keyword">for</span> i <span class="keyword">in</span> order[<span class="string">&#x27;lock_time&#x27;</span>]]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;lock_time中的月数据前5个为：&#x27;</span>,month1[:<span class="number">5</span>])</span><br><span class="line">day1 = [i.day <span class="keyword">for</span> i <span class="keyword">in</span> order[<span class="string">&#x27;lock_time&#x27;</span>]]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;lock_time中的日数据前5个为：&#x27;</span>,day1[:<span class="number">5</span>])</span><br><span class="line">dayname1 = [i.day_name <span class="keyword">for</span> i <span class="keyword">in</span> order[<span class="string">&#x27;lock_time&#x27;</span>]]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;lock_time中的日期数据前5个为：&#x27;</span>,dayname1[:<span class="number">5</span>])</span><br></pre></td></tr></table></figure><h4 id="datetimeindex-与-periodindex-函数">DatetimeIndex 与 PeriodIndex 函数</h4><p>除了将数据字原始 DataFrame 中直接转换为 Timestamp 格式外，还可以将数据<strong>单独提取出来</strong>将其转换为 DatetimeIndex 或者 PeriodIndex。DatetimeIndex 是用来指代一系列时间点的一种数据结构，而 PeriodIndex 则是用来指代一系列时间段的数据结构。<br>转换为 PeriodIndex 的时候需要注意，需要通过<strong>freq 参数</strong>指定时间间隔，常用的时间间隔有 Y 为年，M 为月，D 为日，H 为小时，T 为分钟，S 为秒。两个函数可以用来转换数据还可以用来创建时间序列数据，其参数非常类似。</p><p><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Pandas/26.png" alt="image.png"><br><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Pandas/27.png" alt="image.png"></p><h4 id="timedelta-不同单位的时间">Timedelta–不同单位的时间</h4><p>Timedelta 是时间相关的类中的一个异类，不仅能够使用正数，还能够使用负数表示单位时间，例如 1 秒，2 分钟，3 小时等。使用 Timedelta 类，配合常规的时间相关类能够轻松实现时间的算术运算。目前 Timedelta 函数中时间周期中没有年和月。所有周期名称，对应单位及其说明如下表所示。<br><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Pandas/28.png" alt="image.png"><br><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Pandas/29.png" alt="image.png"></p><h4 id="timedeltaindex">TimedeltaIndex</h4><p>一组 Timedelta 构成的 Index，可以用来作为 Series 或者 DataFrame 的索引</p><h3 id="访问">访问</h3><p>时间序列数据的访问其实可以参考 pandas 的 series 的访问方式，既可以使用 se.index[2]获取行索引的值进行访问，也可以直接调用行索引值进行访问，不过比较方便的是，索引值可以是一个可以被翻译为日期的字符串（功能比较灵活，甚至可以输入年份的字符串匹配所有符合年份的数据，也允许使用：作为索引标签）</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> IPython.core.interactiveshell <span class="keyword">import</span> InteractiveShell</span><br><span class="line">InteractiveShell.ast_node_interactivity = <span class="string">&#x27;all&#x27;</span></span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line">dates = [datetime(<span class="number">2011</span>, <span class="number">1</span>, <span class="number">2</span>), datetime(<span class="number">2011</span>, <span class="number">1</span>, <span class="number">5</span>),</span><br><span class="line">         datetime(<span class="number">2011</span>, <span class="number">1</span>, <span class="number">7</span>), datetime(<span class="number">2011</span>, <span class="number">1</span>, <span class="number">8</span>),</span><br><span class="line">         datetime(<span class="number">2011</span>, <span class="number">1</span>, <span class="number">10</span>), datetime(<span class="number">2011</span>, <span class="number">1</span>, <span class="number">12</span>)]</span><br><span class="line">ts = pd.Series(np.random.randn(<span class="number">6</span>), index=dates)</span><br><span class="line">ts</span><br><span class="line">stamp = ts.index[<span class="number">2</span>]</span><br><span class="line">ts[stamp]</span><br><span class="line">ts[<span class="string">&#x27;1/10/2011&#x27;</span>]</span><br><span class="line">ts[<span class="string">&#x27;20110110&#x27;</span>]</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Because most time series data is ordered chronologically, you can slice with time‐</span></span><br><span class="line"><span class="string">stamps not contained in a time series to perform a range query:</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">ts[datetime(<span class="number">2011</span>, <span class="number">1</span>, <span class="number">7</span>):]</span><br></pre></td></tr></table></figure><p>需要注意的是切片访问相当于在源时间序列上创建一个新的 view（和 numpy 一样的），言外之意是并没有 copy 数据，任何对 view 的修改会作用于原来的数据。<br>截断访问：timeseries.truncate:<br><code>ts.truncate(after='1/9/2011'</code></p><h3 id="连续序列产生-frequencies">连续序列产生&amp;Frequencies</h3><p><code>pd.date_range(start, periods=100, freq,normalize=False)</code>,‘periods’参数用来指定生成的长度，一般在 start 或者 end 缺失时会用到(该函数默认按照天为间隔生成 DatetimeIndex 对象)。<br><code>freq</code>的可选参数有:<br><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Pandas/2022-04-15-19-58-23.png" alt=""><br>freq 参数传入的参数除了上述这种形式，还可以在基础的时间频率的基础上加一些数字，例如’4H’(Putting an integer before the base frequency creates a multiple:).还可以传入一下类似于’1h30min’的字符串，这个也可以被很好的解析。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pd.date_range(<span class="string">&#x27;2000-01-01&#x27;</span>, <span class="string">&#x27;2000-01-03 23:59&#x27;</span>, freq=<span class="string">&#x27;4h&#x27;</span>)</span><br><span class="line"><span class="comment">#week of chance enables you to get dates like the third Friday of each month</span></span><br><span class="line">rng = pd.date_range(<span class="string">&#x27;2012-01-01&#x27;</span>, <span class="string">&#x27;2012-09-01&#x27;</span>, freq=<span class="string">&#x27;WOM-3FRI&#x27;</span>)</span><br><span class="line"><span class="built_in">list</span>(rng)</span><br></pre></td></tr></table></figure><h3 id="指定频率的时间序列生成-频率转换和重采样">指定频率的时间序列生成(频率转换和重采样)</h3><p>pandas 支持处理在格式上间隔不相等的时间序列数据，但是有的时候我们希望生成或者转化成一些间隔相同时间序列数据。</p><blockquote><p>Fortunately pandas has a full suite of standard time series frequencies and tools for resampling, inferring frequencies, and generating fixed-frequency date ranges. For example, you can convert the sample time series to be fixed daily frequency by calling <strong>resample</strong>.</p></blockquote><h3 id="frequencies-and-date-offsets">Frequencies and Date Offsets</h3><p>对于基本的时间间隔，pandas 都提供了一个基础的 frequency，然后这些基础的 frequency 还可以借助乘法器组成 pd 里常用的一些 frequency。</p><blockquote><p>For each base frequency, there is an object defined generally referred to as a <strong>date offset</strong></p></blockquote><p>不均匀的间隔被叫做 anchored offsets。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pandas.tseries.offsets <span class="keyword">import</span> Hour, Minute</span><br><span class="line">hour = Hour()</span><br><span class="line">hour</span><br><span class="line"><span class="comment">#define a multiple of an offset by passing an integer</span></span><br><span class="line">four_hours = Hour(<span class="number">4</span>)</span><br><span class="line">four_hours</span><br></pre></td></tr></table></figure><p>pd 的这些 date offsets 也可以用在 datetime 和 timestamp 对象身上。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pandas.tseries.offsets <span class="keyword">import</span> Day, MonthEnd</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">If you add an anchored offset like MonthEnd, the first increment will “roll forward” a</span></span><br><span class="line"><span class="string">date to the next date according to the frequency rule:</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">now + MonthEnd()</span><br><span class="line">now + MonthEnd(<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>感觉上边运算方式多多少少有些奇怪，这时可以考虑 anchored offset 的两个方法<code>rollforward</code>和<code>rollback</code>来 roll dates bakward and forward。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">offset = MonthEnd()</span><br><span class="line">offset.rollforward(now)</span><br><span class="line">offset.rollback(now)</span><br><span class="line"><span class="comment">#创新性地用在groupby中</span></span><br><span class="line">ts = pd.Series(np.random.randn(<span class="number">20</span>),</span><br><span class="line">               index=pd.date_range(<span class="string">&#x27;1/15/2000&#x27;</span>, periods=<span class="number">20</span>, freq=<span class="string">&#x27;4d&#x27;</span>))</span><br><span class="line">ts</span><br><span class="line">ts.groupby(offset.rollforward).mean()</span><br><span class="line">ts.resample(<span class="string">&#x27;M&#x27;</span>).mean()</span><br></pre></td></tr></table></figure><h4 id="period-时间跨度或时间段">Period–时间跨度或时间段</h4><blockquote><p>Periods represent timespans, like days, months, quarters, or years. The Period class represents this data type, requiring a string or integer and a frequency from Folloing Table.</p></blockquote><p><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Pandas/2022-04-18-00-00-57.png" alt=""></p><p>You have two ways to create a Period Object:</p><ul><li>利用<code>pd.period(,freq)</code># 第一个参数可以是整数也可以是字符串，也可以是一个 frequency</li><li>利用<code>pd.period_range()</code>函数生成一个 Period 序列</li></ul><p>Period 对象支持数学运算(可以直接加减整数，感觉可以看做一个相同 freq 的对象)，如果两个对象的 frequency 相同的话，他们的差则会是整数(the number of units between them)</p><h4 id="periodtimeindex">PeriodtimeIndex</h4><p>借助<code>pd.period_range()</code>或者<code>pd.PeriodIndex(str/list,freq=)</code>来产生,一组 Period 构成的 Index，可以用来作为 Series 或者 DataFrame 的索引。<br><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Pandas/30.png" alt="image.png"></p><h4 id="period-frequency-conversion">Period Frequency Conversion</h4><blockquote><p>Periods and PeriodIndex objects can be converted to another frequency with their <strong>asfreq</strong> method.</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">p = pd.Period(<span class="string">&#x27;2007&#x27;</span>, freq=<span class="string">&#x27;A-DEC&#x27;</span>)</span><br><span class="line">p</span><br><span class="line">p.asfreq(<span class="string">&#x27;M&#x27;</span>, how=<span class="string">&#x27;start&#x27;</span>)</span><br><span class="line">p.asfreq(<span class="string">&#x27;M&#x27;</span>, how=<span class="string">&#x27;end&#x27;</span>)</span><br><span class="line">p = pd.Period(<span class="string">&#x27;2007&#x27;</span>, freq=<span class="string">&#x27;A-JUN&#x27;</span>)</span><br><span class="line">p</span><br><span class="line">p.asfreq(<span class="string">&#x27;M&#x27;</span>, <span class="string">&#x27;start&#x27;</span>)</span><br><span class="line">p.asfreq(<span class="string">&#x27;M&#x27;</span>, <span class="string">&#x27;end&#x27;</span>)</span><br><span class="line">p = pd.Period(<span class="string">&#x27;Aug-2007&#x27;</span>, <span class="string">&#x27;M&#x27;</span>)</span><br><span class="line">p.asfreq(<span class="string">&#x27;A-JUN&#x27;</span>)</span><br></pre></td></tr></table></figure><p>对于不同 frequency 的 period，大的 period 可以看成一系列小的 period 的排列，而两个不同 frequency 的 Period 转换也可以视为是 superperiod 与 subperiod 的转换。<br><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Pandas/2022-04-18-14-42-58.png" alt=""></p><h2 id="shifting-leading-and-lagging-data">Shifting (Leading and Lagging) Data</h2><p>在对 df 或者 series 数据（包括时间序列）进行转换时有的时候可能需要把根据时间整体前移或者后移，这个时候就可以借助 df 和 se 的方法 shift，这种移动只是数据值的移动，索引不会改变（对于时间类型索引的数据，也可以通过指定 freq 参数来对索引进行整体的调整）。这种方法比较常见的应用场景是计算环比。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts = pd.Series(np.random.randn(<span class="number">4</span>),</span><br><span class="line">               index=pd.date_range(<span class="string">&#x27;1/1/2000&#x27;</span>, periods=<span class="number">4</span>, freq=<span class="string">&#x27;M&#x27;</span>))</span><br><span class="line">ts</span><br><span class="line">ts.shift(<span class="number">2</span>)</span><br><span class="line"><span class="comment">#计算环比</span></span><br><span class="line">ts / ts.shift(<span class="number">1</span>) - <span class="number">1</span></span><br></pre></td></tr></table></figure><h1 id="总体的随机排列-permutation-和随机抽样">总体的随机排列（permutation）和随机抽样</h1><h2 id="随机排列">随机排列</h2><p>随机排列可以借助 np.random.permutation(n)实现对 n 维数组的行索引进行一个随机排序，返回值为一个一维数组。然后可以利用 df.iloc 或者 df.take 函数来得到随机排序后的 df。</p><h2 id="随机抽样">随机抽样</h2><p>随机抽样用到的是 df.sample（n）函数，该函数返回值为对于 df 以行为抽样单位进行的随机抽样，返回值是从总体随机抽出的 n 行组成的 df（默认不可以重复，可以调整参数）</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.random.randn(<span class="number">20</span>).reshape((<span class="number">5</span>, <span class="number">4</span>))</span><br><span class="line">a = pd.DataFrame(a)</span><br><span class="line">sampler = np.random.permutation(<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(sampler)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(a.iloc[sampler])</span><br><span class="line"><span class="built_in">print</span>(a.take(sampler))</span><br><span class="line"><span class="built_in">print</span>(a.sample(<span class="number">3</span>))</span><br><span class="line"><span class="built_in">print</span>(a.sample(<span class="number">3</span>, replace=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure><h1 id="数据整理">数据整理</h1><h2 id="数据堆叠">数据堆叠</h2><p>数据堆叠的目的是通过建立多层级索引的方式将数据的列索引或者行索引转为行索引/列索引，这样使得数据集变得更长或者更宽。</p><p>多级标签的重塑主要借助 stack 和 unstack 方法：</p><ul><li>stack：This “rotates” or pivots from the columns in the data to the rows</li><li>unstack：This pivots from the rows into the columns<br>两个函数默认都从最低层级开始操作，然后将转换为另外一个轴的最低层级，可以传入 df 的层级名称或者数字来强制修改操作层级，另外就是堆叠数据（stack 方法）的时候默认是删除缺失值的，可以通过调节 dropna 参数进行调整。<br>另外对于二级索引的 series，还可以借助 <code>series.unstack()</code>方法将二级索引拆成一个 dataframe，同样的也可以借助 <code>stack</code> 方法将一个 df 转化为一个 series（inverse operation)。pd 的多级索引设置成了单独的对象（MultiIndex）</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = pd.DataFrame(np.arange(<span class="number">6</span>).reshape((<span class="number">2</span>, <span class="number">3</span>)),</span><br><span class="line">                    index=pd.Index([<span class="string">&#x27;Ohio&#x27;</span>, <span class="string">&#x27;Colorado&#x27;</span>], name=<span class="string">&#x27;state&#x27;</span>),</span><br><span class="line">                    columns=pd.Index([<span class="string">&#x27;one&#x27;</span>, <span class="string">&#x27;two&#x27;</span>, <span class="string">&#x27;three&#x27;</span>],</span><br><span class="line">                    name=<span class="string">&#x27;number&#x27;</span>))</span><br><span class="line">result = data.stack()</span><br><span class="line">df = pd.DataFrame(&#123;<span class="string">&#x27;left&#x27;</span>: result, <span class="string">&#x27;right&#x27;</span>: result + <span class="number">5</span>&#125;,</span><br><span class="line">                  columns=pd.Index([<span class="string">&#x27;left&#x27;</span>, <span class="string">&#x27;right&#x27;</span>], name=<span class="string">&#x27;side&#x27;</span>))</span><br><span class="line">df</span><br><span class="line">df.drop(index=(<span class="string">&#x27;Ohio&#x27;</span>,<span class="string">&#x27;three&#x27;</span>),inplace=<span class="literal">True</span>)</span><br><span class="line">df.drop(index=[<span class="string">&#x27;three&#x27;</span>],level=<span class="string">&#x27;number&#x27;</span>,inplace=<span class="literal">True</span>)<span class="comment">#会删除二级索引为number的所有行</span></span><br><span class="line">df</span><br><span class="line">df.unstack(<span class="string">&#x27;number&#x27;</span>)</span><br><span class="line">df.unstack(<span class="string">&#x27;state&#x27;</span>)</span><br><span class="line">df.unstack(<span class="string">&#x27;state&#x27;</span>).stack(<span class="string">&#x27;side&#x27;</span>)</span><br><span class="line">df.unstack(<span class="string">&#x27;state&#x27;</span>).stack(<span class="string">&#x27;side&#x27;</span>,dropna=<span class="literal">False</span>)</span><br><span class="line">df.unstack(<span class="string">&#x27;state&#x27;</span>).drop(columns=<span class="string">&#x27;Ohio&#x27;</span>,axis=<span class="number">1</span>,level=<span class="number">1</span>)</span><br><span class="line">df.unstack(<span class="string">&#x27;state&#x27;</span>).drop(columns=<span class="string">&#x27;Ohio&#x27;</span>,axis=<span class="number">1</span>,level=<span class="number">1</span>).stack(<span class="string">&#x27;side&#x27;</span>)</span><br><span class="line">df.unstack(<span class="string">&#x27;state&#x27;</span>).drop(columns=<span class="string">&#x27;Ohio&#x27;</span>,axis=<span class="number">1</span>,level=<span class="number">1</span>).stack(<span class="string">&#x27;side&#x27;</span>,dropna=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h2 id="df拼接">df拼接</h2><p>This section focuses on tools to help combine, join, and rearrange data.<br>这一部分主要介绍了一些将多个 df 的数据组合起来的一些方法：</p><ul><li>Join and Merge 部分主要侧重于类似于 SQL 查询的多表查询和联合的方法</li><li><code>pd.concat()</code> 和 numpy 的 concatenate 有些类似，主要应用于沿某一个轴进行拼接，也可以使用定义在DataFrame和Series对象的类方法<code>append()</code>实现相同的功能。</li><li>combine 方法主要用来对两个表的数据进行 combine，具体 combine 的方法依据传递的函数的返回值</li></ul><ol><li><strong>多个 dataframe 连接(通过 index 匹配进行)(Join and Merge)</strong></li></ol><ul><li>通过一个或多个键将两个数据集的列连接起来（完成 SQl 的 join 操作）:<code>pandas.merge()</code>函数和<code>pandas.DataFrame.join()</code>方法，多表的连接要把被连接的 df 名称以列表的形式传入<ul><li>pd.merge(df1,df2,on=‘column_name’)</li><li>pd.merge(df1,df2,left_on=’’,right_on=’’)</li><li>how：表示数据库的 join 方式，默认是 inner join。可选的有’left’,‘right’,‘output’</li><li>在对多个表进行 join 的时候，行索引会被丢弃<br><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Pandas/35.png" alt="image.png"></li><li>观察参数表可知也可以通过一个的行索引与另外一个表的列索引进行 join（甚至适用于行标签为多级索引的情况）</li><li><code>df.join()</code>方法适用于那些 index 相似或者相同且没有重复列的 dfs,默认使用行索引匹配也支持一个 df 的行索引英语另一个 df 的列索引 join 起来</li></ul></li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">left1 = pd.DataFrame(&#123;<span class="string">&#x27;key&#x27;</span>: [<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>],</span><br><span class="line">                      <span class="string">&#x27;value&#x27;</span>: <span class="built_in">range</span>(<span class="number">6</span>)&#125;)</span><br><span class="line">right1 = pd.DataFrame(&#123;<span class="string">&#x27;group_val&#x27;</span>: [<span class="number">3.5</span>, <span class="number">7</span>]&#125;, index=[<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(left1)</span><br><span class="line"><span class="built_in">print</span>(right1)</span><br><span class="line">left1.join(right1, on=<span class="string">&#x27;key&#x27;</span>)</span><br><span class="line">left2 = pd.DataFrame([[<span class="number">1.</span>, <span class="number">2.</span>], [<span class="number">3.</span>, <span class="number">4.</span>], [<span class="number">5.</span>, <span class="number">6.</span>]],</span><br><span class="line">                     index=[<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;e&#x27;</span>],</span><br><span class="line">                     columns=[<span class="string">&#x27;Ohio&#x27;</span>, <span class="string">&#x27;Nevada&#x27;</span>])</span><br><span class="line">right2 = pd.DataFrame([[<span class="number">7.</span>, <span class="number">8.</span>], [<span class="number">9.</span>, <span class="number">10.</span>], [<span class="number">11.</span>, <span class="number">12.</span>], [<span class="number">13</span>, <span class="number">14</span>]],</span><br><span class="line">                      index=[<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;e&#x27;</span>],</span><br><span class="line">                      columns=[<span class="string">&#x27;Missouri&#x27;</span>, <span class="string">&#x27;Alabama&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(left2)</span><br><span class="line"><span class="built_in">print</span>(right2)</span><br><span class="line">right2</span><br><span class="line">left2.join(right2, how=<span class="string">&#x27;outer&#x27;</span>)、</span><br><span class="line">another = pd.DataFrame([[<span class="number">7.</span>, <span class="number">8.</span>], [<span class="number">9.</span>, <span class="number">10.</span>], [<span class="number">11.</span>, <span class="number">12.</span>], [<span class="number">16.</span>, <span class="number">17.</span>]],</span><br><span class="line">                       index=[<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27;f&#x27;</span>],</span><br><span class="line">                       columns=[<span class="string">&#x27;New York&#x27;</span>, <span class="string">&#x27;Oregon&#x27;</span>])</span><br><span class="line">another</span><br><span class="line">left2.join([right2, another])</span><br><span class="line">left2.join([right2, another], how=<span class="string">&#x27;outer&#x27;</span>)</span><br></pre></td></tr></table></figure><ol><li><strong>dfs 沿轴拼接</strong>（Concatenating)</li></ol><p>沿轴拼接的好处：</p><ul><li>拼接后的数据可以看到数据的来源</li><li>拼接的时候需要删除默认的整数标签</li><li>join 或者 merge 方法实现的其实是表的横向拼接，需要纵向拼接时的情况</li></ul><p>df 的拼接是从 numpy 的拼接引入的，选择沿着不同的轴进行匹配会产生不同的结果，具体匹配情况可以类比数组的拼接，区别是沿着 axis=1 进行叠加时会考虑行索引相同的进行合并。</p><ul><li>数据横向、纵向堆叠：<code>pandas.concat([],axis=,join=)</code>（可以通过 keys 来在合并轴上创建层次索引）</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s1=pd.DataFrame(</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&#x27;height&#x27;</span>:[<span class="number">1.4</span>,<span class="number">1.5</span>,<span class="number">1.6</span>],</span><br><span class="line">        <span class="string">&#x27;weight&#x27;</span>:[<span class="number">160</span>,<span class="number">180</span>,<span class="number">200</span>]</span><br><span class="line">    &#125;,</span><br><span class="line">index=[<span class="string">&#x27;xiaoming&#x27;</span>,<span class="string">&#x27;xiaohonng&#x27;</span>,<span class="string">&#x27;xiaowang&#x27;</span>]</span><br><span class="line">)</span><br><span class="line">s1</span><br><span class="line">s2=pd.DataFrame(</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&#x27;age&#x27;</span>:[<span class="number">14</span>,<span class="number">5</span>,<span class="number">16</span>],</span><br><span class="line">        <span class="string">&#x27;sex&#x27;</span>:[<span class="string">&#x27;male&#x27;</span>,<span class="string">&#x27;female&#x27;</span>,<span class="string">&#x27;male&#x27;</span>]</span><br><span class="line">    &#125;,</span><br><span class="line">index=[<span class="string">&#x27;xiaoming&#x27;</span>,<span class="string">&#x27;xiaohonng&#x27;</span>,<span class="string">&#x27;xiaowang&#x27;</span>]</span><br><span class="line">)</span><br><span class="line">s2</span><br><span class="line">pd.concat([s1,s2])</span><br><span class="line">pd.concat([s1,s2],axis=<span class="number">1</span>,keys=[<span class="string">&#x27;体型&#x27;</span>,<span class="string">&#x27;个人信息&#x27;</span>])</span><br><span class="line"><span class="comment">#写成字典形式</span></span><br><span class="line">pd.concat(&#123;<span class="string">&#x27;体型&#x27;</span>: df1, <span class="string">&#x27;个人信息&#x27;</span>: df2&#125;, axis=<span class="number">1</span>)</span><br><span class="line">pd.concat([s1,s4],keys=[<span class="string">&#x27;表1&#x27;</span>,<span class="string">&#x27;表2&#x27;</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Pandas/36.png" alt="image.png"><br><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Pandas/37.png" alt="image.png"></p><ol start="3"><li></li></ol><p>combine 也是适用于 index 部分或者全部相似的情况，combine 的其实是两个表的值，有点类似于 numpy 的 where 函数，是 if-else 功能的一个等价表示。两种使用方法，一种是 np.where()方法，一种是 pd.combine(self,df,func)(func 为一个传入两个参数的函数。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = pd.Series([np.nan, <span class="number">2.5</span>, np.nan, <span class="number">3.5</span>, <span class="number">4.5</span>, np.nan],</span><br><span class="line">              index=[<span class="string">&#x27;f&#x27;</span>, <span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;a&#x27;</span>])</span><br><span class="line">b = pd.Series(np.arange(<span class="built_in">len</span>(a), dtype=np.float64),</span><br><span class="line">              index=[<span class="string">&#x27;f&#x27;</span>, <span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;a&#x27;</span>])</span><br><span class="line">b[-<span class="number">1</span>] = np.nan</span><br><span class="line">a</span><br><span class="line">b</span><br><span class="line">np.where(pd.isnull(a), b, a)</span><br><span class="line">b.combine(a,<span class="built_in">max</span>)</span><br></pre></td></tr></table></figure><ul><li>pd.combine_first(self,other):将 self 的空值用但缺失值会用 other 的对应值进行填充。</li></ul><h2 id="df重构">df重构</h2><p>数据的重塑主要指的是将数据的shape进行变化，本质上其实是使用<code>stack()</code>和<code>unstack()</code>方法，只是因为比较常用而进行了一个封装(一般来说我们用于处理的数据是不存在索引的，或者说往往会用连续数字做一个简单的索引)</p><h3 id="行列值的重塑-数据透视long-wide">行列值的重塑（数据透视long→wide）</h3><p>这部分主要介绍的是 pivot 函数，pivot 函数实现的是数据从长的形式向宽的形式的转换，一般意义上来说，我们认为存储在 csv 或者数据库中的文件属于长的格式（有多个索引的 key 参数，只有一个 value）。pivot 函数要做的其实就是根据一个 key 的离散取值来把长的表给变成宽的表。<br><code>df.pivot('column_1','column_2','column_3)</code><br>df 是一个我们希望转化的长的表，上述语句的意思其实就是我们希望用原来 df 的’column_1’作为行索引,'column_2’作为列索引,'column_3’作为值对 df 进行一次重整：<br><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Pandas/38.png" alt="image.png"><br>如果不指定最后一个参数，默认会创建多级索引（等价于：<code>df.set_index(['column_1','column_2]).uhstack('column_2')</code>（pivot()其实就是用 set_index()创建层次化索引，再用 unstack()重塑）</p><h3 id="逆透视wide-long">逆透视wide→long</h3><p>pivot 的一个逆运算是 pd.melt(),这个是用来将多列转化一列：<br><code>pd.melt(df, id_vars=['key'], value_vars=['A', 'B'])</code><br>该函数最后返回的是一个以<code>id_vars</code>为保留列,以<code>value_vars</code>中的列名作为列名称为’variable’的列的取值的，'value’列为原列对应取值的一个df。<br>与之类似的还有<code>pd.lreshape()</code>方法，该函数主要用来将有相同含义的列进行一个整合，形如以下的数据集:<br><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Pandas/1659768405465.png" alt="1659768405465"><br>使用<code>melt()</code>方法的整合结果为:<br><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Pandas/1659768452140.png" alt="1659768452140"><br>使用<code>lreshape()</code>方法的整合结果为:<br><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Pandas/1659768438346.png" alt="1659768438346"></p><h1 id="数据预处理">数据预处理</h1><p>pd 在对数据进行处理时会默认不考虑缺失值（数值型数据的缺失值会被写作 NaN，另外需要注意的是 Python 内置的 None 也会被视为缺失值）</p><h2 id="data-cleaning">Data Cleaning</h2><h3 id="去重">去重</h3><ul><li>返回不重复数据：df.unique()</li><li>统计值：df.value_counts()（默认按列计算好像，返回的还是一个 dataframe，值有更改）</li><li>查找是否存在重复数据：df.duplicated()(返回布尔值，默认将已经观察到先前有之后的行返回 True 这个需要调整 keep 函数，默认查找全部列，也可以进行调整)<code>data.drop_duplicates(['k1'])</code> （只查看 k1 列）</li><li>去除重复数据：<code>pandas.DataFrame(Series).drop_duplicates()</code>方法。</li></ul><h3 id="缺失值处理">缺失值处理</h3><p><strong>缺失值识别</strong>：</p><ul><li>pandas.DataFrame.<code>isnull()</code>和 pandas.DataFrame.<code>notnull()</code>方法识别缺失值和非缺失值,两个方法会返回一个与输入同型的布尔df。<ul><li><code>df.isnull().any()</code>：判断存在缺失值的列(<code>any()</code>Return whether any element is True, <strong>potentially over an axis</strong>)</li><li><code>df.isnull().sum()</code>:统计每列缺失值的个数</li></ul></li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#将数据按照指定列分组后统计每组中每列的缺失值情况，筛选出指定列存在缺失值的组并升序排列</span></span><br><span class="line">data_c=data.groupby(<span class="string">&#x27;所在小区&#x27;</span>).apply(<span class="keyword">lambda</span> x: x.isna().<span class="built_in">sum</span>())</span><br><span class="line">data_c[data_c[<span class="string">&#x27;建筑类型&#x27;</span>] &gt; <span class="number">0</span>][<span class="string">&#x27;建筑类型&#x27;</span>].sort_values(ascending=<span class="literal">False</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>缺失值删除</strong></p><ul><li>对缺失值，可以使用 pandas.DataFrame.<code>dropna()</code>方法删除记录或特征(默认删除含有缺失值的行，可以修改 how 参数进行调节，也可以调节 thresh 参数控制删除指定数量缺失值的行，亦可通过调节<code>subset=[col_name]</code>参数来指定删除指定列存在缺失值的行)</li></ul><p><strong>缺失值补充</strong></p><ul><li><code>df.isnull().T.any() == True</code>返回缺失值所在行的索引</li><li>也可以使用 pandas.DataFrame.<code>fillna()</code>方法进行常量填补（）<ul><li>输入字典来指定每一列的填补值</li><li>调整 inplace 参数直接在原 df 上修改</li><li>method 参数可以选择填补方法，使用的方法有’ffill’,‘bfill’</li><li>limit 参数可以指明对缺失值多少个的值进行填补</li><li>也可以利用原数据集的均值进行填补（<code>data.fillna(data.mean())</code> ）</li></ul></li><li>或者使用 pandas.DataFrame.interpolate(), SciPy 的 interpolate 方法进行线性差值、多项式插值、样条插值。</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#拉格朗日插值方法</span></span><br><span class="line"><span class="keyword">from</span> scipy.interpolate <span class="keyword">import</span> lagrange</span><br><span class="line">	<span class="comment">#自定义列向量插值函数,s为列向量,n为被插值的位置,k为取前后的数据个数，	默认5</span></span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">ployinterp_columns</span>(<span class="params">s, n, k=<span class="number">5</span></span>):</span><br><span class="line">		y = s[<span class="built_in">list</span>(<span class="built_in">range</span>(n-k,n)) + <span class="built_in">list</span>(<span class="built_in">range</span>(n+<span class="number">1</span>,n+<span class="number">1</span>+k))] <span class="comment">#取数</span></span><br><span class="line">		y = y[y.notnull()] <span class="comment">#剔除空值</span></span><br><span class="line">		<span class="keyword">return</span> lagrange(y.index, <span class="built_in">list</span>(y))(n) <span class="comment">#插值并返回插值结果</span></span><br><span class="line">		<span class="keyword">for</span> i <span class="keyword">in</span> data.columns:  		<span class="comment">#逐个元素判断是否需要插值</span></span><br><span class="line">			<span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(data)):</span><br><span class="line">				<span class="keyword">if</span> (data[i].isnull())[j]: <span class="comment">#如果为空即插值</span></span><br><span class="line">				data[i][j] = ployinterp_columns(data[i],j)</span><br></pre></td></tr></table></figure><h3 id="截断">截断</h3><p><code>df.clip()</code></p><h3 id="异常值检测">异常值检测</h3><p>异常值的检测主要是根据已有的需求对 df 进行一个筛选，然后通过赋值操作来剔除不需要的异常值。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.random.randn(<span class="number">16</span>).reshape(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;=&quot;</span>*<span class="number">80</span>)</span><br><span class="line">a[a &gt; np.mean(a)] = <span class="literal">None</span></span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="转换数据-哑变量处理-index-dummy-variables">转换数据–哑变量处理（Index/dummy Variables）</h3><ul><li>当特征为分类型时，例如职业、学历、血型、疾病严重程度等等，通常会将原始的多分类变量转化为数值型，这种转化后的特征（或变量）称为哑变量，又称为虚拟变量、虚设变量或名义变量。</li><li>它是人为虚设的变量，通常取值为 0 或 1，来反映<strong>某个变量的不同属性</strong>。</li><li>哑变量的处理过程实际上就是分类型特征的值的编码过程。Pandas 提供了哑变量处理方法<code>pandas.getdummies().</code></li></ul><blockquote><p>这个地方联想一下独热编码，哑变量处理其实进行一个 one-hot 编码</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> dataclasses <span class="keyword">import</span> replace</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.DataFrame(&#123;<span class="string">&#x27;key&#x27;</span>: [<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>],</span><br><span class="line">                   <span class="string">&#x27;data1&#x27;</span>: <span class="built_in">range</span>(<span class="number">6</span>)&#125;)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;初始的df为:\n&quot;</span>, df)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;根据key值列得到的指示变量:\n&quot;</span>, pd.get_dummies(df[<span class="string">&#x27;key&#x27;</span>]))</span><br><span class="line"><span class="comment">#可以调整prefix参数给指示变量加上前缀名称</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Pandas/31.png" alt="image.png"></p><h3 id="字符串数据">字符串数据</h3><p>对字符串的操作有使用字符串内置函数和 re 库进行正则表达式匹配两种方法，pd 将这两种方法都加在了 df 或者 series 对象的 str 属性中，通过 df/series.str.method_name 就可以使用了。直接调用内置的字符串处理函数会有一个问题是这些函数并没有定义 nan 数据的处理方式，因此最好借助 str 属性进行调用。感觉 series.str 就可以看成是一个字符串对象，然后就可以对这个对象调用一些字符串用的方法，包括索引什么的（通过装饰器把函数当属性用）。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = &#123;<span class="string">&#x27;Dave&#x27;</span>: <span class="string">&#x27;dave@google.com&#x27;</span>, <span class="string">&#x27;Steve&#x27;</span>: <span class="string">&#x27;steve@gmail.com&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;Rob&#x27;</span>: <span class="string">&#x27;rob@gmail.com&#x27;</span>, <span class="string">&#x27;Wes&#x27;</span>: np.nan&#125;</span><br><span class="line">data = pd.Series(data)</span><br><span class="line">data</span><br><span class="line">data.isnull()</span><br><span class="line">data.<span class="built_in">str</span>.contains(<span class="string">&#x27;gmail&#x27;</span>)</span><br><span class="line">matches = data.<span class="built_in">str</span>.<span class="keyword">match</span>(pattern, flags=re.IGNORECASE)</span><br><span class="line"><span class="comment">#对查询结果的索引方式，直接索引或者借助str.get</span></span><br><span class="line"> matches.<span class="built_in">str</span>.get(<span class="number">1</span>)</span><br><span class="line">matches.<span class="built_in">str</span>[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>就是因为有 nan 值的存在导致一些本来可以直接用的内置函数要做一些调整：<br><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Pandas/32.png" alt="image.png"><br><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Pandas/33.png" alt="image.png">0</p><h3 id="重置索引">重置索引</h3><p>数据清洗时，会将带空值的行删除，此时 DataFrame 或 Series 类型的数据不再是连续的索引，可以使用<code>reset_index()</code>重置索引。</p><h3 id="数据筛选">数据筛选</h3><p><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Pandas/34.png" alt="image.png"></p><h2 id="数据标准化">数据标准化</h2><h2 id="数据分组和聚合操作">数据分组和聚合操作</h2><blockquote><p>After loading, merging, and preparing a dataset, you may need to compute group statistics<br>or possibly pivot tables for reporting or visualization purposes. pandas provides a<br>flexible groupby interface, enabling you to slice, dice, and summarize datasets in a<br>natural way.</p></blockquote><p>分组的介绍参见前面内容，这里主要介绍聚合。</p><h3 id="聚合">聚合</h3><p><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Pandas/40.png" alt="聚合数据的三种方法.png"><br>除了 Series 方法 quantile 函数不支持对 groupby 后的 df 直接使用以外，常见的统计描述函数都可以直接在 dfGroupBy 上进行聚合操作，为了使用我们自定义的聚合函数，这里引入 python 的一些函数</p><h4 id="使用-agg-方法聚合数据">使用 agg 方法聚合数据</h4><p>agg，aggregate 方法都支持对每个分组应用某函数，包括 Python 内置函数或自定义函数。同时这两个方法能够也能够直接对 DataFrame 进行函数应用操作。<br>在正常使用过程中，agg 函数和 aggregate 函数对 DataFrame 对象操作时功能几乎完全相同，因此只需要掌握其中一个函数即可。它们的参数说明如下表。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">DataFrame.agg(func, axis=<span class="number">0</span>, *args, **kwargs)</span><br><span class="line">DataFrame.aggregate(func, axis=<span class="number">0</span>, *args, **kwargs)</span><br></pre></td></tr></table></figure><p>func 接收函数、字符串、字典，或函数与字符串的列表。</p><ul><li>传入一个函数名组成的列表，则会将每一个函数的函数名作为返回值的列名,如果不希望使用函数名作为列名，可以将列表中的元素写成类似’(column_name,function)'的元组形式来指定列名为name。如果想指定聚合列的列名，可以写成<code>new_column_name=(column_name,function)</code>的形式，多列就并列传入多个参数即可。</li><li>传入一个字典格式</li></ul><p><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Pandas/41.png" alt="image.png"></p><p>自定义函数时的一点注意事项<br>自定义的函数应该是一个用来聚合数组类型数据的函数。这里和 quantile 函数不能用是一样的原因。pd 的统计描述函数是从 np 继承过来的因此写成 np.min 没有差别</p><h4 id="使用-apply-方法聚合数据">使用 apply 方法聚合数据</h4><p><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Pandas/42.png" alt="image.png"></p><blockquote><p>apply splits the object being manipulated into pieces, invokes the passed function on each piece, and then attempts to concatenate（pd.concat） the pieces together.</p></blockquote><p>如果只是对 DataFrame 对象或分组对象进行统一的统计计算，也可以使用 groupby 对象的方法 apply，其格式为：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">DataFrame.apply(func, axis=<span class="number">0</span>, broadcast=<span class="literal">None</span>, raw=<span class="literal">False</span>,</span><br><span class="line"> reduce=<span class="literal">None</span>, result_type=<span class="literal">None</span>, args=(), **kwds)</span><br></pre></td></tr></table></figure><p>其中可变长参数主要是输入一些 func 需要的其他参数<br><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Pandas/43.png" alt="image.png"></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#巧妙利用apply函数拆解字典结构</span></span><br><span class="line">df=pd.DataFrame(&#123;<span class="string">&#x27;info&#x27;</span>:[&#123;<span class="string">&#x27;姓名&#x27;</span>: <span class="string">&#x27;琪亚娜·卡斯兰娜&#x27;</span>, <span class="string">&#x27;生日&#x27;</span>: <span class="string">&#x27;12月7日&#x27;</span>, <span class="string">&#x27;外号&#x27;</span>: <span class="string">&#x27;草履虫&#x27;</span>&#125;</span><br><span class="line">              , &#123;<span class="string">&#x27;姓名&#x27;</span>: <span class="string">&#x27;布洛妮娅·扎伊切克&#x27;</span>, <span class="string">&#x27;生日&#x27;</span>: <span class="string">&#x27;8月18日&#x27;</span>, <span class="string">&#x27;外号&#x27;</span>: <span class="string">&#x27;板鸭&#x27;</span>&#125;</span><br><span class="line">              ,  &#123;<span class="string">&#x27;姓名&#x27;</span>: <span class="string">&#x27;德丽莎·阿波卡利斯&#x27;</span>, <span class="string">&#x27;生日&#x27;</span>: <span class="string">&#x27;3月28日&#x27;</span>, <span class="string">&#x27;外号&#x27;</span>: <span class="string">&#x27;德丽傻&#x27;</span>, <span class="string">&#x27;武器&#x27;</span>: <span class="string">&#x27;犹大的誓约&#x27;</span>&#125;</span><br><span class="line">              ]&#125;,index=[<span class="string">&#x27;001&#x27;</span>,<span class="string">&#x27;002&#x27;</span>,<span class="string">&#x27;003&#x27;</span>])</span><br><span class="line">df</span><br><span class="line">df[<span class="string">&#x27;info&#x27;</span>].apply(pd.Series)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>使用pd的<code>apply</code>函数，我们可以使用自定义参数来实现很多功能，自定义函数过程中一个非常重要的点在于使用<code>apply</code>函数时，在对自定义函数func进行apply时，传入func的参数是对DataFrame的一行或者一列数据（默认是一列数据，通过<code>axis</code>参数进行调整），这时传入func的相当于一个<strong>Series</strong>(index为行索引或者列名)，这一点需要格外注意。如果apply本身传入的是一个Series，那么传入func的就是一个值。</p><blockquote><p>使用<code>agg</code>和<code>apply</code>聚合数据的一个区别体现函数的作用对象上，在自定义函数时，我们使用<code>agg</code>时默认聚合函数的输入是一个数组，而<code>apply</code>的聚合函数的输入参数是一个DataFrame，我想这也一定程度上解释了为什么apply函数会更常用一些。</p></blockquote><h4 id="使用-transform-方法聚合数据">使用 transform 方法聚合数据</h4><p>Pandas 提供了<code>transform()</code>方法对 DataFrame 对象和分组对象的指定列进行统计计算，统计计算可以使用用户自定义函数。<br>其格式为：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pythonDataFrame.transform(func,*args,**kwargs)</span><br></pre></td></tr></table></figure><p>其中<code>func</code>接收用于统计计算的函数。其形式为一般为<code>lambda x: f(x)</code>. 其中 x 为 DataFrame 或分组对象 GroupBy object 的列的泛指。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Z-score标准化，即缩放为均值为0，标准差为1</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;汽车销售表分组后实现组内Z-score标准化后前五行为：\n&#x27;</span>,</span><br><span class="line">      vsGroup.transform(<span class="keyword">lambda</span> x: (x - x.mean()) /</span><br><span class="line">      x.std()).head())</span><br></pre></td></tr></table></figure><h2 id="透视表和交叉表">透视表和交叉表</h2><p><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Pandas/44.png" alt="数据透视表创建.png"></p><h3 id="使用-pivot-table-创建透视表">使用 pivot_table 创建透视表</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#fill_value表示空值的填充值</span></span><br><span class="line">pythonpandas.pivot_table(data, values=<span class="literal">None</span>, index=<span class="literal">None</span>,</span><br><span class="line">columns=<span class="literal">None</span>, aggfunc=<span class="string">&#x27;mean&#x27;</span>, fill_value=<span class="literal">None</span>, margins=<span class="literal">False</span>,</span><br><span class="line">dropna=<span class="literal">True</span>, margins_name=<span class="string">&#x27;All&#x27;</span>)</span><br><span class="line">DataFrame.pivot_table(values=<span class="literal">None</span>, index=<span class="literal">None</span>,</span><br><span class="line">columns=<span class="literal">None</span>, aggfunc=<span class="string">&#x27;mean&#x27;</span>, fill_value=<span class="literal">None</span>, margins=<span class="literal">False</span>,</span><br><span class="line">dropna=<span class="literal">True</span>, margins_name=<span class="string">&#x27;All&#x27;</span>)</span><br></pre></td></tr></table></figure><p>参数名称及含义:</p><ul><li>data:创建表的数据</li><li>index:行分组键，可以写成列表表示多级索引。</li><li>columns:列分组键</li><li>values：数值计算键</li><li>aggfunc: 聚合函数 ，默认为平均值函数</li><li>margins： 接收布尔值，表示是否对透视表的行和列进行汇总</li><li>dropna:是否删除全为Nan的列，默认为False</li></ul><blockquote><p>实际应用过程中出现的一个问题是在做数据透视表时行分组建和计算键不能是同一个键，例如对于一个df的a列，该列存储的是不同类型的文本数据，我想要统计每一个文本数据出现的次数，这个时候就既需要a列作为索引键，又同时需要聚合该列的数据，这种情况下该函数会报错。这个时候一个替代方法是:<code>df.groupby(&quot;district&quot;)['companySize'].value_counts()</code>，就会返回一个以district和companysize为行索引统计company频次的一个表。</p></blockquote><blockquote><p>透视表其实一定程度上来说就是对分组方法(<code>groupby()</code>)的一个封装。</p></blockquote><h3 id="使用-crosstab-创建交叉表">使用 crosstab 创建交叉表</h3><blockquote><p>A cross-tabulation (or crosstab for short) is a special case of a pivot table that computes group frequencies.</p></blockquote><p>交叉表是一种特殊的数据透视表，它仅指定一个特征作为行分组键，一个特征作为列分组键，是为交叉的意思。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pandas.crosstab(index, columns, values=<span class="literal">None</span>, rownames=<span class="literal">None</span>,</span><br><span class="line">colnames=<span class="literal">None</span>, aggfunc=<span class="literal">None</span>, margins=<span class="literal">False</span>, margins_name=<span class="string">&#x27;All&#x27;</span></span><br><span class="line">, dropna=<span class="literal">True</span>, normalize=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><ul><li>index：生成交叉表的行索引标签</li><li>columns：生成交叉表的列标签</li><li>value:表格的值，既可以是数组或者 series 也可以是数组列表</li></ul><p><strong>其它参数与 pandas.pivot_table()方法类似</strong>。注意<code>rownames</code>为行分组键的名称，默认应该是 index 参数对应的列名。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#以vehicle_type作为行、salesman_name为列</span></span><br><span class="line">vsCross = pd.crosstab(index=vs[<span class="string">&#x27;vehicle_type&#x27;</span>],</span><br><span class="line">     columns=vs[<span class="string">&#x27;salesman_name&#x27;</span>],</span><br><span class="line">     values = vs[<span class="string">&#x27;counts&#x27;</span>],aggfunc = np.<span class="built_in">sum</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;以vehicle_type和salesman_name为分组键、</span></span><br><span class="line"><span class="string">    counts为值的汽车销售数据交叉透视表前10行10列</span></span><br><span class="line"><span class="string">    为：\n&#x27;</span>,vsCross.iloc[:<span class="number">10</span>,:<span class="number">10</span>])</span><br></pre></td></tr></table></figure><h2 id="转换数据-dataframe-数据离散化">转换数据–DataFrame 数据离散化</h2><p>在进行数据分析时，需要先了解数据的分布特征，如某个值的出现频次、不同的取值区间样本的多少，需要对数据的分布特征进行初步的了解。对于非数值类数据的统计可以使用<code>astype</code>方法将目标特征的数据类型转换为<code>category</code>类别</p><ul><li>Pandas 提供了按照变量值域进行等宽分割的<code>pandas.cut()</code>方法。</li><li>用户也可以使用 pandas.DataFrame.quantile()方法获得特征的具有相同位置间隔的不同分位数，使用<code>pandas.cut()</code>方法按照各个分位数切割区间，设计等频法离散化连续数据。</li></ul><h3 id="统计等值样本出现的频数">统计等值样本出现的频数</h3><p>要统计相同值样本出现的频数，Pandas 提供了<code>pandas.series.value_counts()方法</code>。其格式如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pythonSeries.value_counts(normalize=<span class="literal">False</span>, sort=<span class="literal">True</span>,</span><br><span class="line">ascending=<span class="literal">False</span>, bins=<span class="literal">None</span>, dropna=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>pandas.series.value_counts()方法将 series 中的<strong>相同值看作一个类别</strong>，<strong>分别返回各个类别的记录数量</strong>，即频次，并根据 sort 的值决定是否按频次排序。<code>acending</code>按照频数升序或者降序。</p><h3 id="统计落入每个区间的频数-等宽法离散数据">统计落入每个区间的频数(等宽法离散数据)</h3><p>使用<code>pandas.cut()</code>方法和<code>pandas.series.value_counts()</code>方法，将数据值域分割为等宽的若干区间，并统计各个区间的样本数量。<br><strong>分割变量值域</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pythonpandas.cut(x, bins, right=<span class="literal">True</span>, labels=<span class="literal">None</span>, retbins=<span class="literal">False</span>,</span><br><span class="line">precision=<span class="number">3</span>, include_lowest=<span class="literal">False</span>, duplicates=<span class="string">&#x27;raise&#x27;</span>)<span class="comment">#right参数用来调整区间的类型（默认左开右闭）</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>其中 x 接收要分割的一维数组，bins 接收正整数，表示要分割为的区间数量。<br>有的时候分割变量也会借助分位数进行分割，这个时候就要用到与 pd.cut()类似的 pd.qcut()方法，若传入 bins 为一个整数，则表示等分的区间个数，若传入的为一个值在 0-1 的列表，则会根据列表进行划分。<br>统计样本数量，依然还用<code>pd.value_counts()方法</code>函数。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">ages = [<span class="number">20</span>, <span class="number">22</span>, <span class="number">25</span>, <span class="number">27</span>, <span class="number">21</span>, <span class="number">23</span>, <span class="number">37</span>, <span class="number">31</span>, <span class="number">61</span>, <span class="number">45</span>, <span class="number">41</span>, <span class="number">32</span>]</span><br><span class="line">bins = [<span class="number">18</span>, <span class="number">25</span>, <span class="number">35</span>, <span class="number">60</span>, <span class="number">100</span>]</span><br><span class="line">cats = pd.cut(ages, bins)</span><br><span class="line"><span class="comment"># a special Categorical object</span></span><br><span class="line"><span class="built_in">print</span>(cats)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(cats.categories)  <span class="comment"># 一个包含所有category名字的对象</span></span><br><span class="line"><span class="built_in">print</span>(cats.codes)  <span class="comment"># 索引值返回</span></span><br><span class="line">pd.value_counts(cats)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#ch4-3,例4-4 查看汽车销售数据表的数值型特征的描述性统计</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">path=<span class="string">&#x27;D:/my_python/ch04/data/&#x27;</span></span><br><span class="line">vs = pd.read_table(path+<span class="string">&#x27;vehicle_sales.csv&#x27;</span>,</span><br><span class="line">                      sep = <span class="string">&#x27;,&#x27;</span>,encoding = <span class="string">&#x27;gbk&#x27;</span>)</span><br><span class="line"><span class="comment">#例4-8 等宽法离散化数据系列，各区间宽度大致相同，求各区间的频数</span></span><br><span class="line">k=<span class="number">5</span><span class="comment">#离散化为等宽的k个区间</span></span><br><span class="line"><span class="comment">#返回k个区间，相当于k个类，以bins作为分割点</span></span><br><span class="line"><span class="comment">#返回的amounts变成了series，value变成了区间。</span></span><br><span class="line">amounts,bins=pd.cut(vs[<span class="string">&#x27;amounts&#x27;</span>],k,retbins=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;销售额分割成&#x27;</span>,k,<span class="string">&#x27;个等宽区间的分割点是：\n&#x27;</span>,bins)</span><br><span class="line"><span class="comment">#统计各个区间记录的频数，**默认按照频数降序排列**</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;汽车销售额等宽离散化为5个区间后每个区间及其频数为：\n&#x27;</span>,</span><br><span class="line">      amounts.value_counts())</span><br><span class="line"><span class="comment">#可视化，按照区间从小到大排列</span></span><br><span class="line"><span class="comment">#a_frequency=amounts.value_counts(sort=False)</span></span><br><span class="line"><span class="comment">#可视化，按照频数降序排列</span></span><br><span class="line">a_frequency=amounts.value_counts()</span><br><span class="line">labels=a_frequency.index[<span class="number">0</span>:k]<span class="comment">#将区间作为标签</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = <span class="string">&#x27;SimHei&#x27;</span><span class="comment">#设置字体为SimHei显示中文</span></span><br><span class="line">plt.rc(<span class="string">&#x27;font&#x27;</span>, size=<span class="number">14</span>)<span class="comment">#设置图中字号大小</span></span><br><span class="line">plt.figure(figsize=(<span class="number">6</span>,<span class="number">4</span>))<span class="comment">#设置画布</span></span><br><span class="line">plt.bar(<span class="built_in">range</span>(k),a_frequency)<span class="comment">#绘制柱状图</span></span><br><span class="line">plt.title(<span class="string">&#x27;销售额等宽法频数统计图&#x27;</span>)<span class="comment">#添加标题</span></span><br><span class="line">plt.xlabel(<span class="string">&#x27;销售额&#x27;</span>)<span class="comment">#添加横轴标签</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;频数&#x27;</span>)<span class="comment">#添加y轴名称</span></span><br><span class="line">plt.xticks(<span class="built_in">range</span>(k),labels,rotation=<span class="number">20</span>)<span class="comment">#横轴刻度与标签对准</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h3 id="等频法离散数据">等频法离散数据</h3><p>对于不均匀分布的数据， 有时需要按<strong>大致相同的样本频次</strong>，观察取得这些频次的样本分布在的不同区间。称为等频法离散化数据，简称为等频法。</p><ul><li>将样本从小到大进行排列，按照样本位置将数据划分为位置间隔相等的区间。位置间隔相同意味着样本出现的频数相同。</li><li>获得每个区间的第一个和最后一个元素的值，两者的差值即为与该位置区间对应的元素取值区间。</li><li>使用 Pandas 的<code>DataFrame.quantile()</code>方法能够获得 DataFrame 的任意分位数，据此可以得到等频的样本值域分割点。</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#例4-9 等频法离散化数据，各区间频数大致相同，求区间分割点。</span></span><br><span class="line"><span class="comment">#自定义等频法离散化函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">same_frequency_cut</span>(<span class="params">data,k</span>):</span><br><span class="line">    <span class="comment">#产生k个分位数位置</span></span><br><span class="line">    w=data.quantile(np.arange(<span class="number">0</span>,<span class="number">1</span>+<span class="number">1.0</span>/k,<span class="number">1.0</span>/k))</span><br><span class="line">    data=pd.cut(data,w)<span class="comment">#按照分位数位置离散化data</span></span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"><span class="comment">#返回的a_frequency变成了series，value变成了区间。</span></span><br><span class="line">a_frequency= same_frequency_cut(vs[<span class="string">&#x27;amounts&#x27;</span>],<span class="number">5</span>).value_counts()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;汽车销售额等频法离散化后各个类别数目分布状况为：&#x27;</span>,<span class="string">&#x27;\n&#x27;</span>,a_frequency)</span><br><span class="line"><span class="comment">#可视化，按照区间从小到大排列</span></span><br><span class="line">a_frequency=same_frequency_cut(vs[<span class="string">&#x27;amounts&#x27;</span>],<span class="number">5</span>).value_counts(sort=<span class="literal">False</span>)</span><br><span class="line">labels=a_frequency.index[<span class="number">0</span>:k]<span class="comment">#将区间作为标签</span></span><br><span class="line">plt.figure(figsize=(<span class="number">6</span>,<span class="number">4</span>))<span class="comment">#设置画布</span></span><br><span class="line">plt.bar(<span class="built_in">range</span>(k),a_frequency)<span class="comment">#绘制柱状图</span></span><br><span class="line">plt.title(<span class="string">&#x27;销售额等频法频数统计图&#x27;</span>)<span class="comment">#添加标题</span></span><br><span class="line">plt.xlabel(<span class="string">&#x27;销售额&#x27;</span>)<span class="comment">#添加横轴标签</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;频数&#x27;</span>)<span class="comment">#添加y轴名称</span></span><br><span class="line">plt.xticks(<span class="built_in">range</span>(k),labels,rotation=<span class="number">20</span>)<span class="comment">#横轴刻度与标签对准</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h1 id="pandas-统计分析">pandas 统计分析</h1><h2 id="dataframe-描述性统计">DataFrame 描述性统计</h2><p>单统计量<br><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Pandas/47.png" alt="image.png"></p><ul><li>极差——<code>Series.ptp(axis=None, skipna=None, level=None, numeric_only=None, **kwargs)</code></li><li><strong>协方差</strong>——<code>DataFrame.cov(min_periods=None)</code></li><li><strong>相关系数</strong>——<code>DataFrame.corr(method='pearson', min_periods=1)</code></li><li>标准误差——<code>Series.sem(axis=None, skipna=None, level=None, ddof=1, numeric_only=None, **kwargs)</code></li><li>分位数——<code>DataFrame.quantile(q=0.5, axis=0, numeric_only=True, interpolation='linear')</code></li><li><strong>非空值</strong>数量——<code>DataFrame.count(axis=0, level=None, numeric_only=False)</code></li><li>平均绝对离差——<code>DataFrame.mad(axis=None, skipna=None, level=None)</code></li></ul><p>这些统计方法都可以用在GroupBy对象后进行分组的描述性统计分析(具体使用方法见分组小节)<br><strong>多统计量</strong><br><code>DataFrame.describe()</code>方法默认返回 DataFrame 全部或指定<strong>数值型字段</strong>的和、均值、标准差、最小值、最大值、25%分位数、50%分位数和 75%分位数。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">DataFrame.describe(percentiles=<span class="literal">None</span>, include=<span class="literal">None</span>, exclude=<span class="literal">None</span>)</span><br><span class="line"><span class="comment"># 一般情况下会把结果进行转置，更符合我们的使用习惯</span></span><br><span class="line">df.describe().T</span><br></pre></td></tr></table></figure><p>除此以外可以对其参数进行调整,来对df中其他类型的变量进行统计描述，具体调整细节help即可，这里不再赘述。</p><blockquote><p><code>DataFrame.describe()</code>函数也可以对 DataFrame 的 category 类型（非数值型）进行描述性统计，返回 count, unique, top,freq 值，分别代表记录的数量、类的数量、记录数量最多的类、记录数量最多的类的记录数量。<br>当我们用数值来进行分类时，进行统计分析时如果不希望作为类别的数值列也被进行统计分析，可以专门将数值类的列转为非数值型数据（参考<a href="./%E7%BB%BC%E5%90%88%E5%AE%9E%E4%BE%8B">综合实例–iris 数据集统计分析</a>代码块第 97 行）。</p></blockquote><h1 id="窗口函数">窗口函数</h1><p>在实际应用过程中，我们可能会存在对整个 df 的局部数据进行统计分析的场景，这时就需要用到所谓的“窗口函数”,可以理解为在整体数据集上创建窗口来进行运算，pd 中提供的几种窗口函数有：</p><ul><li>rolling（移动函数）</li><li>expanding(扩展函数)</li><li>ewm（指数加权函数）</li></ul><blockquote><p>在数据分析的过程中，使用窗口函数能够提升数据的准确性，并且使数据曲线的变化趋势更加平滑，从而让数据分析变得更加准确、可靠。</p></blockquote><p>先咕咕了</p><h1 id="visualizations">visualizations</h1><p>Series 和 Df 都有一个 plot 属性来进行基本的一些绘图，默认情况下 se/df.plot()等价于 <code>se/df.plot.line()</code>[绘制线性图]，可以通过调节传入参数对绘图进行修饰 plot 属性本身包含许多种绘图方式。<br><code>df/se.plot</code> 的方法：</p><ul><li>line()</li><li>bar()(index 作为横坐标)</li><li>barz()(index 作为纵坐标)</li><li>hist():</li><li>density():概率密度函数</li><li>kde()：使用 kde 方法进行估计</li></ul></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kebuaaa.github.io">爱编程的小明</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kebuaaa.github.io/Pandas/">https://kebuaaa.github.io/Pandas/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://kebuaaa.github.io" target="_blank">小明的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Python%E5%BA%93/">Python库</a><a class="post-meta__tags" href="/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/">数据分析</a><a class="post-meta__tags" href="/tags/Pandas/">Pandas</a></div><div class="post-share"><div class="social-share" data-image="https://camo.githubusercontent.com/981d48e57e23a4907cebc4eb481799b5882595ea978261f22a3e131dcd6ebee6/68747470733a2f2f70616e6461732e7079646174612e6f72672f7374617469632f696d672f70616e6461732e737667" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/R%E7%BC%96%E7%A8%8B%E5%85%A5%E9%97%A8/" title="R基础"><img class="cover" src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="https://www.r-project.org/logo/Rlogo.png" onerror='onerror=null,src="https://images.pexels.com/photos/374918/pexels-photo-374918.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=600"' alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">R基础</div></div><div class="info-2"><div class="info-item-1">常用函数(备忘) 函数 含义 install.packages() 装包 update.packages() 更新包 library() 加载包 object 在R中，一个object可以是任何可以赋值给变量的东西（数据结构、函数、甚至是graph），一个object有两个重要的东西叫mode和class，前者决定这个object的存储方式(numeric,character，logical)，后者决定函数如何处理这个object。虽然有object的概念，但是R本身仍然是一种自顶向下式的编程方式，大部分功能都是通过各式各样的函数来实现的。 常用函数 dim()函数返回数据的维度 length()函数返回数据的长度 str()函数返回数据的结构 class()函数返回数据的类型 mode()函数返回数据的存储方式 names()函数返回数据的列名(Gives the names of components in an...</div></div></div></a><a class="pagination-related" href="/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" title="集成学习"><img class="cover" src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="/../top_img/10026.webp" onerror='onerror=null,src="https://images.pexels.com/photos/374918/pexels-photo-374918.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=600"' alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">集成学习</div></div><div class="info-2"><div class="info-item-1">集成学习更深入的内容可以参考周志华老师的《集成学习：基础与算法 》，限于笔者的能力和精力，过于深入的东西笔者还没有理解 许是开始就没学会，也或者是因为时间太长忘光光了，重新学习集成学习这章。 概述 集成学习(Ensemble Learning)是将多个学习器结合，构建一个有较强性能的机器学习器的方法。 集成学习的一般结构: 一组个体学习器+结合策略。一般来说集成的学习器都是弱学习器（弱学习器继承效果相对较好） 根据集成学习的各基估计器类型是否相同，可以分为同质和异质两种方法。 分类 根据每个基学习器是否同属一个种类。可以将集成学习分为同质和异质两种类型。 同质集成学习 同质表示集成的个体学习器属于同种类型，这时的个体学习器又称为基学习器。 目前来说，同质个体学习器的应用最为广泛。一般的集成学习均指同质个体学习器。而同质个体学习器使用最多的模型是 CART 决策树和神经网络。 周志华老师的《机器学习》一书中证明过若基分类器的错误率相互独立，根据 Hoeffding 不等式可得当个体分类器数目 T...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/Matplotlib/" title="Matplotlib库"><img class="cover" src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="https://matplotlib.org/_static/images/logo2.svg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2022-04-01</div><div class="info-item-2">Matplotlib库</div></div><div class="info-2"><div class="info-item-1">将本文结合代码使用效果更佳哦❤️❤️ matplotlib is a desktop plotting package designed for creating (mostly twodimensional) publication-quality plots. The project was started by John Hunter in 2002 to enable a MATLAB-like plotting interface in Python. Over time, matplotlib has spawned a number of add-on toolkits for data visualization that use matplotlib for their underlying plotting. One of these is...</div></div></div></a><a class="pagination-related" href="/Numpy/" title="Numpy"><img class="cover" src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="/top_img/10008.webp" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-27</div><div class="info-item-2">Numpy</div></div><div class="info-2"><div class="info-item-1">numpy(numerical Python) 是 Python 数值计算最重要的基础包，大多数提供科学计算的包都是用 NumPy 的数组为构建基础。 NumPy 可以用于数值计算的一个重要原因是因为他能处理大数组的数据： 在连续的内存块储存数据，独立于其他 Python 内置对象（C 语言编写的算法库，在 C 的基础上封装） 可以在整个数组上执行复杂的计算，不需要 for loop 速查 图片对应pdf.pdf 介绍 基本用法 NumPy 最重要的一个特点就是 ndarray(n 维数组对象，一个快速而灵活的大数据集容器) Creating ndarray python 默认创建数组的数据类型是浮点数（方便科学计算） np.array(): 支持任何序列对象 np.zeros（） np.empty()创建一个数组，值可能为 0 有些情况下为垃圾值 np.arrange():类似于内置的 range 返回一个 数组的数据类型 类型转换–np.astype 可以在创建数组时指定数值类型，也可以通过...</div></div></div></a><a class="pagination-related" href="/Seaborn%E5%BA%93%E7%AE%80%E4%BB%8B/" title="Seaborn库简介"><img class="cover" src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="https://seaborn.pydata.org/_images/logo-wide-lightbg.svg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2022-09-21</div><div class="info-item-2">Seaborn库简介</div></div><div class="info-2"><div class="info-item-1">将本文结合代码使用效果更佳哦❤️❤️ 速查 example gallery: Seaborn 库简介 特点: Seaborn, a statistical graphics library created by Michael Waskom. Seaborn simplifies creating many common visualization types. Unlike when using matplotlib directly, it wasn’t necessary to specify attributes of the plot elements in terms of the color values or marker codes. Behind the scenes, seaborn handled the translation from values in the dataframe to arguments that matplotlib understands. This declarative approach lets you...</div></div></div></a><a class="pagination-related" href="/pd%E5%AE%9E%E6%88%98/" title="Pd实战"><img class="cover" src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="https://camo.githubusercontent.com/981d48e57e23a4907cebc4eb481799b5882595ea978261f22a3e131dcd6ebee6/68747470733a2f2f70616e6461732e7079646174612e6f72672f7374617469632f696d672f70616e6461732e737667" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2022-08-06</div><div class="info-item-2">Pd实战</div></div><div class="info-2"><div class="info-item-1">主要写一些平时看到的比较常用的一些pd的函数的应用，通过应用场景来辅助更好地理解pandas。 合并高度对称的列 在实际运用过程中，我们可能会拿到形如以下形式的数据: 这样的数据集存在几个列的内容完全一致，因此我们希望实现的一个功能就是将这几个列的值合成一个列，得到形如下图的数据形式: import pandas as pddf = pd.DataFrame(&#123; &#x27;爱好1&#x27;: &#123;&#x27;小明&#x27;: &#x27;睡觉&#x27;, &#x27;小红&#x27;: &#x27;弹琴&#x27;&#125;, &#x27;地点1&#x27;: &#123;&#x27;小明&#x27;: &#x27;床上&#x27;, &#x27;小红&#x27;: &#x27;家&#x27;&#125;, &#x27;爱好2&#x27;: &#123; &#x27;小明&#x27;: &#x27;骑马&#x27;, &#x27;小红&#x27;: &#x27;开车&#x27; &#125;,...</div></div></div></a><a class="pagination-related" href="/PyCaret/" title="PyCaret"><img class="cover" src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="/top_img/10032.webp" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2022-07-31</div><div class="info-item-2">PyCaret</div></div><div class="info-2"><div class="info-item-1">PyCaret是Python中的低代码机器学习开发平台，能够自动完成机器学习的整个工作流。 这个库提供了借助相对工整的数据（无缺失值、类型明确）对一类的机器学习模型（分类、回归、聚类）进行训练，该库能够自己对数据集进行处理，并且能够自动生成模型，除此以外还能进行模型的评价和在测试集上的训练，这些都可以在官方文档中的QuickStart中查看。 这个库非常好的一点在于可以直接根据输入数据来判断数据类型然后作相应的数据处理操作，除此以外它也支持自定义的数据处理操作，这些操作可以在官方文档中的Preprocessing中查看。 最后，这个库也支持对于某一个特定的模型进行调优，这些调优可以在官方文档中的Hyperparameter Tuning中查看。</div></div></div></a><a class="pagination-related" href="/datetime%E6%A8%A1%E5%9D%97/" title="datetime模块"><img class="cover" src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="/top_img/10007.webp" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2022-04-02</div><div class="info-item-2">datetime模块</div></div><div class="info-2"><div class="info-item-1">字符串和 Datetime 之间的转换 对于 datetime 模块的所有与时间点相关的对象： 这些对象转化成字符串只需要利用 str 函数（格式化） 也可以利用对象的 strftime（）方法，该方法需要传入参数来指定格式化的形式，规范遵循 ISO C89 compatible（详见 time 模块的时间对象格式化方法）。 通过字符串得到这些对象一般要借助对应类的方法 strptime，这个方法需要传入两个参数，一个参数是要解析的字符串，一个参数是规定了字符串的格式。datetime.strptime('2022-02-10','%Y-%m-%d')需要注意的是两个参数的形式必须完全一致否则会解析错误。 考虑到 strptime（）方法不适用于格式不相同的字符串转化为时间对象，这里可以使用dateutil.parser.parse()函数对一些常见的时间字符串进行自动解析（这个库会在安装 pd 时自动安装），另外也可以调节参数对解析方式微调 from dateutil.parser import...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="/img/avatar.webp" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info-name">爱编程的小明</div><div class="author-info-description">只要不折腾，万般可将就</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">100</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">57</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">18</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/kebuAAA"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/kebuAAA" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="/2945190789@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="/img/wechat.webp" target="_blank" title="欢迎交流"><i class="fa-brands fa-weixin"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">如网页加载较慢请尝试魔法上网，博客图文可能无关可以忽略</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B"><span class="toc-number">1.</span> <span class="toc-text">简介</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#pandas-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84"><span class="toc-number">2.</span> <span class="toc-text">Pandas 数据结构</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#series"><span class="toc-number">2.1.</span> <span class="toc-text">Series</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#dataframe"><span class="toc-number">2.2.</span> <span class="toc-text">DataFrame</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B1%9E%E6%80%A7%EF%BC%9A"><span class="toc-number">2.2.1.</span> <span class="toc-text">属性：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%B9%E6%B3%95"><span class="toc-number">2.2.2.</span> <span class="toc-text">方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA-dataframe"><span class="toc-number">2.2.3.</span> <span class="toc-text">创建 DataFrame</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%BF%E9%97%AE-dataframe"><span class="toc-number">2.2.4.</span> <span class="toc-text">访问 DataFrame</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E5%AD%97%E5%85%B8%E6%96%B9%E5%BC%8F%E8%AE%BF%E9%97%AE-dataframe"><span class="toc-number">2.2.4.1.</span> <span class="toc-text">使用字典方式访问 DataFrame。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E5%B1%9E%E6%80%A7%E6%96%B9%E5%BC%8F%E8%AE%BF%E9%97%AE"><span class="toc-number">2.2.4.2.</span> <span class="toc-text">使用属性方式访问</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%BF%E9%97%AE%E8%A1%8C%E7%9A%84%E7%89%B9%E6%AE%8A%E6%96%B9%E6%B3%95"><span class="toc-number">2.2.4.3.</span> <span class="toc-text">访问行的特殊方法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B4%E6%95%B0%E6%A0%87%E7%AD%BE%E7%9A%84%E7%89%B9%E6%AE%8A%E6%83%85%E5%86%B5"><span class="toc-number">2.2.4.4.</span> <span class="toc-text">整数标签的特殊情况</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%87%E7%89%87%E8%AE%BF%E9%97%AE%E6%96%B9%E6%B3%95"><span class="toc-number">2.2.4.5.</span> <span class="toc-text">切片访问方法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9F%A5%E8%AF%A2%E5%87%BD%E6%95%B0"><span class="toc-number">2.2.4.6.</span> <span class="toc-text">查询函数</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9B%B4%E6%94%B9%E5%90%8D%E7%A7%B0"><span class="toc-number">2.2.5.</span> <span class="toc-text">更改名称</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9B%B4%E6%94%B9-dataframe-%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE"><span class="toc-number">2.2.6.</span> <span class="toc-text">更改 DataFrame 中的数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#sorting-and-ranking"><span class="toc-number">2.2.7.</span> <span class="toc-text">Sorting and Ranking</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#transposing"><span class="toc-number">2.2.8.</span> <span class="toc-text">Transposing</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%87%E7%AD%BE%E9%87%8D%E5%91%BD%E5%90%8D"><span class="toc-number">2.3.</span> <span class="toc-text">标签重命名</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%B4%A2%E5%BC%95"><span class="toc-number">3.</span> <span class="toc-text">索引</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#index-object"><span class="toc-number">3.1.</span> <span class="toc-text">Index object</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E7%BA%A7%E7%B4%A2%E5%BC%95"><span class="toc-number">3.2.</span> <span class="toc-text">多级索引</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#multiindex-%E5%88%9B%E5%BB%BA"><span class="toc-number">3.2.1.</span> <span class="toc-text">MultiIndex 创建</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#reordering-and-sorting-levels"><span class="toc-number">3.2.2.</span> <span class="toc-text">Reordering and Sorting levels</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%B4%A2%E5%BC%95%E9%87%8D%E7%BD%AE"><span class="toc-number">3.3.</span> <span class="toc-text">索引重置</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%87%8D%E5%A4%8D%E6%A0%87%E7%AD%BE%E4%B8%8B%E7%9A%84%E8%BD%B4%E7%B4%A2%E5%BC%95"><span class="toc-number">3.3.1.</span> <span class="toc-text">重复标签下的轴索引</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%B4%A2%E5%BC%95%E9%87%8D%E5%A1%91"><span class="toc-number">3.4.</span> <span class="toc-text">索引重塑</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9F%A5%E6%89%BE%E7%89%B9%E5%AE%9A%E5%80%BC"><span class="toc-number">3.5.</span> <span class="toc-text">查找特定值</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%88%86%E7%BB%84"><span class="toc-number">4.</span> <span class="toc-text">分组</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#groupby-object"><span class="toc-number">4.1.</span> <span class="toc-text">Groupby object</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-groupby-%E8%BF%9B%E8%A1%8C%E6%8F%8F%E8%BF%B0%E6%80%A7%E7%BB%9F%E8%AE%A1"><span class="toc-number">4.2.</span> <span class="toc-text">使用 GroupBy 进行描述性统计</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95"><span class="toc-number">5.</span> <span class="toc-text">基本用法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#pandas-%E8%AF%BB%E5%86%99%E6%96%87%E4%BB%B6"><span class="toc-number">5.1.</span> <span class="toc-text">Pandas 读写文件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%97%E6%9C%AF%E8%BF%90%E7%AE%97%E5%92%8C%E6%95%B0%E6%8D%AE%E5%AF%B9%E9%BD%90"><span class="toc-number">5.2.</span> <span class="toc-text">算术运算和数据对齐</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#df-%E5%92%8C-ser-%E4%B9%8B%E9%97%B4%E7%9A%84%E7%AE%97%E6%9C%AF%E8%BF%90%E7%AE%97"><span class="toc-number">5.2.1.</span> <span class="toc-text">Df 和 Ser 之间的算术运算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97"><span class="toc-number">5.2.2.</span> <span class="toc-text">数学运算</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E8%BD%AC%E6%8D%A2"><span class="toc-number">5.3.</span> <span class="toc-text">数据转换</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E8%BD%AC%E6%8D%A2"><span class="toc-number">5.3.1.</span> <span class="toc-text">数据转换</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#df-replace"><span class="toc-number">5.3.2.</span> <span class="toc-text">df.replace()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#df-str-replace"><span class="toc-number">5.3.3.</span> <span class="toc-text">df.str.replace</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AE"><span class="toc-number">5.4.</span> <span class="toc-text">时间序列数据</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA"><span class="toc-number">5.4.1.</span> <span class="toc-text">创建</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#timestamp-%E6%97%B6%E9%97%B4%E7%82%B9"><span class="toc-number">5.4.1.1.</span> <span class="toc-text">Timestamp–时间点</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%B8%B8%E7%94%A8%E5%B1%9E%E6%80%A7"><span class="toc-number">5.4.1.1.1.</span> <span class="toc-text">常用属性</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#datetimeindex-%E4%B8%8E-periodindex-%E5%87%BD%E6%95%B0"><span class="toc-number">5.4.1.2.</span> <span class="toc-text">DatetimeIndex 与 PeriodIndex 函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#timedelta-%E4%B8%8D%E5%90%8C%E5%8D%95%E4%BD%8D%E7%9A%84%E6%97%B6%E9%97%B4"><span class="toc-number">5.4.1.3.</span> <span class="toc-text">Timedelta–不同单位的时间</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#timedeltaindex"><span class="toc-number">5.4.1.4.</span> <span class="toc-text">TimedeltaIndex</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%BF%E9%97%AE"><span class="toc-number">5.4.2.</span> <span class="toc-text">访问</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%9E%E7%BB%AD%E5%BA%8F%E5%88%97%E4%BA%A7%E7%94%9F-frequencies"><span class="toc-number">5.4.3.</span> <span class="toc-text">连续序列产生&amp;Frequencies</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8C%87%E5%AE%9A%E9%A2%91%E7%8E%87%E7%9A%84%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E7%94%9F%E6%88%90-%E9%A2%91%E7%8E%87%E8%BD%AC%E6%8D%A2%E5%92%8C%E9%87%8D%E9%87%87%E6%A0%B7"><span class="toc-number">5.4.4.</span> <span class="toc-text">指定频率的时间序列生成(频率转换和重采样)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#frequencies-and-date-offsets"><span class="toc-number">5.4.5.</span> <span class="toc-text">Frequencies and Date Offsets</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#period-%E6%97%B6%E9%97%B4%E8%B7%A8%E5%BA%A6%E6%88%96%E6%97%B6%E9%97%B4%E6%AE%B5"><span class="toc-number">5.4.5.1.</span> <span class="toc-text">Period–时间跨度或时间段</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#periodtimeindex"><span class="toc-number">5.4.5.2.</span> <span class="toc-text">PeriodtimeIndex</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#period-frequency-conversion"><span class="toc-number">5.4.5.3.</span> <span class="toc-text">Period Frequency Conversion</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#shifting-leading-and-lagging-data"><span class="toc-number">5.5.</span> <span class="toc-text">Shifting (Leading and Lagging) Data</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%80%BB%E4%BD%93%E7%9A%84%E9%9A%8F%E6%9C%BA%E6%8E%92%E5%88%97-permutation-%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%8A%BD%E6%A0%B7"><span class="toc-number">6.</span> <span class="toc-text">总体的随机排列（permutation）和随机抽样</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%8E%92%E5%88%97"><span class="toc-number">6.1.</span> <span class="toc-text">随机排列</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%8A%BD%E6%A0%B7"><span class="toc-number">6.2.</span> <span class="toc-text">随机抽样</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%95%B4%E7%90%86"><span class="toc-number">7.</span> <span class="toc-text">数据整理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%A0%86%E5%8F%A0"><span class="toc-number">7.1.</span> <span class="toc-text">数据堆叠</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#df%E6%8B%BC%E6%8E%A5"><span class="toc-number">7.2.</span> <span class="toc-text">df拼接</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#df%E9%87%8D%E6%9E%84"><span class="toc-number">7.3.</span> <span class="toc-text">df重构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A1%8C%E5%88%97%E5%80%BC%E7%9A%84%E9%87%8D%E5%A1%91-%E6%95%B0%E6%8D%AE%E9%80%8F%E8%A7%86long-wide"><span class="toc-number">7.3.1.</span> <span class="toc-text">行列值的重塑（数据透视long→wide）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%86%E9%80%8F%E8%A7%86wide-long"><span class="toc-number">7.3.2.</span> <span class="toc-text">逆透视wide→long</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">8.</span> <span class="toc-text">数据预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#data-cleaning"><span class="toc-number">8.1.</span> <span class="toc-text">Data Cleaning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%BB%E9%87%8D"><span class="toc-number">8.1.1.</span> <span class="toc-text">去重</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%BA%E5%A4%B1%E5%80%BC%E5%A4%84%E7%90%86"><span class="toc-number">8.1.2.</span> <span class="toc-text">缺失值处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%88%AA%E6%96%AD"><span class="toc-number">8.1.3.</span> <span class="toc-text">截断</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%82%E5%B8%B8%E5%80%BC%E6%A3%80%E6%B5%8B"><span class="toc-number">8.1.4.</span> <span class="toc-text">异常值检测</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BD%AC%E6%8D%A2%E6%95%B0%E6%8D%AE-%E5%93%91%E5%8F%98%E9%87%8F%E5%A4%84%E7%90%86-index-dummy-variables"><span class="toc-number">8.1.5.</span> <span class="toc-text">转换数据–哑变量处理（Index&#x2F;dummy Variables）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%95%B0%E6%8D%AE"><span class="toc-number">8.1.6.</span> <span class="toc-text">字符串数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%87%8D%E7%BD%AE%E7%B4%A2%E5%BC%95"><span class="toc-number">8.1.7.</span> <span class="toc-text">重置索引</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E7%AD%9B%E9%80%89"><span class="toc-number">8.1.8.</span> <span class="toc-text">数据筛选</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%A0%87%E5%87%86%E5%8C%96"><span class="toc-number">8.2.</span> <span class="toc-text">数据标准化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%88%86%E7%BB%84%E5%92%8C%E8%81%9A%E5%90%88%E6%93%8D%E4%BD%9C"><span class="toc-number">8.3.</span> <span class="toc-text">数据分组和聚合操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%81%9A%E5%90%88"><span class="toc-number">8.3.1.</span> <span class="toc-text">聚合</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-agg-%E6%96%B9%E6%B3%95%E8%81%9A%E5%90%88%E6%95%B0%E6%8D%AE"><span class="toc-number">8.3.1.1.</span> <span class="toc-text">使用 agg 方法聚合数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-apply-%E6%96%B9%E6%B3%95%E8%81%9A%E5%90%88%E6%95%B0%E6%8D%AE"><span class="toc-number">8.3.1.2.</span> <span class="toc-text">使用 apply 方法聚合数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-transform-%E6%96%B9%E6%B3%95%E8%81%9A%E5%90%88%E6%95%B0%E6%8D%AE"><span class="toc-number">8.3.1.3.</span> <span class="toc-text">使用 transform 方法聚合数据</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%80%8F%E8%A7%86%E8%A1%A8%E5%92%8C%E4%BA%A4%E5%8F%89%E8%A1%A8"><span class="toc-number">8.4.</span> <span class="toc-text">透视表和交叉表</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-pivot-table-%E5%88%9B%E5%BB%BA%E9%80%8F%E8%A7%86%E8%A1%A8"><span class="toc-number">8.4.1.</span> <span class="toc-text">使用 pivot_table 创建透视表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-crosstab-%E5%88%9B%E5%BB%BA%E4%BA%A4%E5%8F%89%E8%A1%A8"><span class="toc-number">8.4.2.</span> <span class="toc-text">使用 crosstab 创建交叉表</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BD%AC%E6%8D%A2%E6%95%B0%E6%8D%AE-dataframe-%E6%95%B0%E6%8D%AE%E7%A6%BB%E6%95%A3%E5%8C%96"><span class="toc-number">8.5.</span> <span class="toc-text">转换数据–DataFrame 数据离散化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%9F%E8%AE%A1%E7%AD%89%E5%80%BC%E6%A0%B7%E6%9C%AC%E5%87%BA%E7%8E%B0%E7%9A%84%E9%A2%91%E6%95%B0"><span class="toc-number">8.5.1.</span> <span class="toc-text">统计等值样本出现的频数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%9F%E8%AE%A1%E8%90%BD%E5%85%A5%E6%AF%8F%E4%B8%AA%E5%8C%BA%E9%97%B4%E7%9A%84%E9%A2%91%E6%95%B0-%E7%AD%89%E5%AE%BD%E6%B3%95%E7%A6%BB%E6%95%A3%E6%95%B0%E6%8D%AE"><span class="toc-number">8.5.2.</span> <span class="toc-text">统计落入每个区间的频数(等宽法离散数据)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AD%89%E9%A2%91%E6%B3%95%E7%A6%BB%E6%95%A3%E6%95%B0%E6%8D%AE"><span class="toc-number">8.5.3.</span> <span class="toc-text">等频法离散数据</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#pandas-%E7%BB%9F%E8%AE%A1%E5%88%86%E6%9E%90"><span class="toc-number">9.</span> <span class="toc-text">pandas 统计分析</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#dataframe-%E6%8F%8F%E8%BF%B0%E6%80%A7%E7%BB%9F%E8%AE%A1"><span class="toc-number">9.1.</span> <span class="toc-text">DataFrame 描述性统计</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0"><span class="toc-number">10.</span> <span class="toc-text">窗口函数</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#visualizations"><span class="toc-number">11.</span> <span class="toc-text">visualizations</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/Lasso%E5%9B%9E%E5%BD%92/" title="Lasso回归"><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="/top_img/10048.webp" onerror='this.onerror=null,this.src="https://images.pexels.com/photos/374918/pexels-photo-374918.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=600"' alt="Lasso回归"></a><div class="content"><a class="title" href="/Lasso%E5%9B%9E%E5%BD%92/" title="Lasso回归">Lasso回归</a><time datetime="2023-11-14T16:00:00.000Z" title="更新于 2023-11-15 00:00:00">2023-11-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/%E5%B2%AD%E5%9B%9E%E5%BD%92/" title="岭回归"><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/岭回归_20231109082818.png" onerror='this.onerror=null,this.src="https://images.pexels.com/photos/374918/pexels-photo-374918.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=600"' alt="岭回归"></a><div class="content"><a class="title" href="/%E5%B2%AD%E5%9B%9E%E5%BD%92/" title="岭回归">岭回归</a><time datetime="2023-11-07T16:00:00.000Z" title="更新于 2023-11-08 00:00:00">2023-11-08</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/%E4%BC%98%E9%9B%85%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/" title="优雅论文排版"><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/优雅论文排版_20230921093206.png" onerror='this.onerror=null,this.src="https://images.pexels.com/photos/374918/pexels-photo-374918.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=600"' alt="优雅论文排版"></a><div class="content"><a class="title" href="/%E4%BC%98%E9%9B%85%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/" title="优雅论文排版">优雅论文排版</a><time datetime="2023-09-20T16:00:00.000Z" title="更新于 2023-09-21 00:00:00">2023-09-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/%E5%A4%9A%E5%85%83%E7%BB%9F%E8%AE%A1%E5%88%86%E6%9E%90/" title="多元统计分析"><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/多元统计分析_多元正态曲线.png" onerror='this.onerror=null,this.src="https://images.pexels.com/photos/374918/pexels-photo-374918.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=600"' alt="多元统计分析"></a><div class="content"><a class="title" href="/%E5%A4%9A%E5%85%83%E7%BB%9F%E8%AE%A1%E5%88%86%E6%9E%90/" title="多元统计分析">多元统计分析</a><time datetime="2023-06-16T02:22:54.000Z" title="更新于 2023-06-16 10:22:54">2023-06-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/Hypothesis%20testing/" title="假设检验"><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="/top_img/10036.webp" onerror='this.onerror=null,this.src="https://images.pexels.com/photos/374918/pexels-photo-374918.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=600"' alt="假设检验"></a><div class="content"><a class="title" href="/Hypothesis%20testing/" title="假设检验">假设检验</a><time datetime="2023-05-09T02:34:00.000Z" title="更新于 2023-05-09 10:34:00">2023-05-09</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="footer_custom_text"><div id="runtime"></div></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"></div><div id="rightside-config-show"><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><div class="js-pjax"></div><script src="/Scripts/js/beijing.js"></script><script src="/Scripts/js/foot_style.js"></script><script src="/Scripts/js/fireworks.js"></script><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>(()=>{window.pjax=new Pjax({elements:'a:not([target="_blank"])',selectors:["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"],cacheBust:!1,analytics:!1,scrollRestoration:!1});const e=e=>{e&&Object.values(e).forEach((e=>e()))};document.addEventListener("pjax:send",(()=>{btf.removeGlobalFnEvent("pjaxSendOnce"),btf.removeGlobalFnEvent("themeChange");const t=document.body.classList;t.contains("read-mode")&&t.remove("read-mode"),e(window.globalFn.pjaxSend)})),document.addEventListener("pjax:complete",(()=>{btf.removeGlobalFnEvent("pjaxCompleteOnce"),document.querySelectorAll("script[data-pjax]").forEach((e=>{const t=document.createElement("script"),n=e.text||e.textContent||e.innerHTML||"";Array.from(e.attributes).forEach((e=>t.setAttribute(e.name,e.value))),t.appendChild(document.createTextNode(n)),e.parentNode.replaceChild(t,e)})),e(window.globalFn.pjaxComplete)})),document.addEventListener("pjax:error",(e=>{404===e.request.status&&pjax.loadUrl("/404.html")}))})()</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><div class="js-pjax"><script async>for(var arr=document.getElementsByClassName("recent-post-item"),i=0;i<arr.length;i++)arr[i].classList.add("wow"),arr[i].classList.add("animate__zoomIn"),arr[i].setAttribute("data-wow-duration","2s"),arr[i].setAttribute("data-wow-delay","0.5s"),arr[i].setAttribute("data-wow-offset","100"),arr[i].setAttribute("data-wow-iteration","1")</script><script async>for(var arr=document.getElementsByClassName("card-widget"),i=0;i<arr.length;i++)arr[i].classList.add("wow"),arr[i].classList.add("animate__zoomIn"),arr[i].setAttribute("data-wow-duration",""),arr[i].setAttribute("data-wow-delay",""),arr[i].setAttribute("data-wow-offset",""),arr[i].setAttribute("data-wow-iteration","")</script></div><script defer src="https://unpkg.zhimg.com/hexo-butterfly-wowjs/lib/wow.min.js"></script><script defer src="https://unpkg.zhimg.com/hexo-butterfly-wowjs/lib/wow_init.js"></script></body></html>