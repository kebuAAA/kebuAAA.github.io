<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Scrapy框架 | 小明的博客</title><meta name="keywords" content="Python爬虫"><meta name="author" content="可不"><meta name="copyright" content="可不"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#e68ab8"><meta name="description" content="简单网页的爬取可以利用re模块，复杂网页的爬取对于内容的提取则会显得十分麻烦。Scrapy框架是python下的一个爬虫框架，因为它足够简单方便受到人们的青睐。 选择器（提取数据的机制）Scrapy提取数据有自己的一套机制。 它们被称作选择器（seletors)，通过特定的XPath或者CSS表达式来“选择”HTML文件中的某个部分。XPath是一门用来在XML文件中选择节点的语言， 也可以用在">
<meta property="og:type" content="article">
<meta property="og:title" content="Scrapy框架">
<meta property="og:url" content="https://kebuaaa.github.io/Scrapy%E6%A1%86%E6%9E%B6/index.html">
<meta property="og:site_name" content="小明的博客">
<meta property="og:description" content="简单网页的爬取可以利用re模块，复杂网页的爬取对于内容的提取则会显得十分麻烦。Scrapy框架是python下的一个爬虫框架，因为它足够简单方便受到人们的青睐。 选择器（提取数据的机制）Scrapy提取数据有自己的一套机制。 它们被称作选择器（seletors)，通过特定的XPath或者CSS表达式来“选择”HTML文件中的某个部分。XPath是一门用来在XML文件中选择节点的语言， 也可以用在">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://images.pexels.com/photos/733854/pexels-photo-733854.jpeg?auto=compress&cs=tinysrgb&w=600">
<meta property="article:published_time" content="2022-03-26T02:34:00.000Z">
<meta property="article:modified_time" content="2022-07-09T02:00:34.651Z">
<meta property="article:author" content="可不">
<meta property="article:tag" content="Python爬虫">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://images.pexels.com/photos/733854/pexels-photo-733854.jpeg?auto=compress&cs=tinysrgb&w=600"><link rel="shortcut icon" href="../img/favicon.png"><link rel="canonical" href="https://kebuaaa.github.io/Scrapy%E6%A1%86%E6%9E%B6/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="../css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":1,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"prismjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Scrapy框架',
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2022-07-09 10:00:34'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#e68ab8')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.2"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="../img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="../archives/"><div class="headline">文章</div><div class="length-num">90</div></a><a href="../tags/"><div class="headline">标签</div><div class="length-num">61</div></a><a href="../categories/"><div class="headline">分类</div><div class="length-num">18</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="../index.html"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="../archives/"><i class="fa-fw fas fa-archive"></i><span> 存档</span></a></div><div class="menus_item"><a class="site-page" href="../tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="../categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="../link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="../about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://images.pexels.com/photos/733854/pexels-photo-733854.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=600')"><nav id="nav"><span id="blog_name"><a id="site-name" href="../index.html">小明的博客</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="../index.html"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="../archives/"><i class="fa-fw fas fa-archive"></i><span> 存档</span></a></div><div class="menus_item"><a class="site-page" href="../tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="../categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="../link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="../about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Scrapy框架</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-03-26T02:34:00.000Z" title="发表于 2022-03-26 10:34:00">2022-03-26</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-07-09T02:00:34.651Z" title="更新于 2022-07-09 10:00:34">2022-07-09</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="../categories/%E7%88%AC%E8%99%AB/">爬虫</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="../categories/%E7%88%AC%E8%99%AB/%E7%90%86%E8%AE%BA/">理论</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Scrapy框架"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p><img src="../md_imgs/Scrapy框架/0.jpg" alt="image.png"></p>
<p>简单网页的爬取可以利用re模块，复杂网页的爬取对于内容的提取则会显得十分麻烦。Scrapy框架是python下的一个爬虫框架，因为它足够简单方便受到人们的青睐。</p>
<h1 id="选择器（提取数据的机制）"><a href="#选择器（提取数据的机制）" class="headerlink" title="选择器（提取数据的机制）"></a>选择器（提取数据的机制）</h1><p>Scrapy提取数据有自己的一套机制。 它们被称作选择器（seletors)，通过特定的XPath或者CSS表达式来“选择”HTML文件中的某个部分。XPath是一门用来在XML文件中选择节点的语言， 也可以用在HTML上。 CSS是一门将HTML文档样式化的语言。 选择器由它定义，并与特定的HTML元素的样式相关联。<br>Scrapy的选择器构建于lxml库之上， 这意味着它们在速度和解析准确性上非常相似， 所以看你喜欢哪种选择器就使用哪种吧， 它们从效率上看完全没有区别。</p>
<h2 id="XPath选择器"><a href="#XPath选择器" class="headerlink" title="XPath选择器"></a>XPath选择器</h2><p>XPath是一门在XML文档中查找信息的语言。 如果实在不想自己写的话可以借助edge浏览器的插件SelectorGadget 给自动生成一下<br>在XPath中， 有7种类型的节点： 元素、 属性、 文本、 命名空间、 处理指令、 注释以及文档节点（或称为根节点） 。 XML文档是被作为节点树来对待的。 树的根被称为文档节点或者根节点。<br> 下面以一个简单的xml文件进行说明<br><pre class="line-numbers language-markup" data-language="markup"><code class="language-markup"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>superhero</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>class</span><span class="token punctuation">></span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span> <span class="token attr-name">lang</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>en<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>Tony Stark <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>alias</span><span class="token punctuation">></span></span>Iron Man <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>alias</span><span class="token punctuation">></span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>sex</span><span class="token punctuation">></span></span>male <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>sex</span><span class="token punctuation">></span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>birthday</span><span class="token punctuation">></span></span>1969 <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>birthday</span><span class="token punctuation">></span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>age</span><span class="token punctuation">></span></span>47 <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>age</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>class</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>class</span><span class="token punctuation">></span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span> <span class="token attr-name">lang</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>en<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>Peter Benjamin Parker <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>alias</span><span class="token punctuation">></span></span>Spider Man <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>alias</span><span class="token punctuation">></span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>sex</span><span class="token punctuation">></span></span>male <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>sex</span><span class="token punctuation">></span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>birthday</span><span class="token punctuation">></span></span>unknow <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>birthday</span><span class="token punctuation">></span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>age</span><span class="token punctuation">></span></span>unknown <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>age</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>class</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>class</span><span class="token punctuation">></span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span> <span class="token attr-name">lang</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>en<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>Steven Rogers <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>alias</span><span class="token punctuation">></span></span>Captain America <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>alias</span><span class="token punctuation">></span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>sex</span><span class="token punctuation">></span></span>male <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>sex</span><span class="token punctuation">></span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>birthday</span><span class="token punctuation">></span></span>19200704 <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>birthday</span><span class="token punctuation">></span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>age</span><span class="token punctuation">></span></span>96 <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>age</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>class</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>superhero</span><span class="token punctuation">></span></span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>在这个例子中<superhero>是文档节点，<name lang="en">Tony Stark </name>是元素节点，lang=”en”是属性。Xpath通过在文档中选取节点来进行数据匹配：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>nodeName</th>
<th>提取节点的所有子节点</th>
</tr>
</thead>
<tbody>
<tr>
<td>/</td>
<td>从根节点选取</td>
</tr>
<tr>
<td>//+节点名称</td>
<td>从匹配选择的当前节点选择文档中的节点，不考虑他们的位置</td>
</tr>
<tr>
<td>.</td>
<td>选取当前节点</td>
</tr>
<tr>
<td>..</td>
<td>选取当前节点的父节点</td>
</tr>
<tr>
<td>@+属性名称</td>
<td>选择属性</td>
</tr>
<tr>
<td>*</td>
<td>匹配任何元素节点</td>
</tr>
<tr>
<td>@*</td>
<td>匹配任何属性节点</td>
</tr>
<tr>
<td>Node()</td>
<td>匹配任何类型的节点</td>
</tr>
<tr>
<td>/text（）</td>
<td>节点的文本内容提取</td>
</tr>
<tr>
<td>@href</td>
<td>节点href属性的值</td>
</tr>
</tbody>
</table>
</div>
<p>实际运用：</p>
<ul>
<li>“//div[@id=”images”]/a/text()”，节点名称为div属性为images的a节点的文本内容</li>
<li></li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> os
<span class="token keyword">from</span> scrapy<span class="token punctuation">.</span>selector <span class="token keyword">import</span> Selector <span class="token keyword">as</span> se
mypath<span class="token operator">=</span>os<span class="token punctuation">.</span>getcwd<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">+</span><span class="token string">"/爬虫/code/crawler_script/superHero.xml"</span>
<span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>mypath<span class="token punctuation">)</span> <span class="token keyword">as</span> fp<span class="token punctuation">:</span>
    body<span class="token operator">=</span>fp<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment">#print(body)</span>
<span class="token comment">#print(se(text=body).xpath('/*').extract())</span>
<span class="token comment">#采集第一个class</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>se<span class="token punctuation">(</span>text<span class="token operator">=</span>body<span class="token punctuation">)</span><span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'/html/body/superhero/class[1]'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment">#采集name属性为en的数据</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>se<span class="token punctuation">(</span>text<span class="token operator">=</span>body<span class="token punctuation">)</span><span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'//name[@lang="en"]'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h1 id="Response（Using-selectors）"><a href="#Response（Using-selectors）" class="headerlink" title="Response（Using selectors）"></a>Response（Using selectors）</h1><p>定义在Spider.py中的parse（）方法是<code>[TextResponse](https://docs.scrapy.org/en/latest/topics/request-response.html#scrapy.http.TextResponse)</code>类的一个实例，用来处理每次发起的网页请求传回来的响应文件，可以在这里定义对响应文件的提取规则等内容（请求的回调方法）。其输入的参数response其实就是网页请求的响应文件，本身可以作为选择器使用。<br><pre class="line-numbers language-python" data-language="python"><code class="language-python">response<span class="token punctuation">.</span>selector<span class="token punctuation">(</span><span class="token string">""</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><br>其中selector表示具体的选择器，如xpath，css，re等<br>需要注意的是，使用response.xpath()方法的返回值仍然是一个选择器，也就是说可以继续对提取结果进行进一步的筛选，比如可以对筛选出来的文本继续用re模块进行匹配：<br><pre class="line-numbers language-python" data-language="python"><code class="language-python">response<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>re<span class="token punctuation">(</span><span class="token punctuation">)</span>
sel<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">"string(//a[1])"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>getall<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># convert it to string</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><br>欲将提取结果进行显示，可以借助<code>extract()</code>或者<code>get()</code>函数，默认情况下对于没有数据可以被提取出来时输出None，可以通过给default参数赋其他值来调节：</p>
<ul>
<li><code>get()</code>返回一条结果</li>
<li><code>getall()</code>：返回所有结果</li>
<li><code>extract()</code>:返回所有结果</li>
<li><code>extract_first</code>：返回第一个结果</li>
</ul>
<p>调用<code>getall</code>返回的是一个列表，当爬取的数据不存在时，对列表的索引会导致程序出现<code>IndexError</code>停止，言外之意是不要随意对返回列表进行索引：<br><img src="../md_imgs/Scrapy框架/1.png" alt="image.png"><br>这种情况可以考虑用<code>get（）</code>代替，在有数据时会返回一样的结果，没有的话也只是会返回<code>None</code></p>
<h1 id="Spider"><a href="#Spider" class="headerlink" title="Spider"></a>Spider</h1><p>Scrapy中有一个Spider类，该类并没有提供什么特殊的功能。首先通过初始化的request(<code>start_requests()</code>)去爬取指定的初始链接(<code>start_urls</code>），然后制定一个回调函数（callback ）来处理从网页请求中下载的回应（response）。 在制作自己需要的爬虫规则时，必须先继承Spider类。<br>类的属性：</p>
<ul>
<li>name：自己定义的spider的名字</li>
<li>allowed_domains：包含了spider允许爬取的域名(domain)列表(list)</li>
<li>start_urls：URL列表。当没有制定特定的URL时，spider将从该列表中开始进行爬取。 因此，第一个被获取到的页面的URL将是该列表之一。 后续的URL将会从获取到的数据中提取。</li>
<li>custom_settings:对项目的设置文件进行重写，它必须定义为类属性，因为设置在实例化之前更新。</li>
<li></li>
</ul>
<h2 id="提取爬取结果"><a href="#提取爬取结果" class="headerlink" title="提取爬取结果"></a>提取爬取结果</h2><p>当我们对爬虫的结果进行返回时，默认返回一个字典形式的数据。为了让Scrapy也实现这样的效果，我们可以借助<code>yield</code>来实现：<br><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> scrapy


<span class="token keyword">class</span> <span class="token class-name">QuotesSpider</span><span class="token punctuation">(</span>scrapy<span class="token punctuation">.</span>Spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
    name <span class="token operator">=</span> <span class="token string">"quotes"</span>
    start_urls <span class="token operator">=</span> <span class="token punctuation">[</span>
        <span class="token string">'https://quotes.toscrape.com/page/1/'</span><span class="token punctuation">,</span>
        <span class="token string">'https://quotes.toscrape.com/page/2/'</span><span class="token punctuation">,</span>
    <span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">parse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> quote <span class="token keyword">in</span> response<span class="token punctuation">.</span>css<span class="token punctuation">(</span><span class="token string">'div.quote'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">yield</span> <span class="token punctuation">&#123;</span>
                <span class="token string">'text'</span><span class="token punctuation">:</span> quote<span class="token punctuation">.</span>css<span class="token punctuation">(</span><span class="token string">'span.text::text'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                <span class="token string">'author'</span><span class="token punctuation">:</span> quote<span class="token punctuation">.</span>css<span class="token punctuation">(</span><span class="token string">'small.author::text'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                <span class="token string">'tags'</span><span class="token punctuation">:</span> quote<span class="token punctuation">.</span>css<span class="token punctuation">(</span><span class="token string">'div.tags a.tag::text'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>getall<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            <span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>爬取正常时显示的结果(日志中)：<br><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token number">2016</span><span class="token operator">-</span><span class="token number">09</span><span class="token operator">-</span><span class="token number">19</span> <span class="token number">18</span><span class="token punctuation">:</span><span class="token number">57</span><span class="token punctuation">:</span><span class="token number">19</span> <span class="token punctuation">[</span>scrapy<span class="token punctuation">.</span>core<span class="token punctuation">.</span>scraper<span class="token punctuation">]</span> DEBUG<span class="token punctuation">:</span> Scraped <span class="token keyword">from</span> <span class="token operator">&lt;</span><span class="token number">200</span> https<span class="token punctuation">:</span><span class="token operator">//</span>quotes<span class="token punctuation">.</span>toscrape<span class="token punctuation">.</span>com<span class="token operator">/</span>page<span class="token operator">/</span><span class="token number">1</span><span class="token operator">/</span><span class="token operator">></span>
<span class="token punctuation">&#123;</span><span class="token string">'tags'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'life'</span><span class="token punctuation">,</span> <span class="token string">'love'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">'author'</span><span class="token punctuation">:</span> <span class="token string">'André Gide'</span><span class="token punctuation">,</span> <span class="token string">'text'</span><span class="token punctuation">:</span> <span class="token string">'“It is better to be hated for what you are than to be loved for what you are not.”'</span><span class="token punctuation">&#125;</span>
<span class="token number">2016</span><span class="token operator">-</span><span class="token number">09</span><span class="token operator">-</span><span class="token number">19</span> <span class="token number">18</span><span class="token punctuation">:</span><span class="token number">57</span><span class="token punctuation">:</span><span class="token number">19</span> <span class="token punctuation">[</span>scrapy<span class="token punctuation">.</span>core<span class="token punctuation">.</span>scraper<span class="token punctuation">]</span> DEBUG<span class="token punctuation">:</span> Scraped <span class="token keyword">from</span> <span class="token operator">&lt;</span><span class="token number">200</span> https<span class="token punctuation">:</span><span class="token operator">//</span>quotes<span class="token punctuation">.</span>toscrape<span class="token punctuation">.</span>com<span class="token operator">/</span>page<span class="token operator">/</span><span class="token number">1</span><span class="token operator">/</span><span class="token operator">></span>
<span class="token punctuation">&#123;</span><span class="token string">'tags'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'edison'</span><span class="token punctuation">,</span> <span class="token string">'failure'</span><span class="token punctuation">,</span> <span class="token string">'inspirational'</span><span class="token punctuation">,</span> <span class="token string">'paraphrased'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">'author'</span><span class="token punctuation">:</span> <span class="token string">'Thomas A. Edison'</span><span class="token punctuation">,</span> <span class="token string">'text'</span><span class="token punctuation">:</span> <span class="token string">"“I have not failed. I've just found 10,000 ways that won't work.”"</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></p>
<h2 id="保存爬取结果"><a href="#保存爬取结果" class="headerlink" title="保存爬取结果"></a>保存爬取结果</h2><p>最简单的导出爬取结果的方法为:<br><pre class="line-numbers language-python" data-language="python"><code class="language-python">scrapy crawl quotes <span class="token operator">-</span>O quotes<span class="token punctuation">.</span>json<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><br>“quotes.json”限定了保存文件的格式与名称。也可以导出为csv格式或者JSON Lines格式（jl）<br>csv文件存储的一个好处是能把一个节点所有的文字变成一句话，如果是json格式，保存的会是一个字符串列表。<br>如果想要保存在数据库等操作，需要借助pipelines文件</p>
<h2 id="增加参数"><a href="#增加参数" class="headerlink" title="增加参数"></a>增加参数</h2><p>可以在命令进行操作给Spider类添加任何需要的参数：<br><pre class="line-numbers language-python" data-language="python"><code class="language-python">scrapy crawl myspider <span class="token operator">-</span>a category<span class="token operator">=</span>electronics<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><br>意思即为添加一个值为electronics的属性category</p>
<h2 id="跟踪链接（多个网页的跳转抓取）"><a href="#跟踪链接（多个网页的跳转抓取）" class="headerlink" title="跟踪链接（多个网页的跳转抓取）"></a>跟踪链接（多个网页的跳转抓取）</h2><p>对于有多个相关联的网页内容的抓取，我们可以通过定义parse方法的内容实现。首先利用匹配原则提取出网页跳转的链接，然后再借助response的urljoin方法将待抓取的链接构建一个完整的链接，最后再调用yield来发出一个请求，然后Scrapy会安排送入的网页（next_page）进行访问请求，并在请求结束后利用定义的回调方法（self.parse）执行回调。<br><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> scrapy


<span class="token keyword">class</span> <span class="token class-name">QuotesSpider</span><span class="token punctuation">(</span>scrapy<span class="token punctuation">.</span>Spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
    name <span class="token operator">=</span> <span class="token string">"quotes"</span>
    start_urls <span class="token operator">=</span> <span class="token punctuation">[</span>
        <span class="token string">'https://quotes.toscrape.com/page/1/'</span><span class="token punctuation">,</span>
    <span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">parse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> quote <span class="token keyword">in</span> response<span class="token punctuation">.</span>css<span class="token punctuation">(</span><span class="token string">'div.quote'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">yield</span> <span class="token punctuation">&#123;</span>
                <span class="token string">'text'</span><span class="token punctuation">:</span> quote<span class="token punctuation">.</span>css<span class="token punctuation">(</span><span class="token string">'span.text::text'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                <span class="token string">'author'</span><span class="token punctuation">:</span> quote<span class="token punctuation">.</span>css<span class="token punctuation">(</span><span class="token string">'small.author::text'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                <span class="token string">'tags'</span><span class="token punctuation">:</span> quote<span class="token punctuation">.</span>css<span class="token punctuation">(</span><span class="token string">'div.tags a.tag::text'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>getall<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            <span class="token punctuation">&#125;</span>

        next_page <span class="token operator">=</span> response<span class="token punctuation">.</span>css<span class="token punctuation">(</span><span class="token string">'li.next a::attr(href)'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> next_page <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            next_page <span class="token operator">=</span> response<span class="token punctuation">.</span>urljoin<span class="token punctuation">(</span>next_page<span class="token punctuation">)</span>
            <span class="token keyword">yield</span> scrapy<span class="token punctuation">.</span>Request<span class="token punctuation">(</span>next_page<span class="token punctuation">,</span> callback<span class="token operator">=</span>self<span class="token punctuation">.</span>parse<span class="token punctuation">)</span>
        <span class="token comment">#一个更加方便的方法</span>
        <span class="token comment">#if next_page is not None:</span>
            <span class="token comment">#yield response.follow(next_page, callback=self.parse)</span>
        <span class="token comment">#follow只返回了网页请求，仍需要进行回调</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>与urljoin+Request的方法相比,response.follow提供了一种更加便捷的方法。该方法可以自动对selector类型进行处理（自动提取出节点中的链接）：<br><pre class="line-numbers language-python" data-language="python"><code class="language-python">next_page <span class="token operator">=</span> response<span class="token punctuation">.</span>css<span class="token punctuation">(</span><span class="token string">'li.next a::attr(href)'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> next_page <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            <span class="token keyword">yield</span> response<span class="token punctuation">.</span>follow<span class="token punctuation">(</span>next_page<span class="token punctuation">,</span> callback<span class="token operator">=</span>self<span class="token punctuation">.</span>parse<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><br>另外如果当所有的网页链接可以从一个迭代对象中爬取时，response.follow_all()方法提供了更为便捷的方法。<br><pre class="line-numbers language-python" data-language="python"><code class="language-python">anchors <span class="token operator">=</span> response<span class="token punctuation">.</span>css<span class="token punctuation">(</span><span class="token string">'ul.pager a'</span><span class="token punctuation">)</span>
<span class="token keyword">yield</span> <span class="token keyword">from</span> response<span class="token punctuation">.</span>follow_all<span class="token punctuation">(</span>anchors<span class="token punctuation">,</span> callback<span class="token operator">=</span>self<span class="token punctuation">.</span>parse<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></p>
<h3 id="example"><a href="#example" class="headerlink" title="example"></a>example</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> scrapy


<span class="token keyword">class</span> <span class="token class-name">AuthorSpider</span><span class="token punctuation">(</span>scrapy<span class="token punctuation">.</span>Spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
    name <span class="token operator">=</span> <span class="token string">'author'</span>

    start_urls <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'https://quotes.toscrape.com/'</span><span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">parse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>
        author_page_links <span class="token operator">=</span> response<span class="token punctuation">.</span>css<span class="token punctuation">(</span><span class="token string">'.author + a'</span><span class="token punctuation">)</span>
        <span class="token keyword">yield</span> <span class="token keyword">from</span> response<span class="token punctuation">.</span>follow_all<span class="token punctuation">(</span>author_page_links<span class="token punctuation">,</span> self<span class="token punctuation">.</span>parse_author<span class="token punctuation">)</span>

        pagination_links <span class="token operator">=</span> response<span class="token punctuation">.</span>css<span class="token punctuation">(</span><span class="token string">'li.next a'</span><span class="token punctuation">)</span>
        <span class="token keyword">yield</span> <span class="token keyword">from</span> response<span class="token punctuation">.</span>follow_all<span class="token punctuation">(</span>pagination_links<span class="token punctuation">,</span> self<span class="token punctuation">.</span>parse<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">parse_author</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">def</span> <span class="token function">extract_with_css</span><span class="token punctuation">(</span>query<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">return</span> response<span class="token punctuation">.</span>css<span class="token punctuation">(</span>query<span class="token punctuation">)</span><span class="token punctuation">.</span>get<span class="token punctuation">(</span>default<span class="token operator">=</span><span class="token string">''</span><span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token keyword">yield</span> <span class="token punctuation">&#123;</span>
            <span class="token string">'name'</span><span class="token punctuation">:</span> extract_with_css<span class="token punctuation">(</span><span class="token string">'h3.author-title::text'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            <span class="token string">'birthdate'</span><span class="token punctuation">:</span> extract_with_css<span class="token punctuation">(</span><span class="token string">'.author-born-date::text'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            <span class="token string">'bio'</span><span class="token punctuation">:</span> extract_with_css<span class="token punctuation">(</span><span class="token string">'.author-description::text'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token punctuation">&#125;</span><span class="token comment">#最好的书写是将在items文件中声明好格式，不建议这样写</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>默认情况下，Scrapy 会过滤掉对已经访问过的 URL 的重复请求，避免由于编程错误而过多地访问服务器的问题。这可以通过设置 <code>DUPEFILTER_CLASS</code> 进行配置。<br>这是一个避免从多个页面</p>
<h2 id="动态网页"><a href="#动态网页" class="headerlink" title="动态网页"></a>动态网页</h2><p>动态网页的爬取意味着我们可能需要对headers和cookies进行调整。具体参考：<br><a target="_blank" rel="noopener" href="https://www.yuque.com/u22723808/python/wksuec?view=doc_embed&amp;inner=fppJl">网页抓取教程</a></p>
<h2 id="生成来自多个页面数据组成的item"><a href="#生成来自多个页面数据组成的item" class="headerlink" title="生成来自多个页面数据组成的item"></a>生成来自多个页面数据组成的item</h2><p>using a <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/request-response.html#topics-request-response-ref-request-callback-arguments">trick to pass additional data to the callbacks</a>.</p>
<h1 id="通用爬虫"><a href="#通用爬虫" class="headerlink" title="通用爬虫"></a>通用爬虫</h1><p>Scrapy除了提供Spider类之外，还提供了其他的类来简化我们的工作（对一些稍微有针对性一点的功能进行了封装）</p>
<h2 id="class-scrapy-spiders-CrawlSpider"><a href="#class-scrapy-spiders-CrawlSpider" class="headerlink" title="class scrapy.spiders.CrawlSpider"></a>class scrapy.spiders.CrawlSpider</h2><p>创建：<br><pre class="line-numbers language-python" data-language="python"><code class="language-python">Scrapy genspider <span class="token operator">-</span>t crawl <span class="token punctuation">[</span>爬虫名字<span class="token punctuation">]</span> <span class="token punctuation">[</span>域名<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><br>CrawSpider类的爬虫被广泛应用于爬取各类常规网站。它通过定义一组规则为跟踪链接提供了更加便捷的方法。与Spider类相比，该类新增加了两个属性：</p>
<ul>
<li>rules：包含一系列<code>Rule</code>类，每一个<code>Rule</code>类定义了爬取网站的原则（是否跟踪，是否对输入的链接进行爬取）</li>
<li><p><strong>parse_start_url(</strong>_<strong>response</strong>_<strong>, </strong>_<strong>**kwargs</strong>_<strong>)：可以进行重写的方法</strong></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> scrapy
<span class="token keyword">from</span> scrapy<span class="token punctuation">.</span>spiders <span class="token keyword">import</span> CrawlSpider<span class="token punctuation">,</span> Rule
<span class="token keyword">from</span> scrapy<span class="token punctuation">.</span>linkextractors <span class="token keyword">import</span> LinkExtractor

<span class="token keyword">class</span> <span class="token class-name">MySpider</span><span class="token punctuation">(</span>CrawlSpider<span class="token punctuation">)</span><span class="token punctuation">:</span>
    name <span class="token operator">=</span> <span class="token string">'example.com'</span>
    allowed_domains <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'example.com'</span><span class="token punctuation">]</span>
    start_urls <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'http://www.example.com'</span><span class="token punctuation">]</span>

    rules <span class="token operator">=</span> <span class="token punctuation">(</span>
        <span class="token comment"># Extract links matching 'category.php' (but not matching 'subsection.php')</span>
        <span class="token comment"># and follow links from them (since no callback means follow=True by default).</span>
        Rule<span class="token punctuation">(</span>LinkExtractor<span class="token punctuation">(</span>allow<span class="token operator">=</span><span class="token punctuation">(</span><span class="token string">'category\.php'</span><span class="token punctuation">,</span> <span class="token punctuation">)</span><span class="token punctuation">,</span> deny<span class="token operator">=</span><span class="token punctuation">(</span><span class="token string">'subsection\.php'</span><span class="token punctuation">,</span> <span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>

        <span class="token comment"># Extract links matching 'item.php' and parse them with the spider's method parse_item</span>
        Rule<span class="token punctuation">(</span>LinkExtractor<span class="token punctuation">(</span>allow<span class="token operator">=</span><span class="token punctuation">(</span><span class="token string">'item\.php'</span><span class="token punctuation">,</span> <span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> callback<span class="token operator">=</span><span class="token string">'parse_item'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">parse_item</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>logger<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token string">'Hi, this is an item page! %s'</span><span class="token punctuation">,</span> response<span class="token punctuation">.</span>url<span class="token punctuation">)</span>
        item <span class="token operator">=</span> scrapy<span class="token punctuation">.</span>Item<span class="token punctuation">(</span><span class="token punctuation">)</span>
        item<span class="token punctuation">[</span><span class="token string">'id'</span><span class="token punctuation">]</span> <span class="token operator">=</span> response<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'//td[@id="item_id"]/text()'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>re<span class="token punctuation">(</span><span class="token string">r'ID: (\d+)'</span><span class="token punctuation">)</span>
        item<span class="token punctuation">[</span><span class="token string">'name'</span><span class="token punctuation">]</span> <span class="token operator">=</span> response<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'//td[@id="item_name"]/text()'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token punctuation">)</span>
        item<span class="token punctuation">[</span><span class="token string">'description'</span><span class="token punctuation">]</span> <span class="token operator">=</span> response<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'//td[@id="item_description"]/text()'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token punctuation">)</span>
        item<span class="token punctuation">[</span><span class="token string">'link_text'</span><span class="token punctuation">]</span> <span class="token operator">=</span> response<span class="token punctuation">.</span>meta<span class="token punctuation">[</span><span class="token string">'link_text'</span><span class="token punctuation">]</span>
        url <span class="token operator">=</span> response<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'//td[@id="additional_data"]/@href'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> response<span class="token punctuation">.</span>follow<span class="token punctuation">(</span>url<span class="token punctuation">,</span> self<span class="token punctuation">.</span>parse_additional_page<span class="token punctuation">,</span> cb_kwargs<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>item<span class="token operator">=</span>item<span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">parse_additional_page</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">,</span> item<span class="token punctuation">)</span><span class="token punctuation">:</span>
        item<span class="token punctuation">[</span><span class="token string">'additional_data'</span><span class="token punctuation">]</span> <span class="token operator">=</span> response<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'//p[@id="additional_data"]/text()'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> item<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><code>Link_extractor</code>类主要帮助我们对我们需要的url进行一个筛选（通常利用正则表达式指定筛选原则）常用的两个属性为：</p>
</li>
<li><p>allow：正则表达式，表示需要进行提取的url</p>
</li>
<li>deny：禁止的url</li>
<li>allow_domains:</li>
<li>deny_domains:</li>
</ul>
<p><code>Rule</code>类的主要属性有：</p>
<ul>
<li>link_extractor：<code>Link_extractor</code>的一个实例。对网页进行筛选</li>
<li>callback：用来规定使用的回调函数</li>
<li>follow：布尔值，用来规定是否跟踪网页</li>
<li><p>process_links:从link_extractor传递给这个函数，用来规定不需要爬取的链接</p>
<h1 id="item-pipelines"><a href="#item-pipelines" class="headerlink" title="item pipelines"></a>item pipelines</h1><p>理论上来讲，对网页抓取的数据可以选择放在parse函数中继续处理，但这种方法会牺牲网页抓取的速度，因此我们通常选择用parse函数做一个网页数据抓取，网页数据的处理和写入则放在交给pipelines<br>该类主要给了四个方法的定义。</p>
</li>
<li><p><code>process_item(self, item, spider)</code>item指返回的Item（类），spider指定义的spider</p>
</li>
<li><code>open_spider(self, spider)</code>通过该方法在爬虫开始时进行调整</li>
<li><code>close_spider(self, spider)</code>在爬虫结束时进行相关操作</li>
<li><code>from_crawler(cls, crawler)</code>：类方法，用来获取Scrapy的配置信息</li>
</ul>
<p>该函数会在网页数据抓取后自动进行，为了保证它的运行，一定要记得网页数据提取时要有返回值（yield或者return）。</p>
<h2 id="Some-examples"><a href="#Some-examples" class="headerlink" title="Some examples"></a>Some examples</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> itemadapter <span class="token keyword">import</span> ItemAdapter
<span class="token keyword">from</span> scrapy<span class="token punctuation">.</span>exceptions <span class="token keyword">import</span> DropItem
<span class="token keyword">class</span> <span class="token class-name">PricePipeline</span><span class="token punctuation">:</span>

    vat_factor <span class="token operator">=</span> <span class="token number">1.15</span>

    <span class="token keyword">def</span> <span class="token function">process_item</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> item<span class="token punctuation">,</span> spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
        adapter <span class="token operator">=</span> ItemAdapter<span class="token punctuation">(</span>item<span class="token punctuation">)</span>
        <span class="token keyword">if</span> adapter<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">'price'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> adapter<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">'price_excludes_vat'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                adapter<span class="token punctuation">[</span><span class="token string">'price'</span><span class="token punctuation">]</span> <span class="token operator">=</span> adapter<span class="token punctuation">[</span><span class="token string">'price'</span><span class="token punctuation">]</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>vat_factor
            <span class="token keyword">return</span> item<span class="token comment">#记得及时返回pipeline</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token keyword">raise</span> DropItem<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Missing price in </span><span class="token interpolation"><span class="token punctuation">&#123;</span>item<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> pymongo
<span class="token keyword">from</span> itemadapter <span class="token keyword">import</span> ItemAdapter

<span class="token keyword">class</span> <span class="token class-name">MongoPipeline</span><span class="token punctuation">:</span>

    collection_name <span class="token operator">=</span> <span class="token string">'scrapy_items'</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> mongo_uri<span class="token punctuation">,</span> mongo_db<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>mongo_uri <span class="token operator">=</span> mongo_uri
        self<span class="token punctuation">.</span>mongo_db <span class="token operator">=</span> mongo_db

    <span class="token decorator annotation punctuation">@classmethod</span>
    <span class="token keyword">def</span> <span class="token function">from_crawler</span><span class="token punctuation">(</span>cls<span class="token punctuation">,</span> crawler<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> cls<span class="token punctuation">(</span>
            mongo_uri<span class="token operator">=</span>crawler<span class="token punctuation">.</span>settings<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">'MONGO_URI'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            mongo_db<span class="token operator">=</span>crawler<span class="token punctuation">.</span>settings<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">'MONGO_DATABASE'</span><span class="token punctuation">,</span> <span class="token string">'items'</span><span class="token punctuation">)</span>
        <span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">open_spider</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>client <span class="token operator">=</span> pymongo<span class="token punctuation">.</span>MongoClient<span class="token punctuation">(</span>self<span class="token punctuation">.</span>mongo_uri<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>db <span class="token operator">=</span> self<span class="token punctuation">.</span>client<span class="token punctuation">[</span>self<span class="token punctuation">.</span>mongo_db<span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">close_spider</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>client<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">process_item</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> item<span class="token punctuation">,</span> spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>db<span class="token punctuation">[</span>self<span class="token punctuation">.</span>collection_name<span class="token punctuation">]</span><span class="token punctuation">.</span>insert_one<span class="token punctuation">(</span>ItemAdapter<span class="token punctuation">(</span>item<span class="token punctuation">)</span><span class="token punctuation">.</span>asdict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> item<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>写入json文件：<br><img src="../md_imgs/Scrapy框架/2.png" alt="image.png"></p>
<h2 id="图片爬取"><a href="#图片爬取" class="headerlink" title="图片爬取"></a>图片爬取</h2><p>如果需要下载页面的内的图片，pipelines提供了一种专门的类<code>Imagepipeline</code>来进行处理，具体处理操作可以查看对应的源代码<br><img src="../md_imgs/Scrapy框架/3.png" alt="image.png">（阿里云盘）</p>
<h2 id="调用"><a href="#调用" class="headerlink" title="调用"></a>调用</h2><p>设置文件中默认是不使用pipeline文件的，我们需要将settings文件中对应位置取消注释，将自己设定的类添加到设置文件（settings.py）中，然后设定一个优先级（范围是0~1000，数字越小，优先级越高）<br><pre class="line-numbers language-python" data-language="python"><code class="language-python">ITEM_PIPELINES <span class="token operator">=</span> <span class="token punctuation">&#123;</span>
    <span class="token string">'myproject.pipelines.PricePipeline'</span><span class="token punctuation">:</span> <span class="token number">300</span><span class="token punctuation">,</span>
    <span class="token string">'myproject.pipelines.JsonWriterPipeline'</span><span class="token punctuation">:</span> <span class="token number">800</span><span class="token punctuation">,</span>
<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><br>将提取的数据传给pipeline处理有两种方法，一种是使用<code>yield</code>来返回，第二种是收集所有的数据，用return items 来返回。</p>
<h1 id="Downloader-Middleware（全局改写requests-response）"><a href="#Downloader-Middleware（全局改写requests-response）" class="headerlink" title="Downloader Middleware（全局改写requests/response）"></a>Downloader Middleware（全局改写requests/response）</h1><p>对请求和返回进行修改，还可以处理异常情况（对response进行处理）.</p>
<h1 id="Scrapy日志管理"><a href="#Scrapy日志管理" class="headerlink" title="Scrapy日志管理"></a>Scrapy日志管理</h1><h2 id="终端输出命令的选择"><a href="#终端输出命令的选择" class="headerlink" title="终端输出命令的选择"></a>终端输出命令的选择</h2><p>Scrapy 用的是标准日志等级制度，如下所示（级别越来越低）：</p>
<ul>
<li>CRITICAL（关键）</li>
<li>ERROR（错误）</li>
<li>WARNING（警告）</li>
<li>DEBUG（调试）</li>
<li>INFO（信息）</li>
</ul>
<p>要调整显示层级，只需在setting文件输入：<br><code>LOG_LEVEL = &#39;ERROR&#39;</code><br>这样只会有CRITICAL和ERROR显示出来</p>
<h2 id="输出单独的日志文件"><a href="#输出单独的日志文件" class="headerlink" title="输出单独的日志文件"></a>输出单独的日志文件</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python">scrapy crawl articles <span class="token operator">-</span>s LOG_FILE<span class="token operator">=</span>wiki<span class="token punctuation">.</span>log<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://kebuaaa.github.io">可不</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://kebuaaa.github.io/Scrapy%E6%A1%86%E6%9E%B6/">https://kebuaaa.github.io/Scrapy%E6%A1%86%E6%9E%B6/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kebuaaa.github.io" target="_blank">小明的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="../tags/Python%E7%88%AC%E8%99%AB/">Python爬虫</a></div><div class="post_share"><div class="social-share" data-image="https://images.pexels.com/photos/733854/pexels-photo-733854.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=600" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="../Seaborn%E5%BA%93%E7%AE%80%E4%BB%8B/"><img class="prev-cover" src="https://images.pexels.com/photos/733854/pexels-photo-733854.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=600" onerror="onerror=null;src='https://images.pexels.com/photos/374918/pexels-photo-374918.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=600'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Seaborn库简介</div></div></a></div><div class="next-post pull-right"><a href="../SQL/"><img class="next-cover" src="https://images.pexels.com/photos/159866/books-book-pages-read-literature-159866.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=600" onerror="onerror=null;src='https://images.pexels.com/photos/374918/pexels-photo-374918.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=600'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">SQL</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="../Working%20with%20APIs/" title="Working with APIs"><img class="cover" src="https://images.pexels.com/photos/733854/pexels-photo-733854.jpeg?auto=compress&cs=tinysrgb&w=600" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-26</div><div class="title">Working with APIs</div></div></a></div><div><a href="../python%E7%88%AC%E8%99%AB/" title="python爬虫"><img class="cover" src="https://images.pexels.com/photos/159866/books-book-pages-read-literature-159866.jpeg?auto=compress&cs=tinysrgb&w=600" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-26</div><div class="title">python爬虫</div></div></a></div><div><a href="../urllib/" title="urllib"><img class="cover" src="https://images.pexels.com/photos/768125/pexels-photo-768125.jpeg?auto=compress&cs=tinysrgb&w=600" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-26</div><div class="title">urllib</div></div></a></div><div><a href="../%E6%88%BF%E5%A4%A9%E4%B8%8B%E4%BA%8C%E6%89%8B%E6%88%BF%E4%BA%A4%E6%98%93%E6%95%B0%E6%8D%AE%E7%88%AC%E5%8F%96/" title="房天下二手房交易数据爬取"><img class="cover" src="https://images.pexels.com/photos/317355/pexels-photo-317355.jpeg?auto=compress&cs=tinysrgb&w=600" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-07-05</div><div class="title">房天下二手房交易数据爬取</div></div></a></div><div><a href="../%E7%88%AC%E5%8F%96%E7%94%B5%E5%BD%B1%E9%99%A2%E6%94%BE%E6%98%A0%E7%94%B5%E5%BD%B1%EF%BC%88re%EF%BC%89/" title="爬取电影院放映电影（re）"><img class="cover" src="https://images.pexels.com/photos/733854/pexels-photo-733854.jpeg?auto=compress&cs=tinysrgb&w=600" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-26</div><div class="title">爬取电影院放映电影（re）</div></div></a></div><div><a href="../%E7%BA%BF%E7%A8%8B&%E8%BF%9B%E7%A8%8B/" title="线程&amp;进程"><img class="cover" src="https://images.pexels.com/photos/768125/pexels-photo-768125.jpeg?auto=compress&cs=tinysrgb&w=600" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-07-08</div><div class="title">线程&amp;进程</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%80%89%E6%8B%A9%E5%99%A8%EF%BC%88%E6%8F%90%E5%8F%96%E6%95%B0%E6%8D%AE%E7%9A%84%E6%9C%BA%E5%88%B6%EF%BC%89"><span class="toc-number">1.</span> <span class="toc-text">选择器（提取数据的机制）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#XPath%E9%80%89%E6%8B%A9%E5%99%A8"><span class="toc-number">1.1.</span> <span class="toc-text">XPath选择器</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Response%EF%BC%88Using-selectors%EF%BC%89"><span class="toc-number">2.</span> <span class="toc-text">Response（Using selectors）</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Spider"><span class="toc-number">3.</span> <span class="toc-text">Spider</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8F%90%E5%8F%96%E7%88%AC%E5%8F%96%E7%BB%93%E6%9E%9C"><span class="toc-number">3.1.</span> <span class="toc-text">提取爬取结果</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BF%9D%E5%AD%98%E7%88%AC%E5%8F%96%E7%BB%93%E6%9E%9C"><span class="toc-number">3.2.</span> <span class="toc-text">保存爬取结果</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A2%9E%E5%8A%A0%E5%8F%82%E6%95%B0"><span class="toc-number">3.3.</span> <span class="toc-text">增加参数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B7%9F%E8%B8%AA%E9%93%BE%E6%8E%A5%EF%BC%88%E5%A4%9A%E4%B8%AA%E7%BD%91%E9%A1%B5%E7%9A%84%E8%B7%B3%E8%BD%AC%E6%8A%93%E5%8F%96%EF%BC%89"><span class="toc-number">3.4.</span> <span class="toc-text">跟踪链接（多个网页的跳转抓取）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#example"><span class="toc-number">3.4.1.</span> <span class="toc-text">example</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8A%A8%E6%80%81%E7%BD%91%E9%A1%B5"><span class="toc-number">3.5.</span> <span class="toc-text">动态网页</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E6%9D%A5%E8%87%AA%E5%A4%9A%E4%B8%AA%E9%A1%B5%E9%9D%A2%E6%95%B0%E6%8D%AE%E7%BB%84%E6%88%90%E7%9A%84item"><span class="toc-number">3.6.</span> <span class="toc-text">生成来自多个页面数据组成的item</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%80%9A%E7%94%A8%E7%88%AC%E8%99%AB"><span class="toc-number">4.</span> <span class="toc-text">通用爬虫</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#class-scrapy-spiders-CrawlSpider"><span class="toc-number">4.1.</span> <span class="toc-text">class scrapy.spiders.CrawlSpider</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#item-pipelines"><span class="toc-number">5.</span> <span class="toc-text">item pipelines</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Some-examples"><span class="toc-number">5.1.</span> <span class="toc-text">Some examples</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%BE%E7%89%87%E7%88%AC%E5%8F%96"><span class="toc-number">5.2.</span> <span class="toc-text">图片爬取</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B0%83%E7%94%A8"><span class="toc-number">5.3.</span> <span class="toc-text">调用</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Downloader-Middleware%EF%BC%88%E5%85%A8%E5%B1%80%E6%94%B9%E5%86%99requests-response%EF%BC%89"><span class="toc-number">6.</span> <span class="toc-text">Downloader Middleware（全局改写requests&#x2F;response）</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Scrapy%E6%97%A5%E5%BF%97%E7%AE%A1%E7%90%86"><span class="toc-number">7.</span> <span class="toc-text">Scrapy日志管理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%88%E7%AB%AF%E8%BE%93%E5%87%BA%E5%91%BD%E4%BB%A4%E7%9A%84%E9%80%89%E6%8B%A9"><span class="toc-number">7.1.</span> <span class="toc-text">终端输出命令的选择</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BE%93%E5%87%BA%E5%8D%95%E7%8B%AC%E7%9A%84%E6%97%A5%E5%BF%97%E6%96%87%E4%BB%B6"><span class="toc-number">7.2.</span> <span class="toc-text">输出单独的日志文件</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url('https://images.pexels.com/photos/733854/pexels-photo-733854.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=600')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By 可不</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"></div><div id="rightside-config-show"><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="../js/utils.js"></script><script src="../js/main.js"></script><script src="../js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: '',
      appKey: '',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !true) {
  if (true) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>