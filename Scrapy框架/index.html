<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>Scrapy框架 | 小明的博客</title><meta name="author" content="爱编程的小明"><meta name="copyright" content="爱编程的小明"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#e68ab8"><meta name="description" content="..."><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://kebuaaa.github.io/Scrapy%E6%A1%86%E6%9E%B6/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//www.clarity.ms"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>(()=>{const e={set:(e,t,o)=>{if(!o)return;const a=Date.now()+864e5*o;localStorage.setItem(e,JSON.stringify({value:t,expiry:a}))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const{value:o,expiry:a}=JSON.parse(t);if(!(Date.now()>a))return o;localStorage.removeItem(e)}};window.btf={saveToLocal:e,getScript:(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,Object.entries(t).forEach((([e,t])=>n.setAttribute(e,t))),n.onload=n.onreadystatechange=()=>{n.readyState&&!/loaded|complete/.test(n.readyState)||o()},n.onerror=a,document.head.appendChild(n)})),getCSS:(e,t)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onload=n.onreadystatechange=()=>{n.readyState&&!/loaded|complete/.test(n.readyState)||o()},n.onerror=a,document.head.appendChild(n)})),addGlobalFn:(e,t,o=!1,a=window)=>{const n=a.globalFn||{};n[e]=n[e]||{},n[e][o||Object.keys(n[e]).length]=t,a.globalFn=n}};const t=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},o=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#e68ab8")};btf.activateDarkMode=t,btf.activateLightMode=o;const a=e.get("theme");"dark"===a?t():"light"===a&&o();const n=e.get("aside-status");void 0!==n&&document.documentElement.classList.toggle("hide-aside","hide"===n);/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})()</script><script>var _hmt=_hmt||[];!function(){var t=document.createElement("script");t.src="https://hm.baidu.com/hm.js?4e3a07c287f8fb6cfc09bf5a7fdc1dd7";var a=document.getElementsByTagName("script")[0];a.parentNode.insertBefore(t,a)}(),btf.addGlobalFn("pjaxComplete",(()=>{_hmt.push(["_trackPageview",window.location.pathname])}),"baidu_analytics")</script><script>!function(e,t,n,c,a,i,r){e[n]=e[n]||function(){(e[n].q=e[n].q||[]).push(arguments)},(i=t.createElement(c)).async=1,i.src="https://www.clarity.ms/tag/e8bjif1knd",(r=t.getElementsByTagName(c)[0]).parentNode.insertBefore(i,r)}(window,document,"clarity","script")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:void 0,translate:{defaultEncoding:1,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"簡"},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:!1,highlightFullpage:!1,highlightMacStyle:!1},copy:{success:"复制成功",error:"复制失败",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!1},runtime:"",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"null",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!1,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!1},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"Scrapy框架",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,isShuoshuo:!1}</script><style>#article-container.post-content h1:before,h2:before,h3:before,h4:before,h5:before,h6:before{-webkit-animation:avatar_turn_around 1s linear infinite;-moz-animation:avatar_turn_around 1s linear infinite;-o-animation:avatar_turn_around 1s linear infinite;-ms-animation:avatar_turn_around 1s linear infinite;animation:avatar_turn_around 1s linear infinite}</style><link rel="stylesheet" href="/Scripts/css/transparent.css"><link rel="stylesheet" href="/Scripts/css/font.css"><link rel="stylesheet" href="/Scripts/css/foot_style.css"><link rel="stylesheet" href="/Scripts/css/twikoo_beautify.css"><link rel="stylesheet" href="/Scripts/css/tags.css"><link rel="stylesheet" href="/Scripts/css/cardlistpost.min.css"><style>#recent-posts>.recent-post-item>.recent-post-info>.article-meta-wrap>.tags:before{content:"\A";white-space:pre}#recent-posts>.recent-post-item>.recent-post-info>.article-meta-wrap>.tags>.article-meta__separator{display:none}</style><link rel="stylesheet" href="https://unpkg.zhimg.com/hexo-butterfly-wowjs/lib/animate.min.css" media="print" onload='this.media="screen"'><meta name="generator" content="Hexo 7.3.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet"></head><body><div id="web_bg" style="background-image:url(url(/img/index_img.webp))"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="/img/avatar.webp" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">100</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">57</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">18</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>存档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i> <span>关于</span></a></div><div class="menus_item"><a class="site-page" href="/wallpaper/"><i class="fa-fw fas fa-image fa-fw"></i> <span>壁纸</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://www.travellings.cn/go.html"><i class="fa-fw fa fa-subway"></i> <span>开往</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">小明的博客</span></a><a class="nav-page-title" href="/"><span class="site-name">Scrapy框架</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>存档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i> <span>关于</span></a></div><div class="menus_item"><a class="site-page" href="/wallpaper/"><i class="fa-fw fas fa-image fa-fw"></i> <span>壁纸</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://www.travellings.cn/go.html"><i class="fa-fw fa fa-subway"></i> <span>开往</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">Scrapy框架<a class="post-edit-link" href="https://github.dev/kebuAAA/myblog/blob/main/source/_posts/Scrapy框架.md" title="编辑" target="_blank"><i class="fas fa-pencil-alt"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-03-01T02:34:00.000Z" title="发表于 2022-03-01 10:34:00">2022-03-01</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-03-05T02:34:00.000Z" title="更新于 2022-03-05 10:34:00">2022-03-05</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%88%AC%E8%99%AB/%E7%90%86%E8%AE%BA/">理论</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="container post-content" id="article-container"><blockquote><p>Scrapy确实是提高爬虫效率很好的一个方法，但框架式的内容也对自身对爬虫技能的掌握程度提出了一个全新的要求，目前自身的爬虫技能仍有待进一步加强，相信以后会越做越好。</p></blockquote><p><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Scrapy%E6%A1%86%E6%9E%B6/0.jpg" alt="image.png"></p><p>简单网页的爬取可以利用re模块，复杂网页的爬取对于内容的提取则会显得十分麻烦。Scrapy框架是python下的一个爬虫框架，因为它足够简单方便受到人们的青睐。</p><h1 id="选择器-提取数据的机制">选择器（提取数据的机制）</h1><p>Scrapy提取数据有自己的一套机制。 它们被称作选择器（seletors)，通过特定的XPath或者CSS表达式来“选择”HTML文件中的某个部分。XPath是一门用来在XML文件中选择节点的语言， 也可以用在HTML上。 CSS是一门将HTML文档样式化的语言。 选择器由它定义，并与特定的HTML元素的样式相关联。<br>Scrapy的选择器构建于lxml库之上， 这意味着它们在速度和解析准确性上非常相似， 所以看你喜欢哪种选择器就使用哪种吧， 它们从效率上看完全没有区别。</p><h2 id="xpath选择器">XPath选择器</h2><p>XPath是一门在XML文档中查找信息的语言。 如果实在不想自己写的话可以借助edge浏览器的插件SelectorGadget 给自动生成一下<br>在XPath中， 有7种类型的节点： 元素、 属性、 文本、 命名空间、 处理指令、 注释以及文档节点（或称为根节点） 。 XML文档是被作为节点树来对待的。 树的根被称为文档节点或者根节点。<br>下面以一个简单的xml文件进行说明</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">superhero</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">class</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span> <span class="attr">lang</span>=<span class="string">&quot;en&quot;</span>&gt;</span>Tony Stark <span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">alias</span>&gt;</span>Iron Man <span class="tag">&lt;/<span class="name">alias</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">sex</span>&gt;</span>male <span class="tag">&lt;/<span class="name">sex</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">birthday</span>&gt;</span>1969 <span class="tag">&lt;/<span class="name">birthday</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">age</span>&gt;</span>47 <span class="tag">&lt;/<span class="name">age</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">class</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">class</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span> <span class="attr">lang</span>=<span class="string">&quot;en&quot;</span>&gt;</span>Peter Benjamin Parker <span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">alias</span>&gt;</span>Spider Man <span class="tag">&lt;/<span class="name">alias</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">sex</span>&gt;</span>male <span class="tag">&lt;/<span class="name">sex</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">birthday</span>&gt;</span>unknow <span class="tag">&lt;/<span class="name">birthday</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">age</span>&gt;</span>unknown <span class="tag">&lt;/<span class="name">age</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">class</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">class</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span> <span class="attr">lang</span>=<span class="string">&quot;en&quot;</span>&gt;</span>Steven Rogers <span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">alias</span>&gt;</span>Captain America <span class="tag">&lt;/<span class="name">alias</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">sex</span>&gt;</span>male <span class="tag">&lt;/<span class="name">sex</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">birthday</span>&gt;</span>19200704 <span class="tag">&lt;/<span class="name">birthday</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">age</span>&gt;</span>96 <span class="tag">&lt;/<span class="name">age</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">class</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">superhero</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>在这个例子中<superhero>是文档节点，<name lang="en">Tony Stark</name>是元素节点，lang=&quot;en&quot;是属性。Xpath通过在文档中选取节点来进行数据匹配：</superhero></p><table><thead><tr><th>nodeName</th><th>提取节点的所有子节点</th></tr></thead><tbody><tr><td>/</td><td>从根节点选取</td></tr><tr><td>//+节点名称</td><td>从匹配选择的当前节点选择文档中的节点，不考虑他们的位置</td></tr><tr><td>.</td><td>选取当前节点</td></tr><tr><td>…</td><td>选取当前节点的父节点</td></tr><tr><td>@+属性名称</td><td>选择属性</td></tr><tr><td>*</td><td>匹配任何元素节点</td></tr><tr><td>@*</td><td>匹配任何属性节点</td></tr><tr><td>Node()</td><td>匹配任何类型的节点</td></tr><tr><td>/text（）</td><td>节点的文本内容提取</td></tr><tr><td>@href</td><td>节点href属性的值</td></tr></tbody></table><p>实际运用：</p><ul><li>“//div[@id=“images”]/a/text()”，节点名称为div属性为images的a节点的文本内容</li><li></li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> scrapy.selector <span class="keyword">import</span> Selector <span class="keyword">as</span> se</span><br><span class="line">mypath=os.getcwd()+<span class="string">&quot;/爬虫/code/crawler_script/superHero.xml&quot;</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(mypath) <span class="keyword">as</span> fp:</span><br><span class="line">    body=fp.read()</span><br><span class="line"><span class="comment">#print(body)</span></span><br><span class="line"><span class="comment">#print(se(text=body).xpath(&#x27;/*&#x27;).extract())</span></span><br><span class="line"><span class="comment">#采集第一个class</span></span><br><span class="line"><span class="built_in">print</span>(se(text=body).xpath(<span class="string">&#x27;/html/body/superhero/class[1]&#x27;</span>).extract())</span><br><span class="line"><span class="comment">#采集name属性为en的数据</span></span><br><span class="line"><span class="built_in">print</span>(se(text=body).xpath(<span class="string">&#x27;//name[@lang=&quot;en&quot;]&#x27;</span>).extract())</span><br></pre></td></tr></table></figure><h1 id="response-using-selectors">Response（Using selectors）</h1><p>定义在Spider.py中的parse（）方法是<code>[TextResponse](https://docs.scrapy.org/en/latest/topics/request-response.html#scrapy.http.TextResponse)</code>类的一个实例，用来处理每次发起的网页请求传回来的响应文件，可以在这里定义对响应文件的提取规则等内容（请求的回调方法）。其输入的参数response其实就是网页请求的响应文件，本身可以作为选择器使用。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">response.selector(<span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>其中selector表示具体的选择器，如xpath，css，re等<br>需要注意的是，使用response.xpath()方法的返回值仍然是一个选择器，也就是说可以继续对提取结果进行进一步的筛选，比如可以对筛选出来的文本继续用re模块进行匹配：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">response.xpath().re()</span><br><span class="line">sel.xpath(<span class="string">&quot;string(//a[1])&quot;</span>).getall() <span class="comment"># convert it to string</span></span><br></pre></td></tr></table></figure><p>欲将提取结果进行显示，可以借助<code>extract()</code>或者<code>get()</code>函数，默认情况下对于没有数据可以被提取出来时输出None，可以通过给default参数赋其他值来调节：</p><ul><li><code>get()</code>返回一条结果</li><li><code>getall()</code>：返回所有结果</li><li><code>extract()</code>:返回所有结果</li><li><code>extract_first</code>：返回第一个结果</li></ul><p>调用<code>getall</code>返回的是一个列表，当爬取的数据不存在时，对列表的索引会导致程序出现<code>IndexError</code>停止，言外之意是不要随意对返回列表进行索引：<br><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Scrapy%E6%A1%86%E6%9E%B6/1.png" alt="image.png"><br>这种情况可以考虑用<code>get（）</code>代替，在有数据时会返回一样的结果，没有的话也只是会返回<code>None</code></p><h1 id="spider">Spider</h1><p>Scrapy中有一个Spider类，该类并没有提供什么特殊的功能。首先通过初始化的request(<code>start_requests()</code>)去爬取指定的初始链接(<code>start_urls</code>），然后制定一个回调函数（callback ）来处理从网页请求中下载的回应（response）。 在制作自己需要的爬虫规则时，必须先继承Spider类。<br>类的属性：</p><ul><li>name：自己定义的spider的名字</li><li>allowed_domains：包含了spider允许爬取的域名(domain)列表(list)</li><li>start_urls：URL列表。当没有制定特定的URL时，spider将从该列表中开始进行爬取。 因此，第一个被获取到的页面的URL将是该列表之一。 后续的URL将会从获取到的数据中提取。</li><li>custom_settings:对项目的设置文件进行重写，它必须定义为类属性，因为设置在实例化之前更新。</li><li></li></ul><h2 id="提取爬取结果">提取爬取结果</h2><p>当我们对爬虫的结果进行返回时，默认返回一个字典形式的数据。为了让Scrapy也实现这样的效果，我们可以借助<code>yield</code>来实现：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">QuotesSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&quot;quotes&quot;</span></span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">&#x27;https://quotes.toscrape.com/page/1/&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;https://quotes.toscrape.com/page/2/&#x27;</span>,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="keyword">for</span> quote <span class="keyword">in</span> response.css(<span class="string">&#x27;div.quote&#x27;</span>):</span><br><span class="line">            <span class="keyword">yield</span> &#123;</span><br><span class="line">                <span class="string">&#x27;text&#x27;</span>: quote.css(<span class="string">&#x27;span.text::text&#x27;</span>).get(),</span><br><span class="line">                <span class="string">&#x27;author&#x27;</span>: quote.css(<span class="string">&#x27;small.author::text&#x27;</span>).get(),</span><br><span class="line">                <span class="string">&#x27;tags&#x27;</span>: quote.css(<span class="string">&#x27;div.tags a.tag::text&#x27;</span>).getall(),</span><br><span class="line">            &#125;</span><br></pre></td></tr></table></figure><p>爬取正常时显示的结果(日志中)：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="number">2016</span>-09-<span class="number">19</span> <span class="number">18</span>:<span class="number">57</span>:<span class="number">19</span> [scrapy.core.scraper] DEBUG: Scraped <span class="keyword">from</span> &lt;<span class="number">200</span> https://quotes.toscrape.com/page/<span class="number">1</span>/&gt;</span><br><span class="line">&#123;<span class="string">&#x27;tags&#x27;</span>: [<span class="string">&#x27;life&#x27;</span>, <span class="string">&#x27;love&#x27;</span>], <span class="string">&#x27;author&#x27;</span>: <span class="string">&#x27;André Gide&#x27;</span>, <span class="string">&#x27;text&#x27;</span>: <span class="string">&#x27;“It is better to be hated for what you are than to be loved for what you are not.”&#x27;</span>&#125;</span><br><span class="line"><span class="number">2016</span>-09-<span class="number">19</span> <span class="number">18</span>:<span class="number">57</span>:<span class="number">19</span> [scrapy.core.scraper] DEBUG: Scraped <span class="keyword">from</span> &lt;<span class="number">200</span> https://quotes.toscrape.com/page/<span class="number">1</span>/&gt;</span><br><span class="line">&#123;<span class="string">&#x27;tags&#x27;</span>: [<span class="string">&#x27;edison&#x27;</span>, <span class="string">&#x27;failure&#x27;</span>, <span class="string">&#x27;inspirational&#x27;</span>, <span class="string">&#x27;paraphrased&#x27;</span>], <span class="string">&#x27;author&#x27;</span>: <span class="string">&#x27;Thomas A. Edison&#x27;</span>, <span class="string">&#x27;text&#x27;</span>: <span class="string">&quot;“I have not failed. I&#x27;ve just found 10,000 ways that won&#x27;t work.”&quot;</span>&#125;</span><br></pre></td></tr></table></figure><h2 id="保存爬取结果">保存爬取结果</h2><p>最简单的导出爬取结果的方法为:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">scrapy crawl quotes -O quotes.json</span><br></pre></td></tr></table></figure><p>&quot;quotes.json&quot;限定了保存文件的格式与名称。也可以导出为csv格式或者JSON Lines格式（jl）<br>csv文件存储的一个好处是能把一个节点所有的文字变成一句话，如果是json格式，保存的会是一个字符串列表。<br>如果想要保存在数据库等操作，需要借助pipelines文件</p><h2 id="增加参数">增加参数</h2><p>可以在命令进行操作给Spider类添加任何需要的参数：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">scrapy crawl myspider -a category=electronics</span><br></pre></td></tr></table></figure><p>意思即为添加一个值为electronics的属性category</p><h2 id="跟踪链接-多个网页的跳转抓取">跟踪链接（多个网页的跳转抓取）</h2><p>对于有多个相关联的网页内容的抓取，我们可以通过定义parse方法的内容实现。首先利用匹配原则提取出网页跳转的链接，然后再借助response的urljoin方法将待抓取的链接构建一个完整的链接，最后再调用yield来发出一个请求，然后Scrapy会安排送入的网页（next_page）进行访问请求，并在请求结束后利用定义的回调方法（self.parse）执行回调。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">QuotesSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&quot;quotes&quot;</span></span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">&#x27;https://quotes.toscrape.com/page/1/&#x27;</span>,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="keyword">for</span> quote <span class="keyword">in</span> response.css(<span class="string">&#x27;div.quote&#x27;</span>):</span><br><span class="line">            <span class="keyword">yield</span> &#123;</span><br><span class="line">                <span class="string">&#x27;text&#x27;</span>: quote.css(<span class="string">&#x27;span.text::text&#x27;</span>).get(),</span><br><span class="line">                <span class="string">&#x27;author&#x27;</span>: quote.css(<span class="string">&#x27;small.author::text&#x27;</span>).get(),</span><br><span class="line">                <span class="string">&#x27;tags&#x27;</span>: quote.css(<span class="string">&#x27;div.tags a.tag::text&#x27;</span>).getall(),</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        next_page = response.css(<span class="string">&#x27;li.next a::attr(href)&#x27;</span>).get()</span><br><span class="line">        <span class="keyword">if</span> next_page <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            next_page = response.urljoin(next_page)</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(next_page, callback=self.parse)</span><br><span class="line">        <span class="comment">#一个更加方便的方法</span></span><br><span class="line">        <span class="comment">#if next_page is not None:</span></span><br><span class="line">            <span class="comment">#yield response.follow(next_page, callback=self.parse)</span></span><br><span class="line">        <span class="comment">#follow只返回了网页请求，仍需要进行回调</span></span><br></pre></td></tr></table></figure><p>与urljoin+Request的方法相比,response.follow提供了一种更加便捷的方法。该方法可以自动对selector类型进行处理（自动提取出节点中的链接）：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">next_page = response.css(<span class="string">&#x27;li.next a::attr(href)&#x27;</span>).get()</span><br><span class="line">        <span class="keyword">if</span> next_page <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">yield</span> response.follow(next_page, callback=self.parse)</span><br></pre></td></tr></table></figure><p>另外如果当所有的网页链接可以从一个迭代对象中爬取时，response.follow_all()方法提供了更为便捷的方法。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">anchors = response.css(<span class="string">&#x27;ul.pager a&#x27;</span>)</span><br><span class="line"><span class="keyword">yield</span> <span class="keyword">from</span> response.follow_all(anchors, callback=self.parse)</span><br></pre></td></tr></table></figure><h3 id="example">example</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AuthorSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;author&#x27;</span></span><br><span class="line"></span><br><span class="line">    start_urls = [<span class="string">&#x27;https://quotes.toscrape.com/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        author_page_links = response.css(<span class="string">&#x27;.author + a&#x27;</span>)</span><br><span class="line">        <span class="keyword">yield</span> <span class="keyword">from</span> response.follow_all(author_page_links, self.parse_author)</span><br><span class="line"></span><br><span class="line">        pagination_links = response.css(<span class="string">&#x27;li.next a&#x27;</span>)</span><br><span class="line">        <span class="keyword">yield</span> <span class="keyword">from</span> response.follow_all(pagination_links, self.parse)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_author</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">extract_with_css</span>(<span class="params">query</span>):</span><br><span class="line">            <span class="keyword">return</span> response.css(query).get(default=<span class="string">&#x27;&#x27;</span>).strip()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">yield</span> &#123;</span><br><span class="line">            <span class="string">&#x27;name&#x27;</span>: extract_with_css(<span class="string">&#x27;h3.author-title::text&#x27;</span>),</span><br><span class="line">            <span class="string">&#x27;birthdate&#x27;</span>: extract_with_css(<span class="string">&#x27;.author-born-date::text&#x27;</span>),</span><br><span class="line">            <span class="string">&#x27;bio&#x27;</span>: extract_with_css(<span class="string">&#x27;.author-description::text&#x27;</span>),</span><br><span class="line">        &#125;<span class="comment">#最好的书写是将在items文件中声明好格式，不建议这样写</span></span><br></pre></td></tr></table></figure><p>默认情况下，Scrapy 会过滤掉对已经访问过的 URL 的重复请求，避免由于编程错误而过多地访问服务器的问题。这可以通过设置 <code>DUPEFILTER_CLASS</code> 进行配置。<br>这是一个避免从多个页面</p><h2 id="动态网页">动态网页</h2><p>动态网页的爬取意味着我们可能需要对headers和cookies进行调整。具体参考：<br><a target="_blank" rel="noopener" href="https://www.yuque.com/u22723808/python/wksuec?view=doc_embed&amp;inner=fppJl">网页抓取教程</a></p><h2 id="生成来自多个页面数据组成的item">生成来自多个页面数据组成的item</h2><p>using a <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/request-response.html#topics-request-response-ref-request-callback-arguments">trick to pass additional data to the callbacks</a>.</p><h1 id="通用爬虫">通用爬虫</h1><p>Scrapy除了提供Spider类之外，还提供了其他的类来简化我们的工作（对一些稍微有针对性一点的功能进行了封装）</p><h2 id="class-scrapy-spiders-crawlspider">class scrapy.spiders.CrawlSpider</h2><p>创建：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Scrapy genspider -t crawl [爬虫名字] [域名]</span><br></pre></td></tr></table></figure><p>CrawSpider类的爬虫被广泛应用于爬取各类常规网站。它通过定义一组规则为跟踪链接提供了更加便捷的方法。与Spider类相比，该类新增加了两个属性：</p><ul><li>rules：包含一系列<code>Rule</code>类，每一个<code>Rule</code>类定义了爬取网站的原则（是否跟踪，是否对输入的链接进行爬取）</li><li><strong>parse_start_url(</strong><em><strong>response</strong></em><strong>, <strong><em>**<strong>kwargs</strong></em></strong>)：可以进行重写的方法</strong></li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CrawlSpider, Rule</span><br><span class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MySpider</span>(<span class="title class_ inherited__">CrawlSpider</span>):</span><br><span class="line">    name = <span class="string">&#x27;example.com&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;example.com&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;http://www.example.com&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    rules = (</span><br><span class="line">        <span class="comment"># Extract links matching &#x27;category.php&#x27; (but not matching &#x27;subsection.php&#x27;)</span></span><br><span class="line">        <span class="comment"># and follow links from them (since no callback means follow=True by default).</span></span><br><span class="line">        Rule(LinkExtractor(allow=(<span class="string">&#x27;category\.php&#x27;</span>, ), deny=(<span class="string">&#x27;subsection\.php&#x27;</span>, ))),</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Extract links matching &#x27;item.php&#x27; and parse them with the spider&#x27;s method parse_item</span></span><br><span class="line">        Rule(LinkExtractor(allow=(<span class="string">&#x27;item\.php&#x27;</span>, )), callback=<span class="string">&#x27;parse_item&#x27;</span>),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_item</span>(<span class="params">self, response</span>):</span><br><span class="line">        self.logger.info(<span class="string">&#x27;Hi, this is an item page! %s&#x27;</span>, response.url)</span><br><span class="line">        item = scrapy.Item()</span><br><span class="line">        item[<span class="string">&#x27;id&#x27;</span>] = response.xpath(<span class="string">&#x27;//td[@id=&quot;item_id&quot;]/text()&#x27;</span>).re(<span class="string">r&#x27;ID: (\d+)&#x27;</span>)</span><br><span class="line">        item[<span class="string">&#x27;name&#x27;</span>] = response.xpath(<span class="string">&#x27;//td[@id=&quot;item_name&quot;]/text()&#x27;</span>).get()</span><br><span class="line">        item[<span class="string">&#x27;description&#x27;</span>] = response.xpath(<span class="string">&#x27;//td[@id=&quot;item_description&quot;]/text()&#x27;</span>).get()</span><br><span class="line">        item[<span class="string">&#x27;link_text&#x27;</span>] = response.meta[<span class="string">&#x27;link_text&#x27;</span>]</span><br><span class="line">        url = response.xpath(<span class="string">&#x27;//td[@id=&quot;additional_data&quot;]/@href&#x27;</span>).get()</span><br><span class="line">        <span class="keyword">return</span> response.follow(url, self.parse_additional_page, cb_kwargs=<span class="built_in">dict</span>(item=item))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_additional_page</span>(<span class="params">self, response, item</span>):</span><br><span class="line">        item[<span class="string">&#x27;additional_data&#x27;</span>] = response.xpath(<span class="string">&#x27;//p[@id=&quot;additional_data&quot;]/text()&#x27;</span>).get()</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure><p><code>Link_extractor</code>类主要帮助我们对我们需要的url进行一个筛选（通常利用正则表达式指定筛选原则）常用的两个属性为：</p><ul><li>allow：正则表达式，表示需要进行提取的url</li><li>deny：禁止的url</li><li>allow_domains:</li><li>deny_domains:</li></ul><p><code>Rule</code>类的主要属性有：</p><ul><li>link_extractor：<code>Link_extractor</code>的一个实例。对网页进行筛选</li><li>callback：用来规定使用的回调函数</li><li>follow：布尔值，用来规定是否跟踪网页</li><li>process_links:从link_extractor传递给这个函数，用来规定不需要爬取的链接</li></ul><h1 id="item-pipelines">item pipelines</h1><p>理论上来讲，对网页抓取的数据可以选择放在parse函数中继续处理，但这种方法会牺牲网页抓取的速度，因此我们通常选择用parse函数做一个网页数据抓取，网页数据的处理和写入则放在交给pipelines<br>该类主要给了四个方法的定义。</p><ul><li><code>process_item(self, item, spider)</code>item指返回的Item（类），spider指定义的spider</li><li><code>open_spider(self, spider)</code>通过该方法在爬虫开始时进行调整</li><li><code>close_spider(self, spider)</code>在爬虫结束时进行相关操作</li><li><code>from_crawler(cls, crawler)</code>：类方法，用来获取Scrapy的配置信息</li></ul><p>该函数会在网页数据抓取后自动进行，为了保证它的运行，一定要记得网页数据提取时要有返回值（yield或者return）。</p><h2 id="some-examples">Some examples</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> itemadapter <span class="keyword">import</span> ItemAdapter</span><br><span class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> DropItem</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PricePipeline</span>:</span><br><span class="line"></span><br><span class="line">    vat_factor = <span class="number">1.15</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        adapter = ItemAdapter(item)</span><br><span class="line">        <span class="keyword">if</span> adapter.get(<span class="string">&#x27;price&#x27;</span>):</span><br><span class="line">            <span class="keyword">if</span> adapter.get(<span class="string">&#x27;price_excludes_vat&#x27;</span>):</span><br><span class="line">                adapter[<span class="string">&#x27;price&#x27;</span>] = adapter[<span class="string">&#x27;price&#x27;</span>] * self.vat_factor</span><br><span class="line">            <span class="keyword">return</span> item<span class="comment">#记得及时返回pipeline</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> DropItem(<span class="string">f&quot;Missing price in <span class="subst">&#123;item&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"><span class="keyword">from</span> itemadapter <span class="keyword">import</span> ItemAdapter</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MongoPipeline</span>:</span><br><span class="line"></span><br><span class="line">    collection_name = <span class="string">&#x27;scrapy_items&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, mongo_uri, mongo_db</span>):</span><br><span class="line">        self.mongo_uri = mongo_uri</span><br><span class="line">        self.mongo_db = mongo_db</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">from_crawler</span>(<span class="params">cls, crawler</span>):</span><br><span class="line">        <span class="keyword">return</span> cls(</span><br><span class="line">            mongo_uri=crawler.settings.get(<span class="string">&#x27;MONGO_URI&#x27;</span>),</span><br><span class="line">            mongo_db=crawler.settings.get(<span class="string">&#x27;MONGO_DATABASE&#x27;</span>, <span class="string">&#x27;items&#x27;</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">open_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        self.client = pymongo.MongoClient(self.mongo_uri)</span><br><span class="line">        self.db = self.client[self.mongo_db]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">close_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        self.client.close()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        self.db[self.collection_name].insert_one(ItemAdapter(item).asdict())</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure><p>写入json文件：<br><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Scrapy%E6%A1%86%E6%9E%B6/2.png" alt="image.png"></p><h2 id="图片爬取">图片爬取</h2><p>如果需要下载页面的内的图片，pipelines提供了一种专门的类<code>Imagepipeline</code>来进行处理，具体处理操作可以查看对应的源代码<br><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/Scrapy%E6%A1%86%E6%9E%B6/3.png" alt="image.png">（阿里云盘）</p><h2 id="调用">调用</h2><p>设置文件中默认是不使用pipeline文件的，我们需要将settings文件中对应位置取消注释，将自己设定的类添加到设置文件（<a target="_blank" rel="noopener" href="http://settings.py">settings.py</a>）中，然后设定一个优先级（范围是0~1000，数字越小，优先级越高）</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="string">&#x27;myproject.pipelines.PricePipeline&#x27;</span>: <span class="number">300</span>,</span><br><span class="line">    <span class="string">&#x27;myproject.pipelines.JsonWriterPipeline&#x27;</span>: <span class="number">800</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>将提取的数据传给pipeline处理有两种方法，一种是使用<code>yield</code>来返回，第二种是收集所有的数据，用return items 来返回。</p><h1 id="downloader-middleware-全局改写requests-response">Downloader Middleware（全局改写requests/response）</h1><p>对请求和返回进行修改，还可以处理异常情况（对response进行处理）.</p><h1 id="scrapy日志管理">Scrapy日志管理</h1><h2 id="终端输出命令的选择">终端输出命令的选择</h2><p>Scrapy 用的是标准日志等级制度，如下所示（级别越来越低）：</p><ul><li>CRITICAL（关键）</li><li>ERROR（错误）</li><li>WARNING（警告）</li><li>DEBUG（调试）</li><li>INFO（信息）</li></ul><p>要调整显示层级，只需在setting文件输入：<br><code>LOG_LEVEL = 'ERROR'</code><br>这样只会有CRITICAL和ERROR显示出来</p><h2 id="输出单独的日志文件">输出单独的日志文件</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">scrapy crawl articles -s LOG_FILE=wiki.log</span><br></pre></td></tr></table></figure></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kebuaaa.github.io">爱编程的小明</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kebuaaa.github.io/Scrapy%E6%A1%86%E6%9E%B6/">https://kebuaaa.github.io/Scrapy%E6%A1%86%E6%9E%B6/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://kebuaaa.github.io" target="_blank">小明的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Python%E7%88%AC%E8%99%AB/">Python爬虫</a></div><div class="post-share"><div class="social-share" data-image="/top_img/10047.webp" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/sklearn%E7%94%9F%E6%88%90%E6%95%B0%E6%8D%AE%E9%9B%86/" title="sklearn生成数据集"><img class="cover" src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="/top_img/10034.webp" onerror='onerror=null,src="https://images.pexels.com/photos/374918/pexels-photo-374918.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=600"' alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">sklearn生成数据集</div></div><div class="info-2"><div class="info-item-1">生成数据集 为了方便用户学习机器学习和数据挖掘的方法，机器学习库scikit-learn的数据集模块sklearn.datasets提供了20个样本生成函数，为分类、聚类、回归、主成分分析等各种机器学习方法生成模拟的样本集。 分类、聚类问题样本生成器 make_blobs()方法 sklearn.datasets.make_blobs(n_samples=100,  n_features=2, centers=3, cluster_std=1.0,  center_box=(-10.0, 10.0), shuffle=True,  random_state=None) center_box表示中心由随机数产生时的随机数产生的上下界 random_state表示样本数据的随机数产生方法 sklearn.datasets.make_blobs()函数能够生成指定样本数量、特征数量、类别数量、类别中心、类别样本标准差的分类样本集。 from sklearn.datasets.samples_generator import make_blobsX, y =...</div></div></div></a><a class="pagination-related" href="/%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98%E4%B8%8E%E5%8A%A0%E8%BD%BD/" title="模型的保存与加载"><img class="cover" src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="/top_img/10007.webp" onerror='onerror=null,src="https://images.pexels.com/photos/374918/pexels-photo-374918.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=600"' alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">模型的保存与加载</div></div><div class="info-2"><div class="info-item-1">保存和加载模型 在新版的python中，可以借助joblib库实现对训练得到的模型进行保存和加载。 对模型的保存需要利用到该库里的dump函数，加载的话则借助load函数：</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/Working%20with%20APIs/" title="Working with APIs"><img class="cover" src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="/top_img/10035.webp" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-23</div><div class="info-item-2">Working with APIs</div></div><div class="info-2"><div class="info-item-1">An API is a collection of tools that allows different applications to interact. 与我们获取网页相似，我们对API发出请求数据的请求，然后服务器作出相应，返回我们请求的数据。这一过程在python中主要通过requests库实现 发起请求： get() post():post请求一般会包含数据，因为这个请求本身就是用来发送给服务器请求服务器创建一个object用的 post请求成功会返回一个201的状态码 For example, we use POST requests to send information (instead of retrieve it), and to create objects on the API’s server. With the GitHub API, we can use POST requests to create new repositories. Different API endpoints choose what types of...</div></div></div></a><a class="pagination-related" href="/html/" title="html"><img class="cover" src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/html_20230406215949.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2022-07-24</div><div class="info-item-2">html</div></div><div class="info-2"><div class="info-item-1">简介 HTML 是用来描述网页的一种语言。 HTML 指的是超文本标记语言 (Hyper Text Markup Language) HTML 不是一种编程语言，而是一种标记语言 (markup language) 标记语言是一套标记标签 (markup tag) HTML 使用标记标签来描述网页 HTML 标记标签通常被称为 HTML 标签 (HTML tag): HTML 标签是由尖括号包围的关键词，比如 &lt;html&gt; HTML 标签通常是成对出现的，比如 &lt;b&gt; 和 &lt;/b&gt; 标签对中的第一个标签是开始标签，第二个标签是结束标签 开始和结束标签也被称为开放标签和闭合标签 html文档包括html标签和纯文本，html文档也被称为网页。Web浏览器的作用是读取HTML文档，并以网页的形式显示出来。 常用的html标签 一个html文档大概会包括以下内容，复杂网页一般会包括更多不同的标签以及对标签进行属性的调整来得到更加丰富的页面。 &lt;html&gt; &lt;body&gt; &lt;h1&gt;My First...</div></div></div></a><a class="pagination-related" href="/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/" title="爬虫入门"><img class="cover" src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="/top_img/10046.webp" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2022-02-27</div><div class="info-item-2">爬虫入门</div></div><div class="info-2"><div class="info-item-1">python爬虫书目推荐.xmind 基本内容 爬虫通俗来说就是一个模拟人类请求网站行为的程序。可以自动请求网页、并把数据抓取下来，然后使用一定的规则则提取有价值的数据。也可以理解为使用某种编程语言（这里当然是使用Python语言） 按照一定的顺序、 规则主动抓取互联网特定信息的程序或者脚本。 爬虫可以分为通用爬虫和聚焦爬虫 各大搜索引擎是通用爬虫一个很好的例子，通用爬虫在爬取内容时并不会对网页内容进行筛选，将网页的全部内容给爬取下来。 聚焦爬虫则是只爬取网页上自己需要的内容。 使用语言： php:多线程异步处理能力弱 C/C++:学习成本高，运行速度快但学习和开发成本高 Java:生态圈完善，python爬虫的最大竞争对手。但Java语言本身笨重，代码量大。重构成本搞（有的网站会更新网页编码的规则，需要不断重构来匹配规则） python:语法优美，代码简洁，开发效率高。相关的HTTP请求模块和HTML解析模块非常丰富。还有Scrapy和Scrapy-redis框架让我们开发爬虫变得异常简单。 http协议 HTTP协议：全称是HyperText Transfer...</div></div></div></a><a class="pagination-related" href="/%E7%BA%BF%E7%A8%8B&%E8%BF%9B%E7%A8%8B/" title="线程&amp;进程"><img class="cover" src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="/top_img/10040.webp" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2022-07-08</div><div class="info-item-2">线程&amp;进程</div></div><div class="info-2"><div class="info-item-1">线程和进程是计算机任务处理中的两个概念，一个进程相当于计算机处理的一个任务，一个任务可以找通过多种方式或者找多个不同的人去执行，每一个人或者每一种方式就是一种线程。 多进程问题涉及的是任务的分工问题，一般来说是将一个复杂的任务拆分成多个子任务，每一个子任务执行的时候其它子任务也可以同时执行,例如分布式计算。这种分工的好处是可以保证资源的充分利用，但是如果父任务的执行出现错误或者计算错误，那么后边的任务也会受到影响。多进程问题的优化主要是一个多任务管理的方式问题，一般常用的一种方法是队列。 多线程问题主要涉及到的是协作问题，通过建立多个可以独立完成任务的线程来完成任务，很明显的一个优势是运行的效率会比较高。但是当线程之间如果使用同样的变量时则会存在并发的风险，这会大大降低多线程工作的效率，一般来说多线程的优化问题主要是如何减少线程之间的相互影响，一种比较有效的方式就是加一层锁，限制做个线程同时对一个变量进行更改的权利 在 Python 中，线程不能加速受 CPU 限制的任务，原因是标准 Python 系统中使用了全局解释器锁（GIL）。 GIL 的作用是避免 Python...</div></div></div></a><a class="pagination-related" href="/%E7%BD%91%E9%A1%B5%E4%B8%8B%E8%BD%BD/" title="网页下载"><img class="cover" src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="/top_img/10007.webp" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2022-07-19</div><div class="info-item-2">网页下载</div></div><div class="info-2"><div class="info-item-1">爬虫的第一步是向网页发起模拟请求，一般来说模拟请求的可以借助Python中的urllib模块以及requests模块，其中requests模块是对urllib模块的一个封装，从实用性的角度出发，一般来说我们更建议使用requests模块 request.get发起网页请求 requests库调用是requests.get方法传入url和参数，返回的对象是Response对象，打印出来是显示响应状态码。 Response对象比较重要的三个属性: text:unicode 型的数据，一般是在网页的header中定义的编码形式， content返回的是bytes，二进制型的数据。 json也可以返回json字符串。 如果想要提取文本就用text，但是如果你想要提取图片、文件等二进制文件，就要用content，当然decode之后，中文字符也会正常显示。 修改头文件(Headers) pcUserAgent = &#123;&quot;safari 5.1 – MAC&quot;:&quot;User-Agent:Mozilla/5.0 (Macintosh; U; Intel...</div></div></div></a><a class="pagination-related" href="/%E7%BD%91%E9%A1%B5%E6%8A%93%E5%8C%85/" title="网页抓包"><img class="cover" src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="/top_img/10023.webp" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2022-07-22</div><div class="info-item-2">网页抓包</div></div><div class="info-2"><div class="info-item-1">网页抓包主要指的是对网页的跟踪，包括网页的访问时间、访问者的 IP 地址、访问者的浏览器等信息。在爬虫的过程中，我们看到的网页可能并非是一次就加载出来的，有的网页也可能会分好几步加载，因此跟踪网页的整个加载过程，只有完全掌握了网页抓包的操作，才能得到存放我们需要数据的页面。 网页抓包主要借助的是浏览器的开发者工具，接下来就按照我将使用本博客来对开发者工具进行介绍。 在博客的初始页面打开开发者工具，可以看到如下界面： 默认开发者工具栏出现在右侧，这里为了使用方便放在了下侧，功能一样的。 首先打开的是元素页，这个页面可以用来查看网页的 html 格式和 css 的源码，可以通过左上方小箭头样式来跟踪网页的结构，这对于我们快速定位爬取数据在 html 中的位置有很大的帮助，另外也可以借助右侧的 css 栏目来查看网页加载的 css，每一个样式右上方的蓝色链接存放着 css 的链接，可以用来提取网页美化的样式。 接着是控制台一栏，这一栏我平时用的很少，可以类比为网页的命令行工具，可以用来调取各种你需要的内容（调试 js...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="/img/avatar.webp" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info-name">爱编程的小明</div><div class="author-info-description">只要不折腾，万般可将就</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">100</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">57</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">18</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/kebuAAA"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/kebuAAA" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="/2945190789@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="/img/wechat.webp" target="_blank" title="欢迎交流"><i class="fa-brands fa-weixin"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">如网页加载较慢请尝试魔法上网，博客图文可能无关可以忽略</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%80%89%E6%8B%A9%E5%99%A8-%E6%8F%90%E5%8F%96%E6%95%B0%E6%8D%AE%E7%9A%84%E6%9C%BA%E5%88%B6"><span class="toc-number">1.</span> <span class="toc-text">选择器（提取数据的机制）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#xpath%E9%80%89%E6%8B%A9%E5%99%A8"><span class="toc-number">1.1.</span> <span class="toc-text">XPath选择器</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#response-using-selectors"><span class="toc-number">2.</span> <span class="toc-text">Response（Using selectors）</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#spider"><span class="toc-number">3.</span> <span class="toc-text">Spider</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8F%90%E5%8F%96%E7%88%AC%E5%8F%96%E7%BB%93%E6%9E%9C"><span class="toc-number">3.1.</span> <span class="toc-text">提取爬取结果</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BF%9D%E5%AD%98%E7%88%AC%E5%8F%96%E7%BB%93%E6%9E%9C"><span class="toc-number">3.2.</span> <span class="toc-text">保存爬取结果</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A2%9E%E5%8A%A0%E5%8F%82%E6%95%B0"><span class="toc-number">3.3.</span> <span class="toc-text">增加参数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B7%9F%E8%B8%AA%E9%93%BE%E6%8E%A5-%E5%A4%9A%E4%B8%AA%E7%BD%91%E9%A1%B5%E7%9A%84%E8%B7%B3%E8%BD%AC%E6%8A%93%E5%8F%96"><span class="toc-number">3.4.</span> <span class="toc-text">跟踪链接（多个网页的跳转抓取）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#example"><span class="toc-number">3.4.1.</span> <span class="toc-text">example</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8A%A8%E6%80%81%E7%BD%91%E9%A1%B5"><span class="toc-number">3.5.</span> <span class="toc-text">动态网页</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E6%9D%A5%E8%87%AA%E5%A4%9A%E4%B8%AA%E9%A1%B5%E9%9D%A2%E6%95%B0%E6%8D%AE%E7%BB%84%E6%88%90%E7%9A%84item"><span class="toc-number">3.6.</span> <span class="toc-text">生成来自多个页面数据组成的item</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%80%9A%E7%94%A8%E7%88%AC%E8%99%AB"><span class="toc-number">4.</span> <span class="toc-text">通用爬虫</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#class-scrapy-spiders-crawlspider"><span class="toc-number">4.1.</span> <span class="toc-text">class scrapy.spiders.CrawlSpider</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#item-pipelines"><span class="toc-number">5.</span> <span class="toc-text">item pipelines</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#some-examples"><span class="toc-number">5.1.</span> <span class="toc-text">Some examples</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%BE%E7%89%87%E7%88%AC%E5%8F%96"><span class="toc-number">5.2.</span> <span class="toc-text">图片爬取</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B0%83%E7%94%A8"><span class="toc-number">5.3.</span> <span class="toc-text">调用</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#downloader-middleware-%E5%85%A8%E5%B1%80%E6%94%B9%E5%86%99requests-response"><span class="toc-number">6.</span> <span class="toc-text">Downloader Middleware（全局改写requests&#x2F;response）</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#scrapy%E6%97%A5%E5%BF%97%E7%AE%A1%E7%90%86"><span class="toc-number">7.</span> <span class="toc-text">Scrapy日志管理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%88%E7%AB%AF%E8%BE%93%E5%87%BA%E5%91%BD%E4%BB%A4%E7%9A%84%E9%80%89%E6%8B%A9"><span class="toc-number">7.1.</span> <span class="toc-text">终端输出命令的选择</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BE%93%E5%87%BA%E5%8D%95%E7%8B%AC%E7%9A%84%E6%97%A5%E5%BF%97%E6%96%87%E4%BB%B6"><span class="toc-number">7.2.</span> <span class="toc-text">输出单独的日志文件</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/Lasso%E5%9B%9E%E5%BD%92/" title="Lasso回归"><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="/top_img/10057.webp" onerror='this.onerror=null,this.src="https://images.pexels.com/photos/374918/pexels-photo-374918.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=600"' alt="Lasso回归"></a><div class="content"><a class="title" href="/Lasso%E5%9B%9E%E5%BD%92/" title="Lasso回归">Lasso回归</a><time datetime="2023-11-14T16:00:00.000Z" title="更新于 2023-11-15 00:00:00">2023-11-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/%E5%B2%AD%E5%9B%9E%E5%BD%92/" title="岭回归"><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/岭回归_20231109082818.png" onerror='this.onerror=null,this.src="https://images.pexels.com/photos/374918/pexels-photo-374918.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=600"' alt="岭回归"></a><div class="content"><a class="title" href="/%E5%B2%AD%E5%9B%9E%E5%BD%92/" title="岭回归">岭回归</a><time datetime="2023-11-07T16:00:00.000Z" title="更新于 2023-11-08 00:00:00">2023-11-08</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/%E4%BC%98%E9%9B%85%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/" title="优雅论文排版"><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/优雅论文排版_20230921093206.png" onerror='this.onerror=null,this.src="https://images.pexels.com/photos/374918/pexels-photo-374918.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=600"' alt="优雅论文排版"></a><div class="content"><a class="title" href="/%E4%BC%98%E9%9B%85%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/" title="优雅论文排版">优雅论文排版</a><time datetime="2023-09-20T16:00:00.000Z" title="更新于 2023-09-21 00:00:00">2023-09-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/%E5%A4%9A%E5%85%83%E7%BB%9F%E8%AE%A1%E5%88%86%E6%9E%90/" title="多元统计分析"><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/多元统计分析_多元正态曲线.png" onerror='this.onerror=null,this.src="https://images.pexels.com/photos/374918/pexels-photo-374918.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=600"' alt="多元统计分析"></a><div class="content"><a class="title" href="/%E5%A4%9A%E5%85%83%E7%BB%9F%E8%AE%A1%E5%88%86%E6%9E%90/" title="多元统计分析">多元统计分析</a><time datetime="2023-06-16T02:22:54.000Z" title="更新于 2023-06-16 10:22:54">2023-06-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/Hypothesis%20testing/" title="假设检验"><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="/top_img/10019.webp" onerror='this.onerror=null,this.src="https://images.pexels.com/photos/374918/pexels-photo-374918.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=600"' alt="假设检验"></a><div class="content"><a class="title" href="/Hypothesis%20testing/" title="假设检验">假设检验</a><time datetime="2023-05-09T02:34:00.000Z" title="更新于 2023-05-09 10:34:00">2023-05-09</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="footer_custom_text"><div id="runtime"></div></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"></div><div id="rightside-config-show"><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><div class="js-pjax"></div><script src="/Scripts/js/beijing.js"></script><script src="/Scripts/js/foot_style.js"></script><script src="/Scripts/js/fireworks.js"></script><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>(()=>{window.pjax=new Pjax({elements:'a:not([target="_blank"])',selectors:["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"],cacheBust:!1,analytics:!1,scrollRestoration:!1});const e=e=>{e&&Object.values(e).forEach((e=>e()))};document.addEventListener("pjax:send",(()=>{btf.removeGlobalFnEvent("pjaxSendOnce"),btf.removeGlobalFnEvent("themeChange");const t=document.body.classList;t.contains("read-mode")&&t.remove("read-mode"),e(window.globalFn.pjaxSend)})),document.addEventListener("pjax:complete",(()=>{btf.removeGlobalFnEvent("pjaxCompleteOnce"),document.querySelectorAll("script[data-pjax]").forEach((e=>{const t=document.createElement("script"),n=e.text||e.textContent||e.innerHTML||"";Array.from(e.attributes).forEach((e=>t.setAttribute(e.name,e.value))),t.appendChild(document.createTextNode(n)),e.parentNode.replaceChild(t,e)})),e(window.globalFn.pjaxComplete)})),document.addEventListener("pjax:error",(e=>{404===e.request.status&&pjax.loadUrl("/404.html")}))})()</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><div class="js-pjax"><script async>for(var arr=document.getElementsByClassName("recent-post-item"),i=0;i<arr.length;i++)arr[i].classList.add("wow"),arr[i].classList.add("animate__zoomIn"),arr[i].setAttribute("data-wow-duration","2s"),arr[i].setAttribute("data-wow-delay","0.5s"),arr[i].setAttribute("data-wow-offset","100"),arr[i].setAttribute("data-wow-iteration","1")</script><script async>for(var arr=document.getElementsByClassName("card-widget"),i=0;i<arr.length;i++)arr[i].classList.add("wow"),arr[i].classList.add("animate__zoomIn"),arr[i].setAttribute("data-wow-duration",""),arr[i].setAttribute("data-wow-delay",""),arr[i].setAttribute("data-wow-offset",""),arr[i].setAttribute("data-wow-iteration","")</script></div><script defer src="https://unpkg.zhimg.com/hexo-butterfly-wowjs/lib/wow.min.js"></script><script defer src="https://unpkg.zhimg.com/hexo-butterfly-wowjs/lib/wow_init.js"></script></body></html>