<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>神经网络学习 | 小明的博客</title><meta name="author" content="爱编程的小明"><meta name="copyright" content="爱编程的小明"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#e68ab8"><meta name="description" content="发展 1958年，计算科学家Rosenblatt提出了由两层神经元组成的神经网络，并将之命名为“感知器”(Perceptron)。 感知器有两个层次：输入层和输出层。 输入层里的“输入单元”只负责传输数据，不做计算。 输出层里的“输出单元”则需要对前面一层的输入进行计算。 感知器是当时首个可以学习的"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://kobal.top/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//www.clarity.ms"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://gcore.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://gcore.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?4e3a07c287f8fb6cfc09bf5a7fdc1dd7";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script>!function(e,t,n,c,a,i,r){e[n]=e[n]||function(){(e[n].q=e[n].q||[]).push(arguments)},(i=t.createElement(c)).async=1,i.src="https://www.clarity.ms/tag/e8bjif1knd",(r=t.getElementsByTagName(c)[0]).parentNode.insertBefore(i,r)}(window,document,"clarity","script")</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:void 0,translate:{defaultEncoding:1,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"簡"},noticeOutdate:void 0,highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:120},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!1},runtime:"",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://gcore.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!1,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!1},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"神经网络学习",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2022-08-29 20:57:00"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach((e=>{n.setAttribute(e,t[e])})),document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#e68ab8")};const t=saveToLocal.get("theme");"dark"===t?activateDarkMode():"light"===t&&activateLightMode();const o=saveToLocal.get("aside-status");void 0!==o&&("hide"===o?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><style>#article-container.post-content h1:before,h2:before,h3:before,h4:before,h5:before,h6:before{-webkit-animation:avatar_turn_around 1s linear infinite;-moz-animation:avatar_turn_around 1s linear infinite;-o-animation:avatar_turn_around 1s linear infinite;-ms-animation:avatar_turn_around 1s linear infinite;animation:avatar_turn_around 1s linear infinite}</style><link rel="stylesheet" href="/Scripts/css/transparent.css"><link rel="stylesheet" href="/Scripts/css/font.css"><link rel="stylesheet" href="/Scripts/css/foot_style.css"><link rel="stylesheet" href="/Scripts/css/twikoo_beautify.css"><link rel="stylesheet" href="/Scripts/css/tags.css"><link rel="stylesheet" href="https://gcore.jsdelivr.net/gh/Zfour/Butterfly-double-row-display@1.00/cardlistpost.min.css"><style>#recent-posts>.recent-post-item>.recent-post-info>.article-meta-wrap>.tags:before{content:"\A";white-space:pre}#recent-posts>.recent-post-item>.recent-post-info>.article-meta-wrap>.tags>.article-meta__separator{display:none}</style><link rel="stylesheet" href="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/animate.css/4.1.1/animate.min.css" media="print" onload='this.media="screen"'><meta name="generator" content="Hexo 7.3.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="/img/avatar.webp" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">100</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">57</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">18</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>存档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i> <span>关于</span></a></div><div class="menus_item"><a class="site-page" href="/wallpaper/"><i class="fa-fw fas fa-image fa-fw"></i> <span>壁纸</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://www.travellings.cn/go.html"><i class="fa-fw fa fa-subway"></i> <span>开往</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="小明的博客"><span class="site-name">小明的博客</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>存档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i> <span>关于</span></a></div><div class="menus_item"><a class="site-page" href="/wallpaper/"><i class="fa-fw fas fa-image fa-fw"></i> <span>壁纸</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://www.travellings.cn/go.html"><i class="fa-fw fa fa-subway"></i> <span>开往</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">神经网络学习<a class="post-edit-link" href="https://github.dev/kebuAAA/myblog/blob/main/source/_posts/神经网络学习.md" title="编辑" target="_blank"><i class="fas fa-pencil-alt"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-02-25T12:57:00.000Z" title="发表于 2022-02-25 20:57:00">2022-02-25</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-08-29T12:57:00.000Z" title="更新于 2022-08-29 20:57:00">2022-08-29</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="神经网络学习"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><h1 id="发展">发展</h1><p>1958年，计算科学家Rosenblatt提出了由两层神经元组成的神经网络，并将之命名为“感知器”(Perceptron)。<br>感知器有两个层次：输入层和输出层。<br>输入层里的“输入单元”只负责传输数据，不做计算。<br>输出层里的“输出单元”则需要对前面一层的输入进行计算。<br>感知器是当时首个可以学习的人工神经网络。Rosenblatt现场演示了其学习识别简单图像的过程。<br><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0/0.png" alt="image.png"><br>按照不同的连接方式，神经网络可以分为：</p><ul><li>感知器模型</li><li>多层感知机模型</li><li>前向多层神经网络</li><li>Hopfield神经网络</li><li>动态反馈网络</li><li>自组织神经网络等。</li></ul><p><strong>1986年</strong>，Rumelhar和Hinton等人提出了反向传播（Back Propagation，BP）算法，解决了两层神经网络所需要的复杂计算量问题，从而带动了使用两层神经网络研究的热潮。<br>两层神经网络除了包含一个输入层，一个输出层以外，还增加了一个中间层。<br>中间层和输出层都是计算层。<br>理论证明，<strong>两层神经网络可以无限逼近任意连续函数</strong>。<br>面对复杂的非线性分类任务，带一个隐藏层的两层神经网络可以取得很好的分类结果。<br><strong>2006年</strong>，Hinton在《Science》和相关期刊上发表了论文，首次提出了“深度信念网络”的概念。<br>与传统的训练方式不同，“深度信念网络”有一个“预训练”（Pre-Training）的过程，这可以方便的让神经网络中的权值找到一个接近最优解的值，之后再使用“微调”(Fine-Tuning)技术来对整个网络进行优化训练。<br>这两个技术的运用大幅度减少了训练多层神经网络的时间。<br>他给多层神经网络相关的学习方法赋予了一个新名词——“<strong>深度学习</strong>”。<br><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0/1.png" alt="image.png"></p><h1 id="含义">含义</h1><h2 id="层数的增加意味着抽象表示的深入">层数的增加意味着抽象表示的深入</h2><p>例如在图像分类中，神经网络的第一个隐藏层学习到的是“边缘”的特征。<br>第二个隐藏层学习到的是由“边缘”组成的“形状”的特征。<br>第三个隐藏层学习到的是由“形状”组成的“图案”的特征。<br>最后的隐藏层学习到的是由“图案”组成的“目标”的特征。<br>通过抽取更抽象的特征来对事物进行区分，从而获得更好的区分与分类能力。</p><h2 id="单层神经元的增多意味着维度的升高">单层神经元的增多意味着维度的升高</h2><p>有点类似于SVM核方法，但是需要注意的是每一个神经元都会对应一个激活函数，这点与SVM存在明显差异。</p><h2 id="优点">优点</h2><ul><li>非线性映射逼近</li><li>大规模并行分布式存储</li><li>综合优化处理</li><li>容错性强（不确定性大的问题）</li><li>独特的联想记忆</li><li>自组织自适应自学习能力</li></ul><h2 id="分类">分类</h2><h3 id="前馈神经网络">前馈神经网络</h3><p>前馈神经网络是实际应用中最常见的神经网络类型。<br>第一层是输入。<br>最后一层是输出。<br>如果有多个隐藏层，称为“深层” 神经网络。<br>每层神经元的活动都是下层中活动的非线性函数。</p><h3 id="递归神经网络">递归神经网络</h3><p>递归神经网络的连接中有直接的循环，使得信息有时可以回到开始的地方。<br>它们可能有复杂的动态，这可能会使它们很难训练。<br>它们更具生物现实性。<br>递归神经网络是建模时序数据的一种非常自然的方法。<br>它们相当于每个时间片具有一个隐藏层的非常深的网络；<br>除了它们在每个时间片上使用相同的权重并且它们在每个时间片都得到输入。</p><h3 id="对称连接网络">对称连接网络</h3><p>对称连接网络像递归网络，但是单元之间的连接是对称的（它们在两个方向上具有相同的权重）。<br>对称网络比递归网络更容易分析。 因为它们服从能量函数，所以它们在做的事情上也受到更多的限制。<br>没有隐藏单元的对称连接的网络被称为“霍普菲尔德网络”。<br>具有隐藏单元的对称连接网络称为“玻尔兹曼机器”。</p><h1 id="前馈神经网络">前馈神经网络</h1><h2 id="反向传播算法">反向传播算法</h2><p>BP（Back propagation）<strong>由输入层开始逐层向前计算神经元输出，由输出层开始逐层向后计算神经元的误差</strong>。<br>误差函数优化的过程中按照梯度下降法，<strong>保证误差损失函数快速收敛</strong>。</p><ul><li>梯度下降算法（BGD）：每次学习都会使用整个训练集</li><li>随机梯度下降法（SGD, Stochastic Gradient Descent）. SGD算法在每一轮迭代中只用一条随机选取的数据，这将使得迭代次数大大增加.学习时间短，但可能会导致损失函数剧烈波动（尤其在最优解附近），难以判断是否收敛。</li><li>小批量梯度下降法（MBGD, Mini-Batch Gradient Descent）。MBGD采用一次迭代多条数据的方法，即每次迭代不是仅有一个样本参与训练，而是有一批样本参与迭代训练。批量大小选择合理的话可以在性能上由于以上两种算法。</li></ul><h1 id="概率神经网络-pnn">概率神经网络（PNN）</h1><p>概率神经网络(<strong>Probabilistic Neural Network, PNN</strong>)由D.F.Speeht博士在1989年首先提出。<br>它是径向基网络的一个分支，属于前馈网络的一种。<br>概率神经网络一般由输入层、模式层、求和层和输出层四层构成。有时也把模式层称为隐含层，把求和层叫做竞争层。</p><ul><li>输入层负责把特征向量传入网络</li><li>模式层又叫隐含层，计算输入特征和训练集中各模式的相似度，将距离送入高斯函数得到输出。模式层的数量即训练集元素的数量。</li><li>求和层又叫竞争层。数量是样本的类别个数</li><li>输出层负责输出求和层中得分最高的一类</li></ul><p><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0/2.png" alt="image.png"><br><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0/3.png" alt="image.png"><br><strong>特性</strong><br>训练容易，收敛速度快，非常适用于实时处理；<br>可以实现任意的非线性逼近，用PNN网络形成的判决曲面与贝叶斯最优准则下的曲面非常接近；<br>隐含层采用径向基的非线性映射函数，考虑了不同类别模式样本的交错影响，具有很强的容错性；<br>隐含层的传输函数可以选用各种用来估计概率密度的基函数，且分类结果对基函数的形式不敏感；<br>扩充性能好，增加或减少类别模式时不需要重新进行长时间的训练学习；<br>各层神经元的数目比较固定，易于硬件实现。<br><strong>神经网络的性能并不随着隐层数量和隐层神经元数量的增加而增加。因此，建立神经网络隐层的原则是：在满足分类准确率的前提下，网络结构尽可能简单</strong>。</p><h1 id="卷积神经网络">卷积神经网络</h1><p>卷积神经网络（Convolutional Neural Networks, CNN）是一种包含卷积计算且具有深度结构的前馈神经网络，是<strong>深度学习</strong>（deep learning）的代表算法。卷积神经网络在图像、文本、语音的分析和识别。</p><h2 id="卷积">卷积</h2><p><strong>卷积的本质是一个窗口信号对另一个信号的加权</strong>，通过取一个内部带有权值的窗口，使用该窗口对某个信号进行滤波。移动该窗口对原始信号处理（使得更加平滑），因此卷积的本质是<strong>滤波</strong>。</p><h3 id="一维卷积">一维卷积</h3><p>一维卷积的运算通常为对于一个被研究的数据序列f(t),先确定一个滤波器函数g(t)，然后将g翻转得到得到g(-t)，接着再把翻转后的函数向右平移n个单位得到g(n-t),通过对g(n-t)平移得到在f(t)定义域上的g(n-t),在相同t值处取函数值相乘（定义内积运算），即：</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mi>g</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mo>∫</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mi>g</mi><mo stretchy="false">(</mo><mi>n</mi><mo>−</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(t)g(t)=\int f(t)g(n-t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathdefault" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mclose">)</span><span class="mord mathdefault" style="margin-right:.03588em">g</span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:2.22225em;vertical-align:-.86225em"></span><span class="mop op-symbol large-op" style="margin-right:.44445em;position:relative;top:-.0011249999999999316em">∫</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathdefault" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mclose">)</span><span class="mord mathdefault" style="margin-right:.03588em">g</span><span class="mopen">(</span><span class="mord mathdefault">n</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathdefault">t</span><span class="mclose">)</span></span></span></span></span></p><h3 id="二维卷积">二维卷积</h3><p>在实际生活过程中我们遇到的大多数数据都是二维的，为了对这些数据进行处理我们需要定义二维的卷积运算，具体操作是：首先定义一个带有权值的<strong>小窗口</strong>，即滤波器，然后沿着行或列移动窗口对图像的特征进行加权平均。<br>下面简单介绍一下图像卷积的基本过程：</p><h4 id="卷积">卷积</h4><p>通过设定一个一定大小的滤波器，对图像对应位置的像素进行加权平均可以得到一个值。然后移动滤波器进行重复计算。<br><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0/4.png" alt="image.png"><br>设计不同形状、权值分布的滤波器，实现边缘检测、浮雕、模糊等图像处理与分析目的。<br>因此图像的卷积操作在本质上也是一种滤波。<br>彩色图像一般有3个颜色通道，可以将滤波器分别作用于每个颜色通道，从而得到3个通道的卷积结果。</p><h4 id="填充">填充</h4><p>一般的卷积操作会导致原始图像的大小发生改变，为了使得在卷积后图像的大小不发生改变，常常会对原图进行填充，在原始图像的四周添加0以使得卷积的结果形状保持不变。<br><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0/8.gif" alt="图片1.gif"></p><h4 id="池化">池化</h4><p>池化是对卷积结果进行进一步的降维。本质也是一种卷积操作，使用滤波器（最大值或者最小值函数）提取图像的局部特征。<br>池化的一个好处是<strong>平移旋转</strong>不变性。<br><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0/5.png" alt="image.png"></p><h2 id="卷积神经网络的结构">卷积神经网络的结构</h2><p>卷积神经网络的结构一般包括输入层，卷积层，池化层，全连接层，输出层。<br><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0/6.png" alt="image.png"></p><h3 id="从一个图像卷积神经网络的例子看卷积神经网络">从一个图像卷积神经网络的例子看卷积神经网络</h3><p>从我们认知图像的机制出发（先感知颜色和局部特征，然后感知纹理和几何形状等更复杂的特征，最后形成整个物体的概念），人们设计出了一种卷积神经网络来进行模仿。<br>由多个卷积层构成，每个卷积层包含多个卷积核，用这些卷积核从左向右、从上往下依次扫描整个图像，得到称为特征图（feature map）的输出数据。<br>网络前面的卷积层捕捉图像局部、细节信息，有小的感受野，即输出图像的每个像素只利用输入图像很小的一个范围。<br>后面的卷积层感受野逐层加大，用于捕获图像更复杂，更抽象的信息。<br>经过多个卷积层的运算，最后得到图像在各个不同尺度的抽象表示。</p><ul><li>输入层接收图像的像素数据，既可以是单通道的灰度图像，也可以是三通道彩色图像。</li><li>卷积层使用滤波器对输入层进行卷积操作。卷积操作用于提取图像的特征。可以使用多个卷积核，获取不同的图像特征。</li><li>经过卷积运算之后，图像尺寸变小了。可以先对图像进行扩充（padding），例如在周边补0，然后用尺寸扩大后的图像进行卷积，保证卷积结果图像和原图像尺寸相同。</li><li>卷积操作完成对输入图像的降维和特征抽取，但特征图像的维数还很高，计算耗时，易过拟合。为此引入了下采样技术，也称为pooling即池化操作。池化的做法是对图像的某一个区域用一个值代替，如最大值或平均值。如果采用最大值，叫做max池化；如果采用均值，叫做均值池化。</li><li>最后，一个池化层的输出送入全连接层之前，还要对特征进行正则化，将其变换到[0,1]区间。</li><li>卷积神经网络的输出层一般是类别标签。需要将数据集的类别标签量化编码，为了不使机器学习模型认为类别是有序的，一般需要对类标签进行独热(one-hot)编码。</li></ul><p>在图像分类中，通常有两层以上的卷积层和池化层，用于图像局部特征、全局特征的多次提取和降维。卷积层的神经元与池化层的神经元并不是全连接的。一个卷积核得到的特征图，对应一种池化操作。如果需要再次卷积操作，则只将该池化操作得到结果与下一个卷积层的单个神经元发生联系，而不必与其它神经元都发生联系。<br><strong>实际应用的过程中经常是多通道图像，需要对各个通道进行卷积然后进行累加</strong></p><h3 id="激励函数的选择">激励函数的选择</h3><p>传统的sigmoid激活函数能满足激励函数的要求，但是也存在如下缺点：</p><ul><li>输入值很小或者很大时，输出曲线基本变为水平直线，导致局部梯度值非常非常小，不利于梯度下降的优化算法。</li><li>当输入为0时，输出不为0，由于每一层的输出都要作为下一层的输入，而未0中心化会影响梯度下降，影响梯度下降的动态性。</li></ul><p>在卷积神经网络中，更常使用的是ReLU (Recitified Linear Unit)线性整流函数。<br><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0/7.png" alt="image.png"><br>Relu函数的优点是不会出现梯度消失，收敛速度快；<br>前向计算量小，只需要计算max(0, x)，不像sigmoid中有指数计算；<br>反向传播计算快，导数计算简单，无需指数计算；<br>有些神经元的值为0，使网络具有稀疏性质，可减小过拟合。<br>缺点是比较脆弱，反向传播中如果一个参数为0，后面的参数就会不更新。学习性能和参数设置有关系。</p></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://kobal.top">爱编程的小明</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://kobal.top/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0/">https://kobal.top/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kobal.top" target="_blank">小明的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Deep-Learning/">Deep-Learning</a><a class="post-meta__tags" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">神经网络</a></div><div class="post_share"><div class="social-share" data-image="/top_img/10026.webp" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://gcore.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://gcore.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90/" title="聚类分析"><img class="cover" src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="/top_img/10055.webp" onerror='onerror=null,src="https://images.pexels.com/photos/374918/pexels-photo-374918.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=600"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">聚类分析</div></div></a></div><div class="next-post pull-right"><a href="/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/" title="爬虫入门"><img class="cover" src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="/top_img/10038.webp" onerror='onerror=null,src="https://images.pexels.com/photos/374918/pexels-photo-374918.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=600"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">爬虫入门</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/Tensorflow%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/" title="Tensorflow深度学习框架"><img class="cover" src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="/top_img/10044.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-01</div><div class="title">Tensorflow深度学习框架</div></div></a></div><div><a href="/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="神经网络"><img class="cover" src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="/top_img/10042.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-01</div><div class="title">神经网络</div></div></a></div></div></div><hr class="custom-hr"><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i> <span>评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="/img/avatar.webp" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info__name">爱编程的小明</div><div class="author-info__description">只要不折腾，万般可将就</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">100</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">57</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">18</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/kebuAAA"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/kebuAAA" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="/2945190789@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="/img/wechat.webp" target="_blank" title="欢迎交流"><i class="fa-brands fa-weixin"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">如网页加载较慢请尝试魔法上网，博客图文可能无关可以忽略</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%91%E5%B1%95"><span class="toc-number">1.</span> <span class="toc-text">发展</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%90%AB%E4%B9%89"><span class="toc-number">2.</span> <span class="toc-text">含义</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B1%82%E6%95%B0%E7%9A%84%E5%A2%9E%E5%8A%A0%E6%84%8F%E5%91%B3%E7%9D%80%E6%8A%BD%E8%B1%A1%E8%A1%A8%E7%A4%BA%E7%9A%84%E6%B7%B1%E5%85%A5"><span class="toc-number">2.1.</span> <span class="toc-text">层数的增加意味着抽象表示的深入</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%95%E5%B1%82%E7%A5%9E%E7%BB%8F%E5%85%83%E7%9A%84%E5%A2%9E%E5%A4%9A%E6%84%8F%E5%91%B3%E7%9D%80%E7%BB%B4%E5%BA%A6%E7%9A%84%E5%8D%87%E9%AB%98"><span class="toc-number">2.2.</span> <span class="toc-text">单层神经元的增多意味着维度的升高</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%98%E7%82%B9"><span class="toc-number">2.3.</span> <span class="toc-text">优点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E7%B1%BB"><span class="toc-number">2.4.</span> <span class="toc-text">分类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">2.4.1.</span> <span class="toc-text">前馈神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">2.4.2.</span> <span class="toc-text">递归神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%B9%E7%A7%B0%E8%BF%9E%E6%8E%A5%E7%BD%91%E7%BB%9C"><span class="toc-number">2.4.3.</span> <span class="toc-text">对称连接网络</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">3.</span> <span class="toc-text">前馈神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95"><span class="toc-number">3.1.</span> <span class="toc-text">反向传播算法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A6%82%E7%8E%87%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-pnn"><span class="toc-number">4.</span> <span class="toc-text">概率神经网络（PNN）</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">5.</span> <span class="toc-text">卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF"><span class="toc-number">5.1.</span> <span class="toc-text">卷积</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E7%BB%B4%E5%8D%B7%E7%A7%AF"><span class="toc-number">5.1.1.</span> <span class="toc-text">一维卷积</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E7%BB%B4%E5%8D%B7%E7%A7%AF"><span class="toc-number">5.1.2.</span> <span class="toc-text">二维卷积</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF"><span class="toc-number">5.1.2.1.</span> <span class="toc-text">卷积</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A1%AB%E5%85%85"><span class="toc-number">5.1.2.2.</span> <span class="toc-text">填充</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B1%A0%E5%8C%96"><span class="toc-number">5.1.2.3.</span> <span class="toc-text">池化</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84"><span class="toc-number">5.2.</span> <span class="toc-text">卷积神经网络的结构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%8E%E4%B8%80%E4%B8%AA%E5%9B%BE%E5%83%8F%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BE%8B%E5%AD%90%E7%9C%8B%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">5.2.1.</span> <span class="toc-text">从一个图像卷积神经网络的例子看卷积神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%BF%80%E5%8A%B1%E5%87%BD%E6%95%B0%E7%9A%84%E9%80%89%E6%8B%A9"><span class="toc-number">5.2.2.</span> <span class="toc-text">激励函数的选择</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/Lasso%E5%9B%9E%E5%BD%92/" title="Lasso回归"><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="/top_img/10007.webp" onerror='this.onerror=null,this.src="https://images.pexels.com/photos/374918/pexels-photo-374918.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=600"' alt="Lasso回归"></a><div class="content"><a class="title" href="/Lasso%E5%9B%9E%E5%BD%92/" title="Lasso回归">Lasso回归</a><time datetime="2023-11-14T16:00:00.000Z" title="更新于 2023-11-15 00:00:00">2023-11-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/%E5%B2%AD%E5%9B%9E%E5%BD%92/" title="岭回归"><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/岭回归_20231109082818.png" onerror='this.onerror=null,this.src="https://images.pexels.com/photos/374918/pexels-photo-374918.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=600"' alt="岭回归"></a><div class="content"><a class="title" href="/%E5%B2%AD%E5%9B%9E%E5%BD%92/" title="岭回归">岭回归</a><time datetime="2023-11-07T16:00:00.000Z" title="更新于 2023-11-08 00:00:00">2023-11-08</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/%E4%BC%98%E9%9B%85%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/" title="优雅论文排版"><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/优雅论文排版_20230921093206.png" onerror='this.onerror=null,this.src="https://images.pexels.com/photos/374918/pexels-photo-374918.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=600"' alt="优雅论文排版"></a><div class="content"><a class="title" href="/%E4%BC%98%E9%9B%85%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/" title="优雅论文排版">优雅论文排版</a><time datetime="2023-09-20T16:00:00.000Z" title="更新于 2023-09-21 00:00:00">2023-09-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/%E5%A4%9A%E5%85%83%E7%BB%9F%E8%AE%A1%E5%88%86%E6%9E%90/" title="多元统计分析"><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/多元统计分析_多元正态曲线.png" onerror='this.onerror=null,this.src="https://images.pexels.com/photos/374918/pexels-photo-374918.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=600"' alt="多元统计分析"></a><div class="content"><a class="title" href="/%E5%A4%9A%E5%85%83%E7%BB%9F%E8%AE%A1%E5%88%86%E6%9E%90/" title="多元统计分析">多元统计分析</a><time datetime="2023-06-16T02:22:54.000Z" title="更新于 2023-06-16 10:22:54">2023-06-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/Hypothesis%20testing/" title="假设检验"><img src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="/top_img/10026.webp" onerror='this.onerror=null,this.src="https://images.pexels.com/photos/374918/pexels-photo-374918.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=600"' alt="假设检验"></a><div class="content"><a class="title" href="/Hypothesis%20testing/" title="假设检验">假设检验</a><time datetime="2023-05-09T02:34:00.000Z" title="更新于 2023-05-09 10:34:00">2023-05-09</time></div></div></div></div></div></div></main><footer id="footer" style="background:0 0"><div id="footer-wrap"><div class="footer_custom_text"><div id="runtime"></div></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"></div><div id="rightside-config-show"><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="/js/tw_cn.js?v=4.13.0"></script><script src="https://gcore.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/instant.page/5.1.0/instantpage.min.js" type="module"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/vanilla-lazyload/17.3.1/lazyload.iife.min.js"></script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://gcore.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script><script>(()=>{const t=()=>{twikoo.init(Object.assign({el:"#twikoo-wrap",envId:"https://twikoo.kobal.top/",region:"",onCommentLoaded:()=>{btf.loadLightbox(document.querySelectorAll("#twikoo .tk-content img:not(.tk-owo-emotion)"))}},null))},o=()=>{"object"==typeof twikoo?setTimeout(t,0):getScript("https://cdn.staticfile.org/twikoo/1.6.22/twikoo.all.min.js").then(t)};btf.loadComment(document.getElementById("twikoo-wrap"),o)})()</script></div><script src="/Scripts/js/beijing.js"></script><script src="/Scripts/js/foot_style.js"></script><script src="/Scripts/js/fireworks.js"></script><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.bootcdn.net/ajax/libs/butterfly-extsrc/1.1.3/fireworks.min.js"></script><script src="https://lib.baomitu.com/pjax/0.2.8/pjax.min.js"></script><script>let pjaxSelectors=["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"];var pjax=new Pjax({elements:'a:not([target="_blank"])',selectors:pjaxSelectors,cacheBust:!1,analytics:!1,scrollRestoration:!1});document.addEventListener("pjax:send",(function(){if(btf.removeGlobalFnEvent("pjax"),btf.removeGlobalFnEvent("themeChange"),document.getElementById("rightside").classList.remove("rightside-show"),window.aplayers)for(let e=0;e<window.aplayers.length;e++)window.aplayers[e].options.fixed||window.aplayers[e].destroy();"object"==typeof typed&&typed.destroy();const e=document.body.classList;e.contains("read-mode")&&e.remove("read-mode"),"object"==typeof disqusjs&&disqusjs.destroy()})),document.addEventListener("pjax:complete",(function(){window.refreshFn(),document.querySelectorAll("script[data-pjax]").forEach((e=>{const t=document.createElement("script"),o=e.text||e.textContent||e.innerHTML||"";Array.from(e.attributes).forEach((e=>t.setAttribute(e.name,e.value))),t.appendChild(document.createTextNode(o)),e.parentNode.replaceChild(t,e)})),GLOBAL_CONFIG.islazyload&&window.lazyLoadInstance.update(),"function"==typeof panguInit&&panguInit(),"function"==typeof gtag&&gtag("config","",{page_path:window.location.pathname}),"object"==typeof _hmt&&_hmt.push(["_trackPageview",window.location.pathname]),"function"==typeof loadMeting&&document.getElementsByClassName("aplayer").length&&loadMeting(),"object"==typeof Prism&&Prism.highlightAll()})),document.addEventListener("pjax:error",(e=>{404===e.request.status&&pjax.loadUrl("/404.html")}))</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><div class="js-pjax"><script async>for(var arr=document.getElementsByClassName("recent-post-item"),i=0;i<arr.length;i++)arr[i].classList.add("wow"),arr[i].classList.add("animate__zoomIn"),arr[i].setAttribute("data-wow-duration","2s"),arr[i].setAttribute("data-wow-delay","0.5s"),arr[i].setAttribute("data-wow-offset","100"),arr[i].setAttribute("data-wow-iteration","1")</script><script async>for(var arr=document.getElementsByClassName("card-widget"),i=0;i<arr.length;i++)arr[i].classList.add("wow"),arr[i].classList.add("animate__zoomIn"),arr[i].setAttribute("data-wow-duration",""),arr[i].setAttribute("data-wow-delay",""),arr[i].setAttribute("data-wow-offset",""),arr[i].setAttribute("data-wow-iteration","")</script></div><script defer src="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/wow/1.1.2/wow.min.js"></script><script defer src="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/wow_init.js"></script></body></html>