<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>如何使用机器学习神器sklearn做特征工程？ | 小明的博客</title><meta name="keywords" content="机器学习编程,特征工程"><meta name="author" content="可不"><meta name="copyright" content="可不"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#e68ab8"><meta name="description" content="使用 sklearn 做特征工程 特征工程是什么？有这么一句话在业界广泛流传：数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。那特征工程到底是什么呢？顾名思义，其本质是一项工程活动，目的是最大限度地从原始数据中提取特征以供算法和模型使用。 通过总结和归纳，人们认为特征工程包括以下方面：  特征处理是特征工程的核心部分，sklearn 提供了较为完整的特征处理方法，包括数据预处理，">
<meta property="og:type" content="article">
<meta property="og:title" content="如何使用机器学习神器sklearn做特征工程？">
<meta property="og:url" content="https://kebuaaa.github.io/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%A5%9E%E5%99%A8sklearn%E5%81%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%EF%BC%9F/index.html">
<meta property="og:site_name" content="小明的博客">
<meta property="og:description" content="使用 sklearn 做特征工程 特征工程是什么？有这么一句话在业界广泛流传：数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。那特征工程到底是什么呢？顾名思义，其本质是一项工程活动，目的是最大限度地从原始数据中提取特征以供算法和模型使用。 通过总结和归纳，人们认为特征工程包括以下方面：  特征处理是特征工程的核心部分，sklearn 提供了较为完整的特征处理方法，包括数据预处理，">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://images.pexels.com/photos/159866/books-book-pages-read-literature-159866.jpeg?auto=compress&cs=tinysrgb&w=600">
<meta property="article:published_time" content="2022-03-26T02:34:00.000Z">
<meta property="article:modified_time" content="2022-07-09T02:00:34.666Z">
<meta property="article:author" content="可不">
<meta property="article:tag" content="机器学习编程">
<meta property="article:tag" content="特征工程">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://images.pexels.com/photos/159866/books-book-pages-read-literature-159866.jpeg?auto=compress&cs=tinysrgb&w=600"><link rel="shortcut icon" href="../img/favicon.png"><link rel="canonical" href="https://kebuaaa.github.io/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%A5%9E%E5%99%A8sklearn%E5%81%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%EF%BC%9F/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="../css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":1,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"prismjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '如何使用机器学习神器sklearn做特征工程？',
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2022-07-09 10:00:34'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#e68ab8')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.2"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="../img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="../archives/"><div class="headline">文章</div><div class="length-num">90</div></a><a href="../tags/"><div class="headline">标签</div><div class="length-num">61</div></a><a href="../categories/"><div class="headline">分类</div><div class="length-num">18</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="../index.html"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="../archives/"><i class="fa-fw fas fa-archive"></i><span> 存档</span></a></div><div class="menus_item"><a class="site-page" href="../tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="../categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="../link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="../about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://images.pexels.com/photos/159866/books-book-pages-read-literature-159866.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=600')"><nav id="nav"><span id="blog_name"><a id="site-name" href="../index.html">小明的博客</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="../index.html"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="../archives/"><i class="fa-fw fas fa-archive"></i><span> 存档</span></a></div><div class="menus_item"><a class="site-page" href="../tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="../categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="../link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="../about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">如何使用机器学习神器sklearn做特征工程？</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-03-26T02:34:00.000Z" title="发表于 2022-03-26 10:34:00">2022-03-26</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-07-09T02:00:34.666Z" title="更新于 2022-07-09 10:00:34">2022-07-09</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="../categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="../categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BC%96%E7%A8%8B/">编程</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="如何使用机器学习神器sklearn做特征工程？"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h3 id="使用-sklearn-做特征工程"><a href="#使用-sklearn-做特征工程" class="headerlink" title="使用 sklearn 做特征工程"></a>使用 sklearn 做特征工程</h3><p><img src="../md_imgs/如何使用机器学习神器sklearn做特征工程？/1657254137937.png" alt="1657254137937"></p>
<h2 id="特征工程是什么？"><a href="#特征工程是什么？" class="headerlink" title="特征工程是什么？"></a>特征工程是什么？</h2><p>有这么一句话在业界广泛流传：数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。那特征工程到底是什么呢？顾名思义，其本质是一项工程活动，目的是最大限度地从原始数据中提取特征以供算法和模型使用。</p>
<p>通过总结和归纳，人们认为特征工程包括以下方面：</p>
<p><img src="../md_imgs/如何使用机器学习神器sklearn做特征工程？/1657254161385.png" alt="1657254161385"></p>
<p>特征处理是特征工程的核心部分，sklearn 提供了较为完整的特征处理方法，包括数据预处理，特征选择，降维等。首次接触到 sklearn，通常会被其丰富且方便的算法模型库吸引，但是这里介绍的特征处理库也十分强大！</p>
<p>本文中使用 sklearn 中的 IRIS（鸢尾花）数据集[1]来对特征处理功能进行说明。IRIS 数据集由 Fisher 在 1936 年整理，包含 4 个特征（Sepal.Length（花萼长度）、Sepal.Width（花萼宽度）、Petal.Length（花瓣长度）、Petal.Width（花瓣宽度）），特征值都为正浮点数，单位为厘米。目标值为鸢尾花的分类（Iris Setosa（山鸢尾）、Iris Versicolour（杂色鸢尾），Iris Virginica（维吉尼亚鸢尾））。导入 IRIS 数据集的代码如下：</p>
<pre class="line-numbers language-none"><code class="language-none"> 1 from sklearn.datasets import load_iris   
 2   
 3 #导入IRIS数据集  
 4 iris &#x3D; load_iris()   
 5   
 6 #特征矩阵  
 7 iris.data  
 8   
 9 #目标向量  
10 iris.target<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>通过特征提取，我们能得到未经处理的特征，这时的特征可能有以下问题：</p>
<ul>
<li>不属于同一量纲：</li>
</ul>
<p>即特征的规格不一样，不能够放在一起比较。无量纲化可以解决这一问题。</p>
<ul>
<li>信息冗余：</li>
</ul>
<p>对于某些定量特征，其包含的有效信息为区间划分，例如学习成绩，假若只关心 “及格” 或不 “及格”，那么需要将定量的考分，转换成“1” 和“0”表示及格和未及格。二值化可以解决这一问题。</p>
<ul>
<li>定性特征不能直接使用：</li>
</ul>
<p>某些机器学习算法和模型只能接受定量特征的输入，那么需要将定性特征转换为定量特征。最简单的方式是为每一种定性值指定一个定量值，但是这种方式过于灵活，增加了调参的工作。通常使用哑编码的方式将定性特征转换为定量特征[2]：假设有 N 种定性值，则将这一个特征扩展为 N 种特征，当原始特征值为第 i 种定性值时，第 i 个扩展特征赋值为 1，其他扩展特征赋值为 0。哑编码的方式相比直接指定的方式，不用增加调参的工作，对于线性模型来说，使用哑编码后的特征可达到非线性的效果。</p>
<ul>
<li>存在缺失值：</li>
</ul>
<p>缺失值需要补充。</p>
<ul>
<li>信息利用率低：</li>
</ul>
<p>不同的机器学习算法和模型对数据中信息的利用是不同的，之前提到在线性模型中，使用对定性特征哑编码可以达到非线性的效果。类似地，对定量变量多项式化，或者进行其他的转换，都能达到非线性的效果。</p>
<p>我们使用 sklearn 中的 preproccessing 库来进行数据预处理，可以覆盖以上问题的解决方案。</p>
<h3 id="2-1-无量纲化"><a href="#2-1-无量纲化" class="headerlink" title="2.1 无量纲化"></a>2.1 无量纲化</h3><p>无量纲化使不同规格的数据转换到同一规格。常见的无量纲化方法有标准化和区间缩放法。标准化的前提是特征值服从正态分布，标准化后，其转换成标准正态分布。区间缩放法利用了边界值信息，将特征的取值区间缩放到某个特点的范围，例如[0, 1]等。</p>
<h4 id="2-1-1-标准化"><a href="#2-1-1-标准化" class="headerlink" title="2.1.1 标准化"></a>2.1.1 标准化</h4><p>标准化需要计算特征的均值和标准差，公式表达为：</p>
<p>使用 preproccessing 库的 StandardScaler 类对数据进行标准化的代码如下：</p>
<pre class="line-numbers language-none"><code class="language-none">1 from sklearn.preprocessing import StandardScaler   
2   
3 #标准化，返回值为标准化后的数据  
4 StandardScaler().fit_transform(iris.data)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="2-1-2-区间缩放法"><a href="#2-1-2-区间缩放法" class="headerlink" title="2.1.2 区间缩放法"></a>2.1.2 区间缩放法</h4><p>区间缩放法的思路有多种，常见的一种为利用两个最值进行缩放，公式表达为：</p>
<p>使用 preproccessing 库的 MinMaxScaler 类对数据进行区间缩放的代码如下：</p>
<pre class="line-numbers language-none"><code class="language-none">1 from sklearn.preprocessing import MinMaxScaler   
2   
3 #区间缩放，返回值为缩放到[0, 1]区间的数据  
4 MinMaxScaler().fit_transform(iris.data)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="2-1-3-标准化与归一化的区别"><a href="#2-1-3-标准化与归一化的区别" class="headerlink" title="2.1.3 标准化与归一化的区别"></a>2.1.3 标准化与归一化的区别</h4><p>简单来说，标准化是依照特征矩阵的列处理数据，其通过求 z-score 的方法，将样本的特征值转换到同一量纲下。归一化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为 “单位向量”。规则为 l2 的归一化公式如下：</p>
<p>使用 preproccessing 库的 Normalizer 类对数据进行归一化的代码如下：</p>
<pre class="line-numbers language-none"><code class="language-none">1 from sklearn.preprocessing import Normalizer   
2   
3 #归一化，返回值为归一化后的数据  
4 Normalizer().fit_transform(iris.data)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="2-2-对定量特征二值化"><a href="#2-2-对定量特征二值化" class="headerlink" title="2.2 对定量特征二值化"></a>2.2 对定量特征二值化</h3><p>定量特征二值化的核心在于设定一个阈值，大于阈值的赋值为 1，小于等于阈值的赋值为 0，公式表达如下：</p>
<p><img src="../md_imgs/如何使用机器学习神器sklearn做特征工程？/1657254250324.png" alt="1657254250324"></p>
<p>使用 preproccessing 库的 Binarizer 类对数据进行二值化的代码如下：</p>
<pre class="line-numbers language-none"><code class="language-none">1 from sklearn.preprocessing import Binarizer   
2   
3 #二值化，阈值设置为3，返回值为二值化后的数据  
4 Binarizer(threshold&#x3D;3).fit_transform(iris.data)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="2-3-对定性特征哑编码"><a href="#2-3-对定性特征哑编码" class="headerlink" title="2.3 对定性特征哑编码"></a>2.3 对定性特征哑编码</h3><p>由于 IRIS 数据集的特征皆为定量特征，故使用其目标值进行哑编码（实际上是不需要的）。使用 preproccessing 库的 OneHotEncoder 类对数据进行哑编码的代码如下：</p>
<pre class="line-numbers language-none"><code class="language-none">1 from sklearn.preprocessing import OneHotEncoder   
2   
3 #哑编码，对IRIS数据集的目标值，返回值为哑编码后的数据  
4 OneHotEncoder().fit_transform(iris.target.reshape((-1,1)))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="2-4-缺失值计算"><a href="#2-4-缺失值计算" class="headerlink" title="2.4 缺失值计算"></a>2.4 缺失值计算</h3><p>由于 IRIS 数据集没有缺失值，故对数据集新增一个样本，4 个特征均赋值为 NaN，表示数据缺失。使用 preproccessing 库的 Imputer 类对数据进行缺失值计算的代码如下：</p>
<pre class="line-numbers language-none"><code class="language-none">1 from numpy import vstack, array, nan 2 from sklearn.preprocessing import Imputer   
3   
4 #缺失值计算，返回值为计算缺失值后的数据  
5 #参数missing_value为缺失值的表示形式，默认为NaN  
6 #参数strategy为缺失值填充方式，默认为mean（均值）  
7 Imputer().fit_transform(vstack((array([nan, nan, nan, nan]), iris.data)))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="2-5-数据变换"><a href="#2-5-数据变换" class="headerlink" title="2.5 数据变换"></a>2.5 数据变换</h3><p>常见的数据变换有基于多项式的、基于指数函数的、基于对数函数的。4 个特征，度为 2 的多项式转换公式如下：</p>
<p><img src="../md_imgs/如何使用机器学习神器sklearn做特征工程？/1657254270449.png" alt="1657254270449"></p>
<p>使用 preproccessing 库的 PolynomialFeatures 类对数据进行多项式转换的代码如下：</p>
<pre class="line-numbers language-none"><code class="language-none">1 from sklearn.preprocessing import PolynomialFeatures   
2   
3 #多项式转换  
4 #参数degree为度，默认值为2  
5 PolynomialFeatures().fit_transform(iris.data)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>基于单变元函数的数据变换可以使用一个统一的方式完成，使用 preproccessing 库的 FunctionTransformer 对数据进行对数函数转换的代码如下：</p>
<pre class="line-numbers language-none"><code class="language-none">1 from numpy import log1p 2 from sklearn.preprocessing import FunctionTransformer 3   
4 #自定义转换函数为对数函数的数据变换  
5 #第一个参数是单变元函数  
6 FunctionTransformer(log1p).fit_transform(iris.data)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="2-6-回顾"><a href="#2-6-回顾" class="headerlink" title="2.6 回顾"></a>2.6 回顾</h3><p>| 类 | 功能 | 说明 |</p>
<p>| StandardScaler | 无量纲化 | 标准化，基于特征矩阵的列，将特征值转换至服从标准正态分布 |</p>
<p>| MinMaxScaler | 无量纲化 | 区间缩放，基于最大最小值，将特征值转换到[0, 1]区间上 |</p>
<p>| Normalizer | 归一化 | 基于特征矩阵的行，将样本向量转换为 “单位向量” |</p>
<p>| Binarizer | 二值化 | 基于给定阈值，将定量特征按阈值划分 |</p>
<p>| OneHotEncoder | 哑编码 | 将定性数据编码为定量数据 |</p>
<p>| Imputer | 缺失值计算 | 计算缺失值，缺失值可填充为均值等 |</p>
<p>| PolynomialFeatures | 多项式数据转换 | 多项式数据转换 |</p>
<p>| FunctionTransformer | 自定义单元数据转换 | 使用单变元的函数来转换数据 |</p>
<h2 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h2><p>当数据预处理完成后，我们需要选择有意义的特征输入机器学习的算法和模型进行训练。通常来说，从两个方面考虑来选择特征：</p>
<ul>
<li>特征是否发散：</li>
</ul>
<p>如果一个特征不发散，例如方差接近于 0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。</p>
<ul>
<li>特征与目标的相关性：</li>
</ul>
<p>这点比较显见，与目标相关性高的特征，应当优选选择。除方差法外，本文介绍的其他方法均从相关性考虑。</p>
<p>根据特征选择的形式又可以将特征选择方法分为 3 种：</p>
<ul>
<li>Filter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。</li>
<li>Wrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。</li>
<li>Embedded：嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于 Filter 方法，但是是通过训练来确定特征的优劣。</li>
</ul>
<p>我们使用 sklearn 中的 feature_selection 库来进行特征选择。</p>
<h3 id="3-1-Filter"><a href="#3-1-Filter" class="headerlink" title="3.1 Filter"></a>3.1 Filter</h3><h4 id="3-1-1-方差选择法"><a href="#3-1-1-方差选择法" class="headerlink" title="3.1.1 方差选择法"></a>3.1.1 方差选择法</h4><p>使用方差选择法，先要计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征。使用 feature_selection 库的 VarianceThreshold 类来选择特征的代码如下：</p>
<pre class="line-numbers language-none"><code class="language-none">1 from sklearn.feature_selection import VarianceThreshold   
2   
3 #方差选择法，返回值为特征选择后的数据  
4 #参数threshold为方差的阈值  
5 VarianceThreshold(threshold&#x3D;3).fit_transform(iris.data)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="3-1-2-相关系数法"><a href="#3-1-2-相关系数法" class="headerlink" title="3.1.2 相关系数法"></a>3.1.2 相关系数法</h4><p>使用相关系数法，先要计算各个特征对目标值的相关系数以及相关系数的 P 值。用 feature_selection 库的 SelectKBest 类结合相关系数来选择特征的代码如下：</p>
<pre class="line-numbers language-none"><code class="language-none">1 from sklearn.feature_selection import SelectKBest   
2 from scipy.stats import pearsonr   
3   
4 #选择K个最好的特征，返回选择特征后的数据  
5 #第一个参数为计算评估特征是否好的函数，该函数输入特征矩阵和目标向量，输出二元组（评分，P值）的数组，数组第i项为第i个特征的评分和P值。在此定义为计算相关系数  
6 #参数k为选择的特征个数  
7 SelectKBest(lambda X, Y: array(map(lambda x:pearsonr(x, Y), X.T)).T, k&#x3D;2).fit_transform(iris.data, iris.target)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="3-1-3-卡方检验"><a href="#3-1-3-卡方检验" class="headerlink" title="3.1.3 卡方检验"></a>3.1.3 卡方检验</h4><p>经典的卡方检验是检验定性自变量对定性因变量的相关性。假设自变量有 N 种取值，因变量有 M 种取值，考虑自变量等于 i 且因变量等于 j 的样本频数的观察值与期望的差距，构建统计量：</p>
<script type="math/tex; mode=display">
\chi^{2}=\sum \frac{(A-E)^{2}}{E}</script><p>这个统计量的含义简而言之就是自变量对因变量的相关性。用 feature_selection 库的 SelectKBest 类结合卡方检验来选择特征的代码如下：</p>
<pre class="line-numbers language-none"><code class="language-none">1 from sklearn.feature_selection import SelectKBest   
2 from sklearn.feature_selection import chi2   
3   
4 #选择K个最好的特征，返回选择特征后的数据  
5 SelectKBest(chi2, k&#x3D;2).fit_transform(iris.data, iris.target)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="3-1-4-互信息法"><a href="#3-1-4-互信息法" class="headerlink" title="3.1.4 互信息法"></a>3.1.4 互信息法</h4><p>经典的互信息也是评价定性自变量对定性因变量的相关性的，互信息计算公式如下：</p>
<p><img src="../md_imgs/如何使用机器学习神器sklearn做特征工程？/1657254299652.png" alt="1657254299652"></p>
<p>为了处理定量数据，最大信息系数法被提出，使用 feature_selection 库的 SelectKBest 类结合最大信息系数法来选择特征的代码如下：</p>
<pre class="line-numbers language-none"><code class="language-none"> 1 from sklearn.feature_selection import SelectKBest   
 2 from minepy import MINE   
 3   
 4 #由于MINE的设计不是函数式的，定义mic方法将其为函数式的，返回一个二元组，二元组的第2项设置成固定的P值0.5  
 5 def mic(x, y):   
 6     m &#x3D; MINE()   
 7     m.compute_score(x, y)  
 8     return (m.mic(), 0.5)  
 9   
10 #选择K个最好的特征，返回特征选择后的数据  
11 SelectKBest(lambda X, Y: array(map(lambda x:mic(x, Y), X.T)).T, k&#x3D;2).fit_transform(iris.data, iris.target)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="3-2-Wrapper"><a href="#3-2-Wrapper" class="headerlink" title="3.2 Wrapper"></a>3.2 Wrapper</h3><h4 id="3-2-1-递归特征消除法"><a href="#3-2-1-递归特征消除法" class="headerlink" title="3.2.1 递归特征消除法"></a>3.2.1 递归特征消除法</h4><p>递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。使用 feature_selection 库的 RFE 类来选择特征的代码如下：</p>
<pre class="line-numbers language-none"><code class="language-none">1 from sklearn.feature_selection import RFE   
2 from sklearn.linear_model import LogisticRegression   
3   
4 #递归特征消除法，返回特征选择后的数据  
5 #参数estimator为基模型  
6 #参数n_features_to_select为选择的特征个数  
7 RFE(estimator&#x3D;LogisticRegression(), n_features_to_select&#x3D;2).fit_transform(iris.data, iris.target)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="3-3-Embedded"><a href="#3-3-Embedded" class="headerlink" title="3.3 Embedded"></a>3.3 Embedded</h3><h4 id="3-3-1-基于惩罚项的特征选择法"><a href="#3-3-1-基于惩罚项的特征选择法" class="headerlink" title="3.3.1 基于惩罚项的特征选择法"></a>3.3.1 基于惩罚项的特征选择法</h4><p>使用带惩罚项的基模型，除了筛选出特征外，同时也进行了降维。使用 feature_selection 库的 SelectFromModel 类结合带 L1 惩罚项的逻辑回归模型，来选择特征的代码如下：</p>
<pre class="line-numbers language-none"><code class="language-none">1 from sklearn.feature_selection import SelectFromModel   
2 from sklearn.linear_model import LogisticRegression   
3   
4 #带L1惩罚项的逻辑回归作为基模型的特征选择  
5 SelectFromModel(LogisticRegression(penalty&#x3D;&quot;l1&quot;, C&#x3D;0.1)).fit_transform(iris.data, iris.target)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>L1 惩罚项降维的原理在于保留多个对目标值具有同等相关性的特征中的一个[3]，所以没选到的特征不代表不重要。故，可结合 L2 惩罚项来优化。具体操作为：若一个特征在 L1 中的权值为 1，选择在 L2 中权值差别不大且在 L1 中权值为 0 的特征构成同类集合，将这一集合中的特征平分 L1 中的权值，故需要构建一个新的逻辑回归模型：</p>
<pre class="line-numbers language-none"><code class="language-none"> 1 from sklearn.linear_model import LogisticRegression   
 2   
 3 class LR(LogisticRegression):   
 4     def __init__(self, threshold&#x3D;0.01, dual&#x3D;False, tol&#x3D;1e-4, C&#x3D;1.0,  
 5                  fit_intercept&#x3D;True, intercept_scaling&#x3D;1, class_weight&#x3D;None,  
 6                  random_state&#x3D;None, solver&#x3D;&#39;liblinear&#39;, max_iter&#x3D;100,  
 7                  multi_class&#x3D;&#39;ovr&#39;, verbose&#x3D;0, warm_start&#x3D;False, n_jobs&#x3D;1):  
 8   
 9         #权值相近的阈值  
10         self.threshold &#x3D; threshold   
11         LogisticRegression.__init__(self, penalty&#x3D;&#39;l1&#39;, dual&#x3D;dual, tol&#x3D;tol, C&#x3D;C,   
12                  fit_intercept&#x3D;fit_intercept, intercept_scaling&#x3D;intercept_scaling, class_weight&#x3D;class_weight,   
13                  random_state&#x3D;random_state, solver&#x3D;solver, max_iter&#x3D;max_iter,   
14                  multi_class&#x3D;multi_class, verbose&#x3D;verbose, warm_start&#x3D;warm_start, n_jobs&#x3D;n_jobs)   
15         #使用同样的参数创建L2逻辑回归  
16         self.l2 &#x3D; LogisticRegression(penalty&#x3D;&#39;l2&#39;, dual&#x3D;dual, tol&#x3D;tol, C&#x3D;C, fit_intercept&#x3D;fit_intercept, intercept_scaling&#x3D;intercept_scaling, class_weight &#x3D; class_weight, random_state&#x3D;random_state, solver&#x3D;solver, max_iter&#x3D;max_iter, multi_class&#x3D;multi_class, verbose&#x3D;verbose, warm_start&#x3D;warm_start, n_jobs&#x3D;n_jobs)   
17   
18     def fit(self, X, y, sample_weight&#x3D;None):   
19         #训练L1逻辑回归  
20         super(LR, self).fit(X, y, sample_weight&#x3D;sample_weight) 21         self.coef_old_ &#x3D; self.coef_.copy()   
22         #训练L2逻辑回归  
23         self.l2.fit(X, y, sample_weight&#x3D;sample_weight)   
24   
25         cntOfRow, cntOfCol &#x3D; self.coef_.shape   
26         #权值系数矩阵的行数对应目标值的种类数目  
27         for i in range(cntOfRow): 28             for j in range(cntOfCol):   
29                 coef &#x3D; self.coef_[i][j]   
30                 #L1逻辑回归的权值系数不为0  
31                 if coef \!&#x3D; 0:   
32                     idx &#x3D; [j]   
33                     #对应在L2逻辑回归中的权值系数  
34                     coef1 &#x3D; self.l2.coef_[i][j]   
35                     for k in range(cntOfCol):   
36                         coef2 &#x3D; self.l2.coef_[i][k]   
37                         #在L2逻辑回归中，权值系数之差小于设定的阈值，且在L1中对应的权值为0  
38                         if abs(coef1-coef2) \&lt; self.threshold and j \!&#x3D; k and self.coef_[i][k] &#x3D;&#x3D; 0:   
39 idx.append(k)   
40                     #计算这一类特征的权值系数均值  
41                     mean &#x3D; coef &#x2F; len(idx)   
42                     self.coef_[i][idx] &#x3D; mean   
43         return self<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>使用 feature_selection 库的 SelectFromModel 类结合带 L1 以及 L2 惩罚项的逻辑回归模型，来选择特征的代码如下：</p>
<pre class="line-numbers language-none"><code class="language-none">1 from sklearn.feature_selection import SelectFromModel   
2   
3 #带L1和L2惩罚项的逻辑回归作为基模型的特征选择  
4 #参数threshold为权值系数之差的阈值  
5 SelectFromModel(LR(threshold&#x3D;0.5, C&#x3D;0.1)).fit_transform(iris.data, iris.target)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="3-3-2-基于树模型的特征选择法"><a href="#3-3-2-基于树模型的特征选择法" class="headerlink" title="3.3.2 基于树模型的特征选择法"></a>3.3.2 基于树模型的特征选择法</h4><p>树模型中 GBDT 也可用来作为基模型进行特征选择，使用 feature_selection 库的 SelectFromModel 类结合 GBDT 模型，来选择特征的代码如下：</p>
<pre class="line-numbers language-none"><code class="language-none">1 from sklearn.feature_selection import SelectFromModel 2 from sklearn.ensemble import GradientBoostingClassifier 3   
4 #GBDT作为基模型的特征选择  
5 SelectFromModel(GradientBoostingClassifier()).fit_transform(iris.data, iris.target)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<h3 id="3-4-回顾"><a href="#3-4-回顾" class="headerlink" title="3.4 回顾"></a>3.4 回顾</h3><p>| 类 | 所属方式 | 说明 |</p>
<p>| VarianceThreshold | Filter | 方差选择法 |</p>
<p>| SelectKBest | Filter | 可选关联系数、卡方校验、最大信息系数作为得分计算的方法 |</p>
<p>| RFE | Wrapper | 递归地训练基模型，将权值系数较小的特征从特征集合中消除 |</p>
<p>| SelectFromModel | Embedded | 训练基模型，选择权值系数较高的特征 |</p>
<h2 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h2><p>当特征选择完成后，可以直接训练模型了，但是可能由于特征矩阵过大，导致计算量大，训练时间长的问题，因此降低特征矩阵维度也是必不可少的。常见的降维方法除了以上提到的基于 L1 惩罚项的模型以外，另外还有主成分分析法（PCA）和线性判别分析（LDA），线性判别分析本身也是一个分类模型。PCA 和 LDA 有很多的相似点，其本质是要将原始的样本映射到维度更低的样本空间中，但是 PCA 和 LDA 的映射目标不一样：PCA 是为了让映射后的样本具有最大的发散性；而 LDA 是为了让映射后的样本有最好的分类性能[4]。所以说 PCA 是一种无监督的降维方法，而 LDA 是一种有监督的降维方法。</p>
<h3 id="4-1-主成分分析法（PCA）"><a href="#4-1-主成分分析法（PCA）" class="headerlink" title="4.1 主成分分析法（PCA）"></a>4.1 主成分分析法（PCA）</h3><p>使用 decomposition 库的 PCA 类选择特征的代码如下：</p>
<pre class="line-numbers language-none"><code class="language-none">1 from sklearn.decomposition import PCA 2   
3 #主成分分析法，返回降维后的数据  
4 #参数n_components为主成分数目  
5 PCA(n_components&#x3D;2).fit_transform(iris.data)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="4-2-线性判别分析法（LDA）"><a href="#4-2-线性判别分析法（LDA）" class="headerlink" title="4.2 线性判别分析法（LDA）"></a>4.2 线性判别分析法（LDA）</h3><p>使用 lda 库的 LDA 类选择特征的代码如下：</p>
<pre class="line-numbers language-none"><code class="language-none">1 from sklearn.lda import LDA   
2   
3 #线性判别分析法，返回降维后的数据  
4 #参数n_components为降维后的维数  
5 LDA(n_components&#x3D;2).fit_transform(iris.data, iris.target)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="4-3-回顾"><a href="#4-3-回顾" class="headerlink" title="4.3 回顾"></a>4.3 回顾</h3><p>| 库 | 类 | 说明 |</p>
<p>| decomposition | PCA | 主成分分析法 |</p>
<p>| lda | LDA | 线性判别分析法 |</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>再让我们回归一下本文开始的特征工程的思维导图，我们可以使用 sklearn 完成几乎所有特征处理的工作，而且不管是数据预处理，还是特征选择，抑或降维，它们都是通过某个类的方法 fit_transform 完成的，fit_transform 要不只带一个参数：特征矩阵，要不带两个参数：特征矩阵加目标向量。这些难道都是巧合吗？还是故意设计成这样？方法 fit_transform 中有 fit 这一单词，它和训练模型的 fit 方法有关联吗？接下来，我将在《使用 sklearn 优雅地进行数据挖掘》[5]中阐述其中的奥妙！</p>
<p>参考资料</p>
<p>[1]</p>
<p>IRIS（鸢尾花）数据集: _<a target="_blank" rel="noopener" href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html#sklearn.datasets.load_iris">http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html#sklearn.datasets.load_iris</a>_</p>
<p>[2]</p>
<p>通常使用哑编码的方式将定性特征转换为定量特征: _<a target="_blank" rel="noopener" href="http://www.ats.ucla.edu/stat/mult_pkg/faq/general/dummy.htm">http://www.ats.ucla.edu/stat/mult_pkg/faq/general/dummy.htm</a>_</p>
<p>[3]</p>
<p>L1 惩罚项降维的原理在于保留多个对目标值具有同等相关性的特征中的一个: _<a target="_blank" rel="noopener" href="http://www.zhihu.com/question/28641663/answer/41653367">http://www.zhihu.com/question/28641663/answer/41653367</a>_</p>
<p>[4]</p>
<p>PCA 是为了让映射后的样本具有最大的发散性；而 LDA 是为了让映射后的样本有最好的分类性能: _<a target="_blank" rel="noopener" href="http://www.cnblogs.com/LeftNotEasy/archive/2011/01/08/lda-and-pca-machine-learning.html">http://www.cnblogs.com/LeftNotEasy/archive/2011/01/08/lda-and-pca-machine-learning.html</a>_</p>
<p>[5]</p>
<p>《使用 sklearn 优雅地进行数据挖掘》: _<a target="_blank" rel="noopener" href="http://www.cnblogs.com/jasonfreak/p/5448462.html">http://www.cnblogs.com/jasonfreak/p/5448462.html</a>_</p>
<p>[6]</p>
<p>FAQ: What is dummy coding?: _<a target="_blank" rel="noopener" href="http://www.ats.ucla.edu/stat/mult_pkg/faq/general/dummy.htm">http://www.ats.ucla.edu/stat/mult_pkg/faq/general/dummy.htm</a>_</p>
<p>[7]</p>
<p>IRIS（鸢尾花）数据集: _<a target="_blank" rel="noopener" href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html#sklearn.datasets.load_iris">http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html#sklearn.datasets.load_iris</a>_</p>
<p>[8]</p>
<p>卡方检验: _<a target="_blank" rel="noopener" href="http://wiki.mbalib.com/wiki/%E5%8D%A1%E6%96%B9%E6%A3%80%E9%AA%8C">http://wiki.mbalib.com/wiki/%E5%8D%A1%E6%96%B9%E6%A3%80%E9%AA%8C</a>_</p>
<p>[9]</p>
<p>干货：结合 Scikit-learn 介绍几种常用的特征选择方法: _<a target="_blank" rel="noopener" href="http://dataunion.org/14072.html">http://dataunion.org/14072.html</a>_</p>
<p>[10]</p>
<p>机器学习中，有哪些特征选择的工程方法？: _<a target="_blank" rel="noopener" href="http://www.zhihu.com/question/28641663/answer/41653367">http://www.zhihu.com/question/28641663/answer/41653367</a>_</p>
<p>[11]</p>
<p>机器学习中的数学 (4)- 线性判别分析（LDA）, 主成分分析 (PCA): _<a target="_blank" rel="noopener" href="http://www.cnblogs.com/LeftNotEasy/archive/2011/01/08/lda-and-pca-machine-learning.html">http://www.cnblogs.com/LeftNotEasy/archive/2011/01/08/lda-and-pca-machine-learning.html</a>_</p>
<blockquote>
<p>作者：jasonfreak</p>
<p>链接：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/jasonfreak/p/5448385.html">https://www.cnblogs.com/jasonfreak/p/5448385.html</a></p>
</blockquote>
<p><strong>·**</strong>推荐阅读 <strong>**·</strong></p>
<p><strong><a target="_blank" rel="noopener" href="http://mp.weixin.qq.com/s?__biz=MzA3ODYwNDkzOQ==&amp;mid=2659068847&amp;idx=1&amp;sn=8db0be0906a3509f375375106b0db1bb&amp;chksm=84cab44bb3bd3d5de009be958a3a03870d6db95bf574cbfd084eca8888ecbe968c2176ec76bf&amp;scene=21#wechat_redirect">盘点 2021 最佳数据可视化作品</a></strong></p>
<p><strong><a target="_blank" rel="noopener" href="http://mp.weixin.qq.com/s?__biz=MzA3ODYwNDkzOQ==&amp;mid=2659069034&amp;idx=1&amp;sn=35c1f7ca00e4f7f0aef99f9d7472a98b&amp;chksm=84cab38eb3bd3a98ce378cde1074aad800b95dfa7d95a19cc327d43803216466553b4c112c86&amp;scene=21#wechat_redirect">「Python 实用秘技 04」pdf 文件批量添加文字水印</a></strong></p>
<p><strong><a target="_blank" rel="noopener" href="http://mp.weixin.qq.com/s?__biz=MzA3ODYwNDkzOQ==&amp;mid=2659069201&amp;idx=1&amp;sn=35d452d34f933824a8b726d5e5d49166&amp;chksm=84cab2f5b3bd3be354179dea529702cc85fb797002d5375ccfe984c6f618d115bf0bc24694b0&amp;scene=21#wechat_redirect">新一代 Python 包管理工具来了</a></strong></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://kebuaaa.github.io">可不</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://kebuaaa.github.io/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%A5%9E%E5%99%A8sklearn%E5%81%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%EF%BC%9F/">https://kebuaaa.github.io/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%A5%9E%E5%99%A8sklearn%E5%81%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%EF%BC%9F/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kebuaaa.github.io" target="_blank">小明的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="../tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BC%96%E7%A8%8B/">机器学习编程</a><a class="post-meta__tags" href="../tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/">特征工程</a></div><div class="post_share"><div class="social-share" data-image="https://images.pexels.com/photos/159866/books-book-pages-read-literature-159866.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=600" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="../%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98%EF%BC%88SVM%EF%BC%89/"><img class="prev-cover" src="https://images.pexels.com/photos/768125/pexels-photo-768125.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=600" onerror="onerror=null;src='https://images.pexels.com/photos/374918/pexels-photo-374918.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=600'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">对偶问题（SVM）</div></div></a></div><div class="next-post pull-right"><a href="../%E5%A4%8D%E5%88%B6%E4%B8%8E%E6%8B%B7%E8%B4%9D/"><img class="next-cover" src="https://images.pexels.com/photos/768125/pexels-photo-768125.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=600" onerror="onerror=null;src='https://images.pexels.com/photos/374918/pexels-photo-374918.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=600'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">复制与拷贝</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="../Tensorflow%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/" title="Tensorflow深度学习框架"><img class="cover" src="https://images.pexels.com/photos/768125/pexels-photo-768125.jpeg?auto=compress&cs=tinysrgb&w=600" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-26</div><div class="title">Tensorflow深度学习框架</div></div></a></div><div><a href="../scikit-learn%E5%BA%93/" title="scikit-learn库"><img class="cover" src="https://images.pexels.com/photos/159866/books-book-pages-read-literature-159866.jpeg?auto=compress&cs=tinysrgb&w=600" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-26</div><div class="title">scikit-learn库</div></div></a></div><div><a href="../%E4%BA%A7%E7%94%9F%E5%92%8C%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86/" title="产生和加载数据集"><img class="cover" src="https://images.pexels.com/photos/768125/pexels-photo-768125.jpeg?auto=compress&cs=tinysrgb&w=600" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-26</div><div class="title">产生和加载数据集</div></div></a></div><div><a href="../%E5%86%B3%E7%AD%96%E6%A0%91/" title="决策树"><img class="cover" src="https://images.pexels.com/photos/317355/pexels-photo-317355.jpeg?auto=compress&cs=tinysrgb&w=600" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-26</div><div class="title">决策树</div></div></a></div><div><a href="../%E5%9B%9E%E5%BD%92/" title="回归"><img class="cover" src="https://images.pexels.com/photos/733854/pexels-photo-733854.jpeg?auto=compress&cs=tinysrgb&w=600" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-26</div><div class="title">回归</div></div></a></div><div><a href="../%E5%9B%BE%E5%83%8F/" title="图像"><img class="cover" src="https://images.pexels.com/photos/317355/pexels-photo-317355.jpeg?auto=compress&cs=tinysrgb&w=600" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-26</div><div class="title">图像</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-sklearn-%E5%81%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B"><span class="toc-number">1.</span> <span class="toc-text">使用 sklearn 做特征工程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number"></span> <span class="toc-text">特征工程是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number"></span> <span class="toc-text">数据预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E6%97%A0%E9%87%8F%E7%BA%B2%E5%8C%96"><span class="toc-number">1.</span> <span class="toc-text">2.1 无量纲化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-1-%E6%A0%87%E5%87%86%E5%8C%96"><span class="toc-number">1.1.</span> <span class="toc-text">2.1.1 标准化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-2-%E5%8C%BA%E9%97%B4%E7%BC%A9%E6%94%BE%E6%B3%95"><span class="toc-number">1.2.</span> <span class="toc-text">2.1.2 区间缩放法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-3-%E6%A0%87%E5%87%86%E5%8C%96%E4%B8%8E%E5%BD%92%E4%B8%80%E5%8C%96%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">1.3.</span> <span class="toc-text">2.1.3 标准化与归一化的区别</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E5%AF%B9%E5%AE%9A%E9%87%8F%E7%89%B9%E5%BE%81%E4%BA%8C%E5%80%BC%E5%8C%96"><span class="toc-number">2.</span> <span class="toc-text">2.2 对定量特征二值化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E5%AF%B9%E5%AE%9A%E6%80%A7%E7%89%B9%E5%BE%81%E5%93%91%E7%BC%96%E7%A0%81"><span class="toc-number">3.</span> <span class="toc-text">2.3 对定性特征哑编码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-%E7%BC%BA%E5%A4%B1%E5%80%BC%E8%AE%A1%E7%AE%97"><span class="toc-number">4.</span> <span class="toc-text">2.4 缺失值计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-%E6%95%B0%E6%8D%AE%E5%8F%98%E6%8D%A2"><span class="toc-number">5.</span> <span class="toc-text">2.5 数据变换</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-6-%E5%9B%9E%E9%A1%BE"><span class="toc-number">6.</span> <span class="toc-text">2.6 回顾</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9"><span class="toc-number"></span> <span class="toc-text">特征选择</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-Filter"><span class="toc-number">1.</span> <span class="toc-text">3.1 Filter</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-1-%E6%96%B9%E5%B7%AE%E9%80%89%E6%8B%A9%E6%B3%95"><span class="toc-number">1.1.</span> <span class="toc-text">3.1.1 方差选择法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-2-%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0%E6%B3%95"><span class="toc-number">1.2.</span> <span class="toc-text">3.1.2 相关系数法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-3-%E5%8D%A1%E6%96%B9%E6%A3%80%E9%AA%8C"><span class="toc-number">1.3.</span> <span class="toc-text">3.1.3 卡方检验</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-4-%E4%BA%92%E4%BF%A1%E6%81%AF%E6%B3%95"><span class="toc-number">1.4.</span> <span class="toc-text">3.1.4 互信息法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-Wrapper"><span class="toc-number">2.</span> <span class="toc-text">3.2 Wrapper</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-1-%E9%80%92%E5%BD%92%E7%89%B9%E5%BE%81%E6%B6%88%E9%99%A4%E6%B3%95"><span class="toc-number">2.1.</span> <span class="toc-text">3.2.1 递归特征消除法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-Embedded"><span class="toc-number">3.</span> <span class="toc-text">3.3 Embedded</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-1-%E5%9F%BA%E4%BA%8E%E6%83%A9%E7%BD%9A%E9%A1%B9%E7%9A%84%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E6%B3%95"><span class="toc-number">3.1.</span> <span class="toc-text">3.3.1 基于惩罚项的特征选择法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-2-%E5%9F%BA%E4%BA%8E%E6%A0%91%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E6%B3%95"><span class="toc-number">3.2.</span> <span class="toc-text">3.3.2 基于树模型的特征选择法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-%E5%9B%9E%E9%A1%BE"><span class="toc-number">4.</span> <span class="toc-text">3.4 回顾</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%99%8D%E7%BB%B4"><span class="toc-number"></span> <span class="toc-text">降维</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%E6%B3%95%EF%BC%88PCA%EF%BC%89"><span class="toc-number">1.</span> <span class="toc-text">4.1 主成分分析法（PCA）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90%E6%B3%95%EF%BC%88LDA%EF%BC%89"><span class="toc-number">2.</span> <span class="toc-text">4.2 线性判别分析法（LDA）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E5%9B%9E%E9%A1%BE"><span class="toc-number">3.</span> <span class="toc-text">4.3 回顾</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number"></span> <span class="toc-text">总结</span></a></div></div></div></div></main><footer id="footer" style="background-image: url('https://images.pexels.com/photos/159866/books-book-pages-read-literature-159866.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=600')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By 可不</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"></div><div id="rightside-config-show"><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="../js/utils.js"></script><script src="../js/main.js"></script><script src="../js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: '',
      appKey: '',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !true) {
  if (true) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>