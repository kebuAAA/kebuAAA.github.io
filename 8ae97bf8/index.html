<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>特征工程 | 小明的博客</title><meta name="keywords" content="特征工程"><meta name="author" content="爱编程的小明"><meta name="copyright" content="爱编程的小明"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#e68ab8"><meta name="description" content="机器学习中的特征工程到底是在说什么">
<meta property="og:type" content="article">
<meta property="og:title" content="特征工程">
<meta property="og:url" content="https://kobal.cn/8ae97bf8/index.html">
<meta property="og:site_name" content="小明的博客">
<meta property="og:description" content="机器学习中的特征工程到底是在说什么">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/top_img/10009.webp">
<meta property="article:published_time" content="2022-07-11T07:18:14.000Z">
<meta property="article:modified_time" content="2022-07-15T07:18:14.000Z">
<meta property="article:author" content="爱编程的小明">
<meta property="article:tag" content="特征工程">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/top_img/10009.webp"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://kobal.cn/8ae97bf8/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/font-awesome/6.0.0/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.staticfile.org/fancyapps-ui/4.0.27/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":1,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"prismjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://gcore.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://gcore.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '特征工程',
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2022-07-15 15:18:14'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#e68ab8')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><style>#article-container.post-content h1:before, h2:before, h3:before, h4:before, h5:before, h6:before { -webkit-animation: avatar_turn_around 1s linear infinite; -moz-animation: avatar_turn_around 1s linear infinite; -o-animation: avatar_turn_around 1s linear infinite; -ms-animation: avatar_turn_around 1s linear infinite; animation: avatar_turn_around 1s linear infinite; }</style><link rel="stylesheet" href="/Scripts/css/transparent.css"><link rel="stylesheet" href="/Scripts/css/font.css"><link rel="stylesheet" href="/Scripts/css/foot_style.css"><link rel="stylesheet" href="/Scripts/css/twikoo_beautify.css"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://unpkg.zhimg.com/hexo-butterfly-wowjs/lib/animate.min.css" media="print" onload="this.media='screen'"><link rel="stylesheet" href="/Scripts/css/cardlistpost.min.css"/>
<style>#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags:before {content:"\A";
  white-space: pre;}#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags > .article-meta__separator{display:none}</style>
<link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-tag-plugins@latest/lib/tag_plugins.min.css" media="defer" onload="this.media='all'"><script src="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-tag-plugins@latest/lib/mindmap.min.css" media="defer" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.2.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/avatar.webp" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">98</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">61</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">20</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 存档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page" href="/wallpaper/"><i class="fa-fw fas fa-image fa-fw"></i><span> 壁纸</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://travellings.link"><i class="fa-fw fa fa-subway"></i><span> 开往</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">小明的博客</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 存档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page" href="/wallpaper/"><i class="fa-fw fas fa-image fa-fw"></i><span> 壁纸</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://travellings.link"><i class="fa-fw fa fa-subway"></i><span> 开往</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">特征工程<a class="post-edit-link" href="https://github.dev/kebuAAA/myblog/blob/main/source/_posts/特征工程.md" title="编辑" target="_blank"><i class="fas fa-pencil-alt"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-07-11T07:18:14.000Z" title="发表于 2022-07-11 15:18:14">2022-07-11</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-07-15T07:18:14.000Z" title="更新于 2022-07-15 15:18:14">2022-07-15</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA/">基础理论</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="特征工程"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><blockquote>
<p>数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。</p>
</blockquote>
<p>特征工程本质是一项<strong>工程活动</strong>，目的是最大限度地从原始数据中提取特征以供算法和模型使用。一般来说包含以下几个方面的内容:<br>
<img src= "https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/1657524085385.png" alt="1657524085385"></p>
<h1 id="数据预处理">数据预处理</h1>
<p>一般来说，我们搜集到的数据都要经过一定处理来消除存在的一些问题才能用于模型的训练（原数据信息的冗余问题、量纲不同问题、缺失值问题），因此数据预处理工作是非常有必要的！</p>
<h2 id="缺失值处理">缺失值处理</h2>
<p>对缺失值需要进行插补以补全，才能作为机器学习模型训练的数据使用。<br>
常用的插补方法</p>
<ul>
<li>
<p>均值插补</p>
</li>
<li>
<p>同类均值插补</p>
</li>
<li>
<p>众数插补</p>
</li>
<li>
<p>建模预测：利用机器学习算法对数据集的缺失值进行预测</p>
</li>
<li>
<p>高维映射：将属性映射到高维空间，采用独热编码技术，将包含K个离散取值范围的属性值扩展为K+1个属性值，若该属性值确实，则将扩展后的第K+1个属性值设为1.这种方法较为精确，保留了所有信息，也未添加任何额外信息，但若预处理时所有的变量都这么处理会大大增加数据的维度。这样做的好处是完整保留了原始数据的全部信息，不用考虑缺失值；缺点是计算量大大提升且只有在样本量非常大的时候效果才好</p>
</li>
<li>
<p>多重插补：待插补的值是随机的，实践时通常是估计待插补的值，并叠加不同的噪声，形成多组可选插补值，</p>
</li>
</ul>
<p>插补处理只是将为知值以人们的主观估计值，不一定完全符合客观事实。在一些情况下，根据所在具体问题领域的理解，需要手动插补缺失值，插补的效果会更好。</p>
<h2 id="无量纲化">无量纲化</h2>
<h3 id="z-score标准化">Z-score标准化</h3>
<p>标准化是依照特征<strong>矩阵的列</strong>处理数据，其通过求 z-score 的方法，将样本的特征值转换到<strong>同一量纲</strong>下。<strong>前提是数据要服从正态分布</strong>！</p>
<h3 id="区间放缩法">区间放缩法</h3>
<p>一般利用最大最小值放缩到[0,1]区间</p>
<h3 id="归一化">归一化</h3>
<p>归一化是依照特征<strong>矩阵的行</strong>处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为 “<strong>单位向量</strong>”。归一化和单位化比较像。</p>
<blockquote>
<p>注意标准化与归一化的区别:<br>
简单来说，标准化是依照特征<strong>矩阵的列</strong>处理数据，其通过求 z-score 的方法，将样本的特征值转换到<strong>同一量纲</strong>下。<br>
归一化是依照特征<strong>矩阵的行</strong>处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为 “<strong>单位向量</strong>”。</p>
</blockquote>
<h3 id="对数标准化">对数标准化</h3>
<p><img src= "https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/1657525170113.png" alt="1657525170113"><br>
对于特定数据集，采用对数函数进行变换会取得更好的分析效果。优点：</p>
<ul>
<li>
<p>对数函数在其定义域内是单调增函数，取对数后不会改变数据的相对关系；</p>
</li>
<li>
<p>取对数能够<strong>缩小数据的绝对数值</strong>，方便计算；</p>
</li>
<li>
<p>取对数后，可以将乘法计算转换为加法计算；</p>
</li>
<li>
<p>某些情况下，在数据的整个值域中，不同区间带来的影响不同，对数函数自变量x的值越小，函数值y的变化越快，也就是说，<strong>对数值小的部分差异的敏感程度比数值大的部分的差异敏感程度更高</strong>；</p>
</li>
<li>
<p><strong>取对数之后不会改变数据的性质和相关关系，但压缩了变量的尺度，使得数据更加平稳，消弱了模型的共线性、异方差性等。</strong></p>
</li>
</ul>
<blockquote>
<p>由于三角函数中的反正切函数与对数函数具有相似的性质，也可以使用反正切函数实现数据的标准化转换。(<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>=</mo><mfrac><mrow><mn>2</mn><mi>arctan</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mi>π</mi></mfrac></mrow><annotation encoding="application/x-tex">x^{\prime}=\frac{2 \arctan (x)}{\pi}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.751892em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.355em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">π</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.485em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mspace mtight" style="margin-right:0.19516666666666668em;"></span><span class="mop mtight"><span class="mtight">a</span><span class="mtight">r</span><span class="mtight">c</span><span class="mtight">t</span><span class="mtight">a</span><span class="mtight">n</span></span><span class="mopen mtight">(</span><span class="mord mathdefault mtight">x</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>)</p>
</blockquote>
<h2 id="定量特征二值化">定量特征二值化</h2>
<p>特征二值化是将数值型的属性按照<strong>阈值</strong>转换为布尔值（1或0）的属性,主要目的是将特征的取值变到我们想要研究的问题上去。</p>
<h2 id="对定性特征哑编码">对定性特征哑编码</h2>
<p>为了使计算机能够有效地从数据集中进行机器学习，我们需要把数据库中的非数值型字段进行编码，但又不能简单地用数值来对分类属性值进行编码。例如将“中国”、“美国”、“英国”分别用1,2,3进行编码，机器学习的估计器将认为这些属性值是有序的。<br>
将分类特征转化为能够被机器学习模型使用的编码是one-of-K或one-hot编码，称为独热编码，又称一位有效编码。<br>
采用N位状态寄存器来对N个可能的取值进行编码，每个状态都由独立的寄存器来表示，并且在任意时刻只有其中一位有效。例如，例如对六个状态（即分类属性的6个值）进行编码：<br>
自然编码：000,001,010，……101<br>
独热编码：000001,000010,000100,001000,010000,100000.<br>
优点：可以处理非数值属性，一定程度上扩充了特征。</p>
<h2 id="数据变换">数据变换</h2>
<p>数据变换在我看来更像是一种特征探索的过程，相当于是在已有的特征基础上探究新的可能的特征。<br>
常见的数据变换有基于多项式的、基于指数函数的、基于对数函数的。4 个特征，度为 2 的多项式转换公式如下：<br>
<img src= "https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/1657524652513.png" alt="1657524652513"></p>
<h1 id="特征选择">特征选择</h1>
<p>特征选择的好处:更容易理解，减少训练周期和避免维度诅咒。</p>
<p>通常来说，从两个方面考虑来选择特征：</p>
<ul>
<li>特征是否发散：</li>
</ul>
<p>如果一个特征不发散，例如方差接近于 0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。</p>
<ul>
<li>特征与目标的相关性：</li>
</ul>
<p>这点比较显见，与目标相关性高的特征，应当优选选择。除方差法外，本文介绍的其他方法均从相关性考虑。</p>
<blockquote>
<p>数据本身的波动特征以及与目标的相关程度的问题<br>
根据特征选择的形式又可以将特征选择方法分为 3 种：</p>
</blockquote>
<h2 id="filter">Filter</h2>
<p>过滤法，按照<strong>发散性</strong>或者<strong>相关性</strong>对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。</p>
<h2 id="wrapper">Wrapper</h2>
<p>包装法，根据目标函数（通常是预测效果评分/学习器的性能），每次选择若干特征，或者排除若干特征。<br>
<strong>优点</strong>是直接针对特定学习器进行优化，因此通常包裹式特征选择比过滤式特征选择更好<br>
<strong>缺点</strong>是由于特征选择过程需要多次训练学习器，故计算开销要比过滤式特征选择要大得多。<br>
对学习器的评价准则</p>
<ul>
<li>
<p>距离度量：差异性或者分离性的度量，常用的距离度量方法有欧式距离等。</p>
</li>
<li>
<p>信息增益度量：特征f的信息增益定义为使用特征f的先验不确定性与期望的后验不确性之间的差异。若特征f1的信息增益大于特征f2的信息增益，则认为特征f1优于特征f2。</p>
</li>
<li>
<p>依赖性度量：又称为相关性度量，通常采用<strong>皮尔逊相关系数</strong>（Pearson Correlation Coefficient）来计算特征f与类别C之间的相关度，相关性好，则特征更优。属性之间的相关性越低越好。</p>
</li>
<li>
<p>一致性度量：一致性度量观察两个样本，若它们的特征值相同，且所属类别也相同，则认为它们是一致的。尝试找出与原始特征集具有一样辨别能力的最小的属性子集。</p>
</li>
<li>
<p>分类器错误率度量：分类器错误率度量使用学习器的性能作为最终的评价阈值。它倾向于选择那些在分类器上表现较好的子集。</p>
</li>
</ul>
<p>稳定性选择是一种基于二次抽样和选择算法相结合较新的方法，选择算法可以是回归、SVM或其他类似的方法。它的主要思想是在不同的数据子集和特征子集上运行特征选择算法，不断的重复，最终汇总特征选择结果，比如可以统计某个特征被认为是重要特征的频率（被选为重要特征的次数除以它所在的子集被测试的次数）。理想情况下，重要特征的得分会接近100%。稍微弱一点的特征得分会是非0的数，而最无用的特征得分将会接近于0。</p>
<h2 id="embedded">Embedded</h2>
<p>嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于 Filter 方法，但是是通过<strong>训练</strong>来确定特征的优劣。</p>
<blockquote>
<p>典型的嵌入式特征选择方法是决策树算法，如ID3，C4.5以及CART算法等。决策树算法在树增长过程的每个递归步都必须选择一个特征，将样本划分成较小的子集。选择特征的依据是划分后子节点的纯度。划分后子节点越纯，则说明划分效果越好。决策树生成的过程也就是特征选择的过程。</p>
</blockquote>
<blockquote>
<p>特征选择可以使用ITMO_FS，它是一个特征选择库，它可以为 ML 模型进行特征选择。拥有的观察值越少，就越需要谨慎处理过多的特征，以避免过度拟合。所谓“谨慎”意思是应该规范你的模型。通常一个更简单的模型（更少的特征），更容易理解和解释。</p>
</blockquote>
<h1 id="特征降维">特征降维</h1>
<p>随着数据的生成和数据收集量的不断增加，数据可视化、机器学习变得越来越困难，数据的存储、传输也变得异常困难。这不得不迫使人们开展<strong>数据压缩</strong>技术的研究。<br>
<strong>数据压缩</strong>技术可以帮助对数据进行存储和分析。<br>
特征降维将对数据集和机器学习带来如下好处：</p>
<ul>
<li>
<p>随着特征维度降低，数据存储所需的空间会随之减少；</p>
</li>
<li>
<p>低维数据有助于减少计算和机器学习训练用时；</p>
</li>
<li>
<p>一些算法在高维度数据上容易表现不佳，降维可提高算法可用性；</p>
</li>
<li>
<p>降维可以用删除冗余特征解决多重共线性问题；</p>
</li>
<li>
<p>降维有助于数据可视化。</p>
</li>
</ul>
<p>特征降维方法一般可分为线性降维和非线性降维两大类，非线性降维又分为基于核函数的方法和基于特征值的方法。降维算法有主成分分析、奇异值分解和线性判别分析，但需要清楚地知道想用哪种工具来寻找模式或从数据中推断出新的信息。其中主成分分析和线性判别分析都属于线性降维方法</p>
<h3 id="主成分分析-pca">主成分分析（PCA）</h3>
<p>PCA（Principal Component Analysis）主成分分析，提取数据集的<strong>主要特征</strong>成分，忽略次要特征成分，达到降维目的。<br>
PCA通过<strong>线性变换</strong>，将N维空间的原始数据变换到一个较低的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi></mrow><annotation encoding="application/x-tex">R</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.00773em;">R</span></span></span></span>维空间<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>R</mi><mo>&lt;</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">(R&lt;N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span></span></span></span>)，达到降维目的。<br>
在降维过程中，不可避免的要造成信息损失。如原来在高维空间可分的点，在低维空间可能变成一个点，变得不可分。因此，要在降维过程中尽量减少这种损失。<br>
<strong>为使样本投影到低维空间后尽可能分散</strong>，它们的方差要尽可能大。这就构成了PCA的基本思想。<br>
具体过程为选取一组N个R维的正交基组成的矩阵P，然后令P左乘数据集X得到变换后的数据集的X’，进而实现了数据集的维数由N变换为R（R&lt;N）<br>
这样的正交变换可能会导致原本可分的空间变得不可分于是PCA问题就变成了一个正交基的优化问题，故需要寻找一组正交基使得样本数据尽量分散。<br>
评价数据的分散程度则需要借助方差。另一方面，我们还希望变换后各特征之间的相关性尽可能小，评价相关性则利用协方差。<br>
则降维问题的<strong>优化目标</strong>为：将一组N维向量降为R维（R大于0，小于N），其目标是选择R个单位（模为1）正交基，使得原始数据变换到这组基上后，各特征两两间协方差为0，而特征的方差则尽可能大。<br>
<img src= "https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/1657537115748.png" alt="1657537115748"><br>
<img src= "https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="../md_imgs/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/1657537128911.png" alt="1657537128911"><br>
为了使方差尽量大，协方差为0，其实就是寻找一个P将协方差矩阵给对角化，且需要留出最大的R个特征值。<br>
故对于M条N维数据，PCA算法步骤为：</p>
<ol>
<li>
<p>写出N行M列矩阵X</p>
</li>
<li>
<p>将X的每一行（）零均值化</p>
</li>
<li>
<p>求出协方差矩阵C=<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mi>M</mi></mfrac><mi>A</mi><msup><mi>A</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">\frac{1}{M}AA^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.10903em;">M</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord mathdefault">A</span><span class="mord"><span class="mord mathdefault">A</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span></p>
</li>
<li>
<p>求出协方差矩阵的特征值和对应的特征向量</p>
</li>
<li>
<p>将特征向量按对应特征值大小从上到下按行拍成矩阵，取前R行组成矩阵P</p>
</li>
<li>
<p>Y=PX即降维后的数据。</p>
</li>
</ol>
<h3 id="奇异值分解">奇异值分解</h3>
<p>奇异值分解是指将一个矩阵拆分成三个不同的矩阵的一种方法。</p>
<blockquote>
<p>与PCA算法相比，两者都可以对数据进行降维处理。但PCA跳过了不太重要的成分，而SVD只是把它们变成特殊的数据，表示为三个不同的矩阵，更容易操作和分析。当涉及到<strong>概率方法</strong>时，对于更抽象的问题，最好使用线性判别分析算法。</p>
</blockquote>
<h3 id="线性判别分析-lda">线性判别分析（LDA）</h3>
<p>LDA(Linear Discriminant Analysis), 线性判别分析，是一种<strong>有监督学习</strong>的降维技术，它的数据集的每个样本是有类别标签的。<br>
LDA的思想可以用一句话概括，就是“投影后类内方差最小，类间方差最大”。<br>
即数据集投影到低维空间后，希望每一种类别数据的投影点尽可能的接近，而不同类别的数据的类别中心之间的距离尽可能的远。<br>
<strong>数据集合标准化、正则化和特征降维都是为应对不同数量级对机器学习性能的影响，减小计算量的一种无奈之举，实际上这些预处理操作都会不可避免地造成信息的丢失。</strong></p>
<blockquote>
<p>PCA 和 LDA 有很多的相似点，其本质是要将原始的样本映射到维度更低的样本空间中，但是 PCA 和 LDA 的映射目标不一样：PCA 是为了让映射后的样本具有最大的发散性；而 LDA 是为了让映射后的样本有最好的<strong>分类性能</strong>。所以说 PCA 是一种无监督的降维方法，而 LDA 是一种有监督的降维方法。<br>
它经常被用于人脸识别、客户识别和医学领域，以识别病人的疾病状况。</p>
</blockquote>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://kobal.cn">爱编程的小明</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://kobal.cn/8ae97bf8/">https://kobal.cn/8ae97bf8/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kobal.cn" target="_blank">小明的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/">特征工程</a></div><div class="post_share"><div class="social-share" data-image="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/top_img/10009.webp" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://gcore.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://gcore.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/ad1229bf/"><img class="prev-cover" src= "https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/top_img/10029.webp" onerror="onerror=null;src='https://images.pexels.com/photos/374918/pexels-photo-374918.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=600'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">help大法</div></div></a></div><div class="next-post pull-right"><a href="/41b133f5/"><img class="next-cover" src= "https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/top_img/10001.webp" onerror="onerror=null;src='https://images.pexels.com/photos/374918/pexels-photo-374918.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=600'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">随机森林</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/1bbec40e/" title="如何使用机器学习神器sklearn做特征工程？"><img class="cover" src= "https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/img/loading.gif" data-lazy-src="https://gcore.jsdelivr.net/gh/kebuAAA/Picloud@main/top_img/10006.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-04-05</div><div class="title">如何使用机器学习神器sklearn做特征工程？</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">1.</span> <span class="toc-text">数据预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BC%BA%E5%A4%B1%E5%80%BC%E5%A4%84%E7%90%86"><span class="toc-number">1.1.</span> <span class="toc-text">缺失值处理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%97%A0%E9%87%8F%E7%BA%B2%E5%8C%96"><span class="toc-number">1.2.</span> <span class="toc-text">无量纲化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#z-score%E6%A0%87%E5%87%86%E5%8C%96"><span class="toc-number">1.2.1.</span> <span class="toc-text">Z-score标准化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8C%BA%E9%97%B4%E6%94%BE%E7%BC%A9%E6%B3%95"><span class="toc-number">1.2.2.</span> <span class="toc-text">区间放缩法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BD%92%E4%B8%80%E5%8C%96"><span class="toc-number">1.2.3.</span> <span class="toc-text">归一化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%B9%E6%95%B0%E6%A0%87%E5%87%86%E5%8C%96"><span class="toc-number">1.2.4.</span> <span class="toc-text">对数标准化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9A%E9%87%8F%E7%89%B9%E5%BE%81%E4%BA%8C%E5%80%BC%E5%8C%96"><span class="toc-number">1.3.</span> <span class="toc-text">定量特征二值化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AF%B9%E5%AE%9A%E6%80%A7%E7%89%B9%E5%BE%81%E5%93%91%E7%BC%96%E7%A0%81"><span class="toc-number">1.4.</span> <span class="toc-text">对定性特征哑编码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%8F%98%E6%8D%A2"><span class="toc-number">1.5.</span> <span class="toc-text">数据变换</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9"><span class="toc-number">2.</span> <span class="toc-text">特征选择</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#filter"><span class="toc-number">2.1.</span> <span class="toc-text">Filter</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#wrapper"><span class="toc-number">2.2.</span> <span class="toc-text">Wrapper</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#embedded"><span class="toc-number">2.3.</span> <span class="toc-text">Embedded</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E9%99%8D%E7%BB%B4"><span class="toc-number">3.</span> <span class="toc-text">特征降维</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90-pca"><span class="toc-number">3.0.1.</span> <span class="toc-text">主成分分析（PCA）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3"><span class="toc-number">3.0.2.</span> <span class="toc-text">奇异值分解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90-lda"><span class="toc-number">3.0.3.</span> <span class="toc-text">线性判别分析（LDA）</span></a></li></ol></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background: transparent"><div id="footer-wrap"><div class="footer_custom_text"><div id="runtime"></div></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"></div><div id="rightside-config-show"><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.staticfile.org/fancyapps-ui/4.0.27/fancybox.umd.min.js"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/instant.page/5.1.0/instantpage.min.js" type="module"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/vanilla-lazyload/17.3.1/lazyload.iife.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>(()=>{
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://twikoo.kobal.cn/',
      region: '',
      onCommentLoaded: function () {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))
  }

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://twikoo.kobal.cn/',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      countELement.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const runFn = () => {
    init()
    
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') {
      setTimeout(runFn,0)
      return
    } 
    getScript('https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/twikoo/1.4.18/twikoo.all.min.js').then(runFn)
  }

  if ('Twikoo' === 'Twikoo' || !true) {
    if (true) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script src="/Scripts/js/beijing.js"></script><script src="/Scripts/js/foot_style.js"></script><canvas class="fireworks" mobile="false"></canvas><script src="https://gcore.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --><div class="js-pjax"><script async="async">var arr = document.getElementsByClassName('recent-post-item');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '2s');
    arr[i].setAttribute('data-wow-delay', '0.5s');
    arr[i].setAttribute('data-wow-offset', '100');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('card-widget');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script></div><script defer src="https://unpkg.zhimg.com/hexo-butterfly-wowjs/lib/wow.min.js"></script><script defer src="https://unpkg.zhimg.com/hexo-butterfly-wowjs/lib/wow_init.js"></script><script defer src="https://npm.elemecdn.com/jquery@latest/dist/jquery.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/kity@2.0.4/dist/kity.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/hexo-tag-plugins@latest/lib/mindmap.min.js"></script><!-- hexo injector body_end end --></body></html>